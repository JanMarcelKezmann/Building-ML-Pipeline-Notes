{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 9: Advanced Model Deployments with TensorFlow Serving.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxK2QBVfWkR-"
      },
      "source": [
        "# Chapter 9: Advanced Model Deployments with TensorFlow Serving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc0oyNGJWp3B"
      },
      "source": [
        "## Decoupling Deployment Cycles\n",
        "\n",
        "The basic deployments shown in the previous chapter work well, but have one restriction: The trained and validated model needs to be either included in the deployment container image during the build step or mounted into the container during the container runtime. This requires either DevOps processes or a good coordination of the data science and DevOps temas during the deployment phase of a new model version.<br>\n",
        "Since TensorFlow Serving frequently polls the model storage location, unloads the previously loaded model and loads a newer model upon detection, we only need to deploy our model serving container once."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4estTsILXgcT"
      },
      "source": [
        "### Workflow Overview\n",
        "\n",
        "See figure 9-1 on page 259 for the separation of workflows.<br>\n",
        "If your bucket folders are publicly accessible, you can serve the remote models by simply updating the model base path to the remote path:\n",
        "\n",
        "    !docker run -p 8500:8500 \\ # specify the default ports\n",
        "                -p 8501:8501 \\\n",
        "                -e MODEL_NAME=my_model \\ # specify your model # Remaining configuration remains the same\n",
        "                -e MODEL_BASE_PATH=s3://bucketname/model_path/ \\ # Remote bucket path\n",
        "                -t tensorflow/serving # specify the docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynqjzUe4YdMI"
      },
      "source": [
        "#### Accessing private model from AWS S3\n",
        "\n",
        "    !docker run -p 8500:8500 \\ # specify the default ports\n",
        "                -p 8501:8501 \\\n",
        "                -e MODEL_NAME=my_model \\ # specify your model # Remaining configuration remains the same\n",
        "                -e MODEL_BASE_PATH=s3://bucketname/model_path/ \\ # Remote bucket path\n",
        "                -e AWS_ACCESS_KEY_ID=XXXXX \\ # The name of the environment variables is important\n",
        "                -e AWS_SECRET_ACCESS_KEY=XXXXX \\\n",
        "                -t tensorflow/serving # specify the docker image\n",
        "\n",
        "For details regarding further conifguration options see page 260f.<br>\n",
        "With these few additional environment variables provided to TensorFlow Serving, you are now able to load models from remote AWS S3 buckets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaEbDe8GZZ3w"
      },
      "source": [
        "#### Accessing private models from GCP Buckets\n",
        "\n",
        "GCP authenticates users through service accounts. To access private GCP Storage buckets, you need to create a service account file. Unlike in AWS GCP authentication expects a JSON file with the service account credentials. For the following example, we assume that you have saved your nely created service account credential file under /home/you_username/.credentials/ on your host machine. This has to be downloaded and saved as credentials.json.\n",
        "\n",
        "    !docker run -p 8500:8500 \\ # specify the default ports\n",
        "                -p 8501:8501 \\\n",
        "                -e MODEL_NAME=my_model \\ # specify your model # Remaining configuration remains the same\n",
        "                -e MODEL_BASE_PATH=gcp://bucketname/model_path/ \\ # Remote bucket path\n",
        "                -v /home/your_username/.credentials/:/credentials/ # Mount host directory with credentials\n",
        "                -e GOOGLE_APPLICATION_CREDENTIALS=/credentials/sa-credentials.json \\ # Specify path inside of the container\n",
        "                -t tensorflow/serving # specify the docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7CHPMStZgkU"
      },
      "source": [
        "### Optimization of Remote Model Loading\n",
        "\n",
        "Its recommend to reduce the polling frequency to every 120 seconds, which still provides you up to 30 potential updates per hour but generates 60 times less traffic than the default. You can change this in docker run with:\n",
        " - --file_system_poll_wait_seconds=120\n",
        "\n",
        "If time is set to 0 than TensorFlow Serving will not attempt to refresh the loaded model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dub0rQxqmHPy"
      },
      "source": [
        "## Model Optimization for Deployments\n",
        "\n",
        "There are three optimization methods for smaller models that allow faster model inferences:\n",
        " - Model Quantization (allows to reduce the computation complexity of a model by reducing the precision of the weight's representation, e.g. from 32-bit floats to bfloat16 format.)\n",
        "  - Model Quantization is applied after model training and are often called post-training quantization. Since a quantized model can underfit it is important to test the quantize model. Useful tools are:\n",
        "    - <a href=\"https://developer.nvidia.com/tensorrt\">Nvidia's TensorRT</a>\n",
        "    - <a href=\"https://www.tensorflow.org/lite\">TFLite Library</a>\n",
        " - Model Pruning (Idea is to reduce the trained network to a smaller one by removing unnecessary wegihts, i.e. weights that are set to 0. This speds up inference and prediction and compresses the model to smaller model sizes). You can prune the model durint their training phase through tool like TensorFlow's model optimization package *tensorflow-model-optimization*\n",
        " - Model Distillation (The idea of training a smaller, less comples neural network to learn trained tasks from a much more extensive network.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q8ygrrgovfw"
      },
      "source": [
        "## Using TensorRT with TensorFlow Serving\n",
        "\n",
        "After a model is trained, you need to optimize the model with TensorRT's own optimizer or with saved_model_cli. The optimized model can then be loaded into TensorFlow Serving."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS33Nn9eT0h0"
      },
      "source": [
        "!saved_model_cli convert --dir saved_models/ \\\n",
        "                         --output_dir trt-savedmodel/ \\\n",
        "                         --tag_set serve tensorrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aozYWjy0pNLo"
      },
      "source": [
        "AFter the conversion you can load the model in our GPU setup of TensorFlow Serving as follows:\n",
        "\n",
        "    !docker run -p 8500:8500 \\ # specify the default ports\n",
        "                -p 8501:8501 \\\n",
        "                --mount type=bind,sourch=/path/to/models,target=/models/my_model \\\n",
        "                -e MODEL_NAME=my_model \\ # specify your model # Remaining configuration remains the same\n",
        "                -t tensorflow/serving # specify the docker image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lIN7HlRplbR"
      },
      "source": [
        "## TFLite\n",
        "\n",
        "Traditionally TFLite was used to convert ML models to smaller model sizes for deployment to mobile or IoT devices, however these models can also be used for TensorFlow Serving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk0Q7lTpp-qd"
      },
      "source": [
        "### Steps to Optimize Your Model with TFLite\n",
        "\n",
        "The conversion process consits of four steps:\n",
        "\n",
        " - 1. Loading the exported model\n",
        " - 2. Defining your model optimization goals\n",
        " - 3. Converting the model\n",
        " - 4. Saving the optimized model as a TFLite model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sfAowjgcpT5N"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zNTHjnBcqQm0"
      },
      "source": [
        "saved_model_dir = \"path_to_saved_model\"\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "converter.optimizations = [\n",
        "                           # Set the optimization strategy\n",
        "                           tf.lite.Optimize.DEFAULT\n",
        "]\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(\"/tmp/model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tU21e701qrxU"
      },
      "source": [
        "There are some further optimization options for TFLite, see page 269f. in the book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrPJShfcq32r"
      },
      "source": [
        "### Serving TFLite Models with TensorFlow Serving\n",
        "\n",
        "You only need to start TensorFlow Serving with the enabled *use_tflite_model* flag and it will lod the optimized model as shown in the following example:\n",
        "\n",
        "    !docker run -p 8501:8501 \\\n",
        "                --mount type=bind,source=/path/to/models,target=/models/my_model \\\n",
        "                -e MODEL_NAME=my_model \\ # specify your model # Remaining configuration remains the same\n",
        "                -e MODEL_BASE_PATH=/models \\\n",
        "                -t tensorflow/serving # specify the docker image\n",
        "                --use_tflite_model=true # Enable TFLite model loading\n",
        "\n",
        "**Deploy Your Models to Edge Devices**\n",
        "After optimizing your model with TFLite, you can also deploy the model to a variety of mobile and edge devices:\n",
        " - Android and iOS mobile phones\n",
        " - ARM64-based computers\n",
        " - Microcontrollers and other embedded devices (e.g. Raspberry Pi)\n",
        " - Edge devices (e.g. IoT devices)\n",
        " - Edge TPUs (e.g. Coral)\n",
        "\n",
        "For Details read the publication *Practical Deep Learning for Clud, Mobile, Edge* by *Anirudh Koul et al (O'Reilly)*. If you are looking for materials on edge devices with a focus on TFMicro, we recommend *TinyML* by *Pete Warden and Daniel Situnayake (O'Reilly)*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7IVo5eOssadW"
      },
      "source": [
        "## Monitoring Your TensorFlow Serving Instances\n",
        "\n",
        "Use Prometheus a free application for real-time event logging and alerting, that is currently und Apache License 2.0 and allows you to monitor your inference setup. Usually it is used with Kubernetes, but it can easily be used without it.<br>\n",
        "TensorFlow Serving and Prometheus have to run simultaniously, so that Prometheus can pull metrics from TensorFlow Serving via a REST endpoint, which requires that they are enabled for TensorFlow Serving even if you are only using gPRC endpoints in your application."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cz8tIeFfw3o4"
      },
      "source": [
        "### Prometheus Setup\n",
        "\n",
        "Before configuring TensorFlow Serving to provide metrics to Prometheus, we need to set up and configure our Prometheus instance. For simplicity you can run two Docker instances as shown in Figure-9 on page 272. In a more elaborate setup, the applications would be Kubernetes deployment.<br>\n",
        "Create a Prometheus configuration file before starting it, locate it at /tmp/prometheus.yml with the following configuration details:\n",
        "    global:\n",
        "        scrape_interval: 15s\n",
        "        evaluation_interval: 15s\n",
        "        external_labels:\n",
        "            monitor: \"tf-serving-monitor\n",
        "\n",
        "    scrape_configs:\n",
        "        - job_name: \"prometheus\n",
        "          scrape_intervall: 5s # Interval when metrics are pulled\n",
        "          metrics_path: /monitoring/prometheus/metrics # Metrics enpoints from TensorFlow Serving\n",
        "\n",
        "    static_configs:\n",
        "        - targets: [\"host.docker.internal:8501\"] # Replace with the IP address of your application\n",
        "\n",
        "Once you have creatd your Prometheus configuration file, you can start the Docker container, which runs the Prometheus instance:\n",
        "\n",
        "    !docker run -p 9090:9090 \\ # Enable port 9090\n",
        "                -v /tmp/prometheus.yml:/etc/prometheus/prometheus.yml \\ # Mount your configuration file\n",
        "                prom/prometheus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKyflQM7yrNp"
      },
      "source": [
        "### TensorFlow Serving Configuration\n",
        "\n",
        "We need to write a small configuration file to configure the logging settings.\n",
        "Ans save it, e.g. like /tmp/monitoring_config.txt\n",
        "\n",
        "    prometheus_config {\n",
        "        enable: true,\n",
        "        path: \"/monitoring/prometheus/metrics\" # URL path for metrics data it needs to match the path we specified in the Prometheus configuration that we previously created (/tmp/prometheus.yml)\n",
        "    }\n",
        "\n",
        "Add path of *monitoring_config_file* and TensorFlow Serving will provide a REST endpoint with the metrics data for Prometheus:\n",
        "\n",
        "    !docker run -p 8501:8501 \\\n",
        "                --mount type=bind,source=`pwd`,target=/models/my_model \\\n",
        "                --mount type=bind,source=/tmp,target=/models_config tensorflow/serving \\\n",
        "                --monitoring_config_file=/model_config/monitoring_config.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnswxR_pTtIT"
      },
      "source": [
        "## Simple Scaling with TensorFlow Serving and Kubernetes\n",
        "\n",
        "Previous deployment methods are really good for one or more model versions and a good number of deployments, but it is not enough for applications experiencing a high volume of prediction requests. In this case the Docker container with TensorFlow Serving needs to be replicated to reply to the additional prediction requests. The orchestration of the container is usually managed by tools like Docker Swarm or Kubernetes.<br>\n",
        "For the following example it is assumed that you will habe a Kubernetes cluster runing and that access to the cluster will be via *kubectl*.\n",
        "The first source code example hihglights two aspects:\n",
        " - Deploying via Kubernetes without building specific Docker containers\n",
        " - Handling the Google Cloud authentication to access the remote model storage location.\n",
        "\n",
        " For code details see page 276f. in the book.\n",
        "\n",
        "With the example, we can now deploy and scale your TensorFlow or Keras models without building custom Docker images. The service account credential file within the Kubernetes environment can be created with the following command:\n",
        "\n",
        "    !kubectl create secret generic gcp-credentials --from-file=/path/to/your/user-gcp-sa.json\n",
        "\n",
        "For the corresponding service setup in Kubernetes look at page 279.<br>\n",
        "\n",
        "**Further Reading on Kubernetes and Kubeflow**\n",
        " - Kubernetes: Up and Running, 2nd edition by Brendan Burns et al. (O'Reilly)\n",
        " - Kubeflow Operations Guide by Josh Patterson et al. (O'reilly)\n",
        " - Kubeflow for Machine Learning (forthcoming) by Holden Karaus et al. (O'Reilly)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJIwRepTs3l"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        " - <a href=\"https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html#Using_CreateAccessKey9\">AWS Access Keys</a>\n",
        " - <a href=\"https://www.tensorflow.org/model_optimization/guide/pruning\">TF Optimization Methods</a>\n",
        " - <a href=\"https://www.tensorflow.org/model_optimization/guide/pruning/pruning_with_keras\">TF indepth pruning example</a>\n",
        " - <a href=\"https://docs.nvidia.com/deeplearning/tensorrt/developer-guide/index.html\">Nvidia TensorRT Documentation</a>"
      ]
    }
  ]
}