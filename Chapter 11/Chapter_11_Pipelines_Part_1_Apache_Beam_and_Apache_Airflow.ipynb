{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_UxA69aLg43"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xkTWvW5IcFs",
        "outputId": "7ce08b49-2474-45e5-c6ee-9e22757901b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd drive/My\\ Drive/Building\\ ML\\ Pipelines/\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Building ML Pipelines\n",
            "Requirement already satisfied: tensorflow>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: tfx>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.24.0)\n",
            "Requirement already satisfied: tensorboard_plugin_fairness_indicators>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 3)) (0.24.0)\n",
            "Requirement already satisfied: tensorflow_hub>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 4)) (0.9.0)\n",
            "Requirement already satisfied: tensorflow_privacy>=0.5.1 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: pandas>=1.1.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (1.1.2)\n",
            "Requirement already satisfied: witwidget in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 7)) (1.7.0)\n",
            "Requirement already satisfied: google-cloud-core>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (1.4.3)\n",
            "Requirement already satisfied: apache-beam[gcp]>=2.24.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 9)) (2.24.0)\n",
            "Requirement already satisfied: apache-airflow in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (1.10.12)\n",
            "Requirement already satisfied: oauth2client<4.0.0 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 11)) (3.0.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (0.35.1)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (2.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (0.2.0)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.4.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.18.5)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.32.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.6.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (0.10.0)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow>=2.3.0->-r requirements.txt (line 1)) (0.3.3)\n",
            "Requirement already satisfied: tensorflow-data-validation<0.25,>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.1)\n",
            "Requirement already satisfied: click<8,>=7 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (7.1.2)\n",
            "Requirement already satisfied: kubernetes<12,>=10.0.1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (11.0.0)\n",
            "Requirement already satisfied: ml-metadata<0.25,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.0)\n",
            "Requirement already satisfied: attrs<20,>=19.3.0 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (19.3.0)\n",
            "Requirement already satisfied: keras-tuner<2,>=1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (1.0.1)\n",
            "Requirement already satisfied: tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,<3,>=1.15 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: google-api-python-client<2,>=1.7.8 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (1.7.12)\n",
            "Requirement already satisfied: tfx-bsl<0.25,>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.1)\n",
            "Requirement already satisfied: tensorflow-transform<0.25,>=0.24.1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.1)\n",
            "Requirement already satisfied: pyyaml<6,>=3.12 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (5.3.1)\n",
            "Requirement already satisfied: jinja2<3,>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (2.11.2)\n",
            "Requirement already satisfied: pyarrow<0.18,>=0.17 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.17.1)\n",
            "Requirement already satisfied: docker<5,>=4.1 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (4.3.1)\n",
            "Requirement already satisfied: tensorflow-model-analysis<0.25,>=0.24.3 in /usr/local/lib/python3.6/dist-packages (from tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.3)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy>=0.5.1->-r requirements.txt (line 5)) (1.1.0)\n",
            "Requirement already satisfied: dm-tree~=0.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow_privacy>=0.5.1->-r requirements.txt (line 5)) (0.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.1.2->-r requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=1.1.2->-r requirements.txt (line 6)) (2018.9)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.6/dist-packages (from witwidget->-r requirements.txt (line 7)) (7.5.1)\n",
            "Requirement already satisfied: jupyter<2,>=1.0 in /usr/local/lib/python3.6/dist-packages (from witwidget->-r requirements.txt (line 7)) (1.0.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0dev,>=1.19.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core>=1.4.2->-r requirements.txt (line 8)) (1.22.4)\n",
            "Requirement already satisfied: mock<3.0.0,>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (2.0.0)\n",
            "Requirement already satisfied: crcmod<2.0,>=1.7 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.7)\n",
            "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (2.5.8)\n",
            "Requirement already satisfied: avro-python3!=1.9.2,<1.10.0,>=1.8.1; python_version >= \"3.0\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.9.2.1)\n",
            "Requirement already satisfied: future<1.0.0,>=0.18.2 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.18.2)\n",
            "Requirement already satisfied: dill<0.3.2,>=0.3.1.1 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.3.1.1)\n",
            "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (3.11.0)\n",
            "Requirement already satisfied: httplib2<0.18.0,>=0.8 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.17.4)\n",
            "Requirement already satisfied: fastavro<0.24,>=0.21.4 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.23.6)\n",
            "Requirement already satisfied: typing-extensions<3.8.0,>=3.7.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (3.7.4.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (2.24.0)\n",
            "Requirement already satisfied: pydot<2,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: google-cloud-language<2,>=1.3.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.3.0)\n",
            "Requirement already satisfied: google-cloud-datastore<2,>=1.7.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.8.0)\n",
            "Requirement already satisfied: google-cloud-vision<2,>=0.38.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-dlp<2,>=0.12.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.0.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.21.0)\n",
            "Requirement already satisfied: grpcio-gcp<1,>=0.2.2; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.2.2)\n",
            "Requirement already satisfied: google-apitools<0.5.32,>=0.5.31; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.5.31)\n",
            "Requirement already satisfied: cachetools<4,>=3.1.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (3.1.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.18.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.22.1)\n",
            "Requirement already satisfied: google-cloud-videointelligence<2,>=1.8.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.16.0)\n",
            "Requirement already satisfied: google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.7.0)\n",
            "Requirement already satisfied: google-cloud-bigtable<2,>=0.31.1; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.5.1)\n",
            "Requirement already satisfied: google-cloud-spanner<2,>=1.13.0; extra == \"gcp\" in /usr/local/lib/python3.6/dist-packages (from apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.19.0)\n",
            "Requirement already satisfied: lazy-object-proxy~=1.3 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: flask-admin==1.5.4 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.5.4)\n",
            "Requirement already satisfied: python-slugify<5.0,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (4.0.1)\n",
            "Requirement already satisfied: alembic<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.4.3)\n",
            "Requirement already satisfied: colorlog==4.0.2 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (4.0.2)\n",
            "Requirement already satisfied: python-daemon>=2.1.1 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (2.2.4)\n",
            "Requirement already satisfied: markdown<3.0,>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (2.6.11)\n",
            "Requirement already satisfied: tabulate<0.9,>=0.7.5 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.8.7)\n",
            "Requirement already satisfied: tenacity==4.12.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (4.12.0)\n",
            "Requirement already satisfied: werkzeug<1.0.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.16.1)\n",
            "Requirement already satisfied: unicodecsv>=0.14.1 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.14.1)\n",
            "Requirement already satisfied: json-merge-patch==0.2 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.2)\n",
            "Requirement already satisfied: flask<2.0,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.1.2)\n",
            "Requirement already satisfied: setproctitle<2,>=1.1.8 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.1.10)\n",
            "Requirement already satisfied: configparser<3.6.0,>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (3.5.3)\n",
            "Requirement already satisfied: jsonschema~=3.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: gunicorn<21.0,>=19.5.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (20.0.4)\n",
            "Requirement already satisfied: cattrs~=1.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: pendulum==1.4.4 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.4.4)\n",
            "Requirement already satisfied: flask-login<0.5,>=0.3 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.4.1)\n",
            "Requirement already satisfied: thrift>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.13.0)\n",
            "Requirement already satisfied: iso8601>=0.1.12 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.1.13)\n",
            "Requirement already satisfied: flask-appbuilder~=2.2; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (2.3.4)\n",
            "Requirement already satisfied: croniter<0.4,>=0.3.17 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.3.34)\n",
            "Requirement already satisfied: sqlalchemy-jsonfield~=0.9; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.9.0)\n",
            "Requirement already satisfied: argcomplete~=1.10 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.12.1)\n",
            "Requirement already satisfied: graphviz>=0.12 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.14.2)\n",
            "Requirement already satisfied: flask-swagger<0.3,>=0.2.13 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.2.14)\n",
            "Requirement already satisfied: funcsigs<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.0.2)\n",
            "Requirement already satisfied: python-nvd3~=0.15.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.15.0)\n",
            "Requirement already satisfied: cached-property~=1.5 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.5.2)\n",
            "Requirement already satisfied: flask-wtf<0.15,>=0.14.2 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (0.14.3)\n",
            "Requirement already satisfied: tzlocal<2.0.0,>=1.4 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: pygments<3.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (2.6.1)\n",
            "Requirement already satisfied: zope.deprecation<5.0,>=4.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (4.4.0)\n",
            "Requirement already satisfied: psutil<6.0.0,>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (5.4.8)\n",
            "Requirement already satisfied: flask-caching<1.4.0,>=1.3.3 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: sqlalchemy~=1.3 in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.3.19)\n",
            "Requirement already satisfied: email-validator in /usr/local/lib/python3.6/dist-packages (from apache-airflow->-r requirements.txt (line 10)) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4.0.0->-r requirements.txt (line 11)) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4.0.0->-r requirements.txt (line 11)) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client<4.0.0->-r requirements.txt (line 11)) (0.2.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->-r requirements.txt (line 1)) (50.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow>=2.3.0->-r requirements.txt (line 1)) (1.7.0)\n",
            "Requirement already satisfied: tensorflow-metadata<0.25,>=0.24 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.25,>=0.24.1->tfx>=0.24.0->-r requirements.txt (line 2)) (0.24.0)\n",
            "Requirement already satisfied: joblib<0.15,>=0.12 in /usr/local/lib/python3.6/dist-packages (from tensorflow-data-validation<0.25,>=0.24.1->tfx>=0.24.0->-r requirements.txt (line 2)) (0.14.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12,>=10.0.1->tfx>=0.24.0->-r requirements.txt (line 2)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12,>=10.0.1->tfx>=0.24.0->-r requirements.txt (line 2)) (2020.6.20)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.6/dist-packages (from kubernetes<12,>=10.0.1->tfx>=0.24.0->-r requirements.txt (line 2)) (1.3.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.6/dist-packages (from kubernetes<12,>=10.0.1->tfx>=0.24.0->-r requirements.txt (line 2)) (0.57.0)\n",
            "Requirement already satisfied: terminaltables in /usr/local/lib/python3.6/dist-packages (from keras-tuner<2,>=1->tfx>=0.24.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tuner<2,>=1->tfx>=0.24.0->-r requirements.txt (line 2)) (4.41.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from keras-tuner<2,>=1->tfx>=0.24.0->-r requirements.txt (line 2)) (0.22.2.post1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.6/dist-packages (from keras-tuner<2,>=1->tfx>=0.24.0->-r requirements.txt (line 2)) (0.4.3)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx>=0.24.0->-r requirements.txt (line 2)) (0.0.4)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client<2,>=1.7.8->tfx>=0.24.0->-r requirements.txt (line 2)) (3.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2<3,>=2.7.3->tfx>=0.24.0->-r requirements.txt (line 2)) (1.1.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (5.5.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (4.3.3)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (4.10.1)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (5.0.7)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (3.5.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (5.3.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (4.7.7)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (5.6.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.19.0->google-cloud-core>=1.4.2->-r requirements.txt (line 8)) (1.52.0)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python3.6/dist-packages (from mock<3.0.0,>=1.0.1->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (5.5.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.6/dist-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.6.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: pyparsing>=2.1.4 in /usr/local/lib/python3.6/dist-packages (from pydot<2,>=1.2.0->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (2.4.7)\n",
            "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery<2,>=1.6.0; extra == \"gcp\"->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.4.1)\n",
            "Requirement already satisfied: fasteners>=0.14 in /usr/local/lib/python3.6/dist-packages (from google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.15)\n",
            "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /usr/local/lib/python3.6/dist-packages (from google-cloud-pubsub<2,>=0.39.0; extra == \"gcp\"->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (0.12.3)\n",
            "Requirement already satisfied: wtforms in /usr/local/lib/python3.6/dist-packages (from flask-admin==1.5.4->apache-airflow->-r requirements.txt (line 10)) (2.3.3)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify<5.0,>=3.0.0->apache-airflow->-r requirements.txt (line 10)) (1.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.6/dist-packages (from alembic<2.0,>=1.0->apache-airflow->-r requirements.txt (line 10)) (1.1.3)\n",
            "Requirement already satisfied: python-editor>=0.3 in /usr/local/lib/python3.6/dist-packages (from alembic<2.0,>=1.0->apache-airflow->-r requirements.txt (line 10)) (1.0.4)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.6/dist-packages (from python-daemon>=2.1.1->apache-airflow->-r requirements.txt (line 10)) (0.16)\n",
            "Requirement already satisfied: lockfile>=0.10 in /usr/local/lib/python3.6/dist-packages (from python-daemon>=2.1.1->apache-airflow->-r requirements.txt (line 10)) (0.12.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask<2.0,>=1.1.0->apache-airflow->-r requirements.txt (line 10)) (1.1.0)\n",
            "Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from jsonschema~=3.0->apache-airflow->-r requirements.txt (line 10)) (0.17.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from jsonschema~=3.0->apache-airflow->-r requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: pytzdata>=2018.3.0.0 in /usr/local/lib/python3.6/dist-packages (from pendulum==1.4.4->apache-airflow->-r requirements.txt (line 10)) (2020.1)\n",
            "Requirement already satisfied: Flask-JWT-Extended<4,>=3.18 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (3.24.1)\n",
            "Requirement already satisfied: Flask-SQLAlchemy<3,>=2.4 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (2.4.4)\n",
            "Requirement already satisfied: marshmallow<3.0.0,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (2.21.0)\n",
            "Requirement already satisfied: sqlalchemy-utils<1,>=0.32.21 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (0.36.8)\n",
            "Requirement already satisfied: marshmallow-enum<2,>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (1.5.1)\n",
            "Requirement already satisfied: PyJWT>=1.7.1 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (1.7.1)\n",
            "Requirement already satisfied: prison<1.0.0,>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (0.1.3)\n",
            "Requirement already satisfied: Flask-OpenID<2,>=1.2.5 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (1.2.5)\n",
            "Requirement already satisfied: marshmallow-sqlalchemy<1,>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (0.23.1)\n",
            "Requirement already satisfied: apispec[yaml]<2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (1.3.3)\n",
            "Requirement already satisfied: Flask-Babel<2,>=1 in /usr/local/lib/python3.6/dist-packages (from flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (1.0.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.6/dist-packages (from croniter<0.4,>=0.3.17->apache-airflow->-r requirements.txt (line 10)) (5.5.0)\n",
            "Requirement already satisfied: typing>=3.6; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from sqlalchemy-jsonfield~=0.9; python_version >= \"3.5\"->apache-airflow->-r requirements.txt (line 10)) (3.7.4.3)\n",
            "Requirement already satisfied: dnspython>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from email-validator->apache-airflow->-r requirements.txt (line 10)) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx>=0.24.0->-r requirements.txt (line 2)) (3.1.0)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (4.8.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (1.0.18)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (4.4.2)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (0.7.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.3.1->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (0.2.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (5.3.5)\n",
            "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python3.6/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (4.6.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.9.1)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (19.0.2)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (1.9.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (3.2.1)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.4.4)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.8.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (1.4.2)\n",
            "Requirement already satisfied: monotonic>=0.1 in /usr/local/lib/python3.6/dist-packages (from fasteners>=0.14->google-apitools<0.5.32,>=0.5.31; extra == \"gcp\"->apache-beam[gcp]>=2.24.0->-r requirements.txt (line 9)) (1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->jsonschema~=3.0->apache-airflow->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: python3-openid>=2.0 in /usr/local/lib/python3.6/dist-packages (from Flask-OpenID<2,>=1.2.5->flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (3.2.0)\n",
            "Requirement already satisfied: Babel>=2.3 in /usr/local/lib/python3.6/dist-packages (from Flask-Babel<2,>=1->flask-appbuilder~=2.2; python_version >= \"3.6\"->apache-airflow->-r requirements.txt (line 10)) (2.8.0)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.6/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (0.6.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->witwidget->-r requirements.txt (line 7)) (0.2.5)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (0.5.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter<2,>=1.0->witwidget->-r requirements.txt (line 7)) (20.4)\n",
            "/content/drive/My Drive\n",
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpJj0bhJIkU5"
      },
      "source": [
        "# Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow\n",
        "\n",
        "In this and the next chapter, we will put all the components together and show how to run the full pipeline with three orchestrators:\n",
        " - Apache Beam\n",
        " - Apache Airflow\n",
        " - Kubeflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmNmEmYAQUKl"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_dir = \"drive/My Drive/Building ML Pipelines/\"\n",
        "chap_dir = base_dir + \"Chapter 11/\"\n",
        "data_dir = base_dir + \"Data/\"\n",
        "out_dir = chap_dir + \"Outputs/\"\n",
        "csv_data_dir = base_dir + \"CSV Data/\"\n",
        "csv_dir = csv_data_dir + \"consumer_complaints_with_narrative.csv\"\n",
        "eval_data_dir = base_dir + \"Chapter 6/Outputs/CsvExampleGen/examples/1/eval/data_tfrecord-00000-of-00001.gz\""
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV6jpokmJBeH"
      },
      "source": [
        "## Which Orchestration Tool to Choose?\n",
        "\n",
        "You need to pick only one Tool to run the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMQOnULJdGd"
      },
      "source": [
        "### Apache Beam\n",
        "\n",
        "If your are looking for a minimal installation, reusing Beam to orchestrate is a logical choice. It is easy to set up and also allows you to use any existing distributed data processing infrastructure you might already be familiar with (e.g. Google Cloud Dataflow). It could also be used as an intermediate step to ensure the correctness of the pipeline.<br>\n",
        "However, Apache Beam is missing a variety of tools for scheduling your model updates or monitoring the process of a pipeline job, that's where the other two tools shine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TXQk8iIKF6g"
      },
      "source": [
        "### Apache Airflow\n",
        "\n",
        "It is often used in companies for data-loading tasks. If you use Apache Airflow in combinatino with a production-ready database like PostgreSQL, you can take advantage of executing partial pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJAA9_4KKcdB"
      },
      "source": [
        "### Kubeflow Pipelines\n",
        "\n",
        "If you already have experience with Kubernetes and access to a Kubernetes cluster, it makes sense to consider Kubeflow Pipelines. While the setup is more complicated, it opens up a variety of new opportunities, including the ability to view TFDV and TFMA visualizations, model lineage and the artifact collections. It is further an excellent infrastructure platform to deploy machine learning models. Inference routing through the tool Istio is currently state of the art in the field of machine learning infrastructure.<br>\n",
        "Setting up Kubernetes with a variety of cloud providers is also possible, which makes it very efficient and scalable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk99G4jtLXXH"
      },
      "source": [
        "### Kubeflow Pipelines on AI Platform\n",
        "\n",
        "Kubeflow pipelines can run on Google's AI Platform, which is part of GCP. This takes care of much of the infrastructure and lets you load data more easily from Google Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3kpYExZL-_B"
      },
      "source": [
        "## Converting Your Interactive TFX Pipeline to a Production Pipeline\n",
        "\n",
        "In order to automate our pipelines, we will need to write a Python script that will run all these components without any input from us.<br>\n",
        "Fortunately, we already have all the pieces of this script, here is a summary:\n",
        " - ExampleGen (Chapter 3)\n",
        " - StatisticsGen (Chapter 4)\n",
        " - SchemaGen (Chapter 4)\n",
        " - ExampleValidator (Chapter 4)\n",
        " - Transform (Chapter 5)\n",
        " - Trainer (chapter 6)\n",
        " - Resolver (Chapter 7)\n",
        " - Evaluator (Chapter 7)\n",
        " - Pusher (Chapter 7)\n",
        "\n",
        "Here is the full base pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHbnNdnzImKY"
      },
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.components import CsvExampleGen, Evaluator, ExampleValidator, Pusher, ResolverNode, SchemaGen, StatisticsGen, Trainer, Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.proto import pusher_pb2, trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO3hd8VcNJZu"
      },
      "source": [
        "def init_components(csv_data_dir, module_file, serving_model_dir,\n",
        "                   training_steps=2000, eval_steps=200):\n",
        "    \n",
        "    \n",
        "    \n",
        "    examples = external_input(csv_data_dir)\n",
        "    \n",
        "    example_gen = CsvExampleGen(input=examples)\n",
        "    \n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs['examples']\n",
        "    )\n",
        "    \n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        infer_feature_shape=True\n",
        "    )\n",
        "    \n",
        "    example_validator = ExampleValidator(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    transform = Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file=module_file\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        module_file=module_file,\n",
        "        # Override the executor to load the run_fn() function\n",
        "        # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=training_steps),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=eval_steps)\n",
        "    )\n",
        "\n",
        "    model_resolver = ResolverNode(\n",
        "        instance_name=\"latest_blessed_model_resolver\",\n",
        "        resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "        model=Channel(type=Model),\n",
        "        model_blessing=Channel(type=ModelBlessing)\n",
        "    )\n",
        "           \n",
        "    eval_config = tfma.EvalConfig(\n",
        "        model_specs=[tfma.ModelSpec(label_key=\"consumer_disputed\")],\n",
        "        slicing_specs=[\n",
        "            tfma.SlicingSpec(),\n",
        "            tfma.SlicingSpec(feature_keys=[\"product\"]),\n",
        "        ],\n",
        "        metrics_specs=[\n",
        "            tfma.MetricsSpec(\n",
        "                metrics=[\n",
        "                    tfma.MetricConfig(class_name=\"BinaryAccuracy\"),\n",
        "                    tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "                    tfma.MetricConfig(class_name=\"AUC\"),\n",
        "                    tfma.MetricConfig(class_name='Precision'),\n",
        "                    tfma.MetricConfig(class_name='Recall')\n",
        "                ],\n",
        "                thresholds={\n",
        "                    \"AUC\": tfma.config.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={\"value\": 0.65}\n",
        "                        ),\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={\"value\": 0.01},\n",
        "                        ),\n",
        "                    )\n",
        "                },\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs[\"examples\"],\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        baseline_model=model_resolver.outputs[\"model\"],\n",
        "        eval_config=eval_config\n",
        "    )\n",
        "\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        model_blessing=evaluator.outputs[\"blessing\"],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=serving_model_dir\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    components = [\n",
        "                  example_gen,\n",
        "                  statistics_gen,\n",
        "                  schema_gen,\n",
        "                  example_validator,\n",
        "                  transform,\n",
        "                  trainer,\n",
        "                  model_resolver,\n",
        "                  evaluator,\n",
        "                  pusher\n",
        "    ]\n",
        "\n",
        "    return components"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQyt44uNJuI"
      },
      "source": [
        "In the example project, we have split the component instantiation from the pipeline configuration to focus on the pipeline setup for the different orchestrators.<br>\n",
        "The *init_components* function instantiates the components, it requires three inputs in addition to the number of training steps and evaluation steps:\n",
        " - data_dir\n",
        " - module_file\n",
        " - serving_model_dir\n",
        "\n",
        "Besides the minor tweaks to the Google Cloud setup we will discuss in the next Chapter, the component setup will be identical for each orchestration platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfebwIlQ7Ml"
      },
      "source": [
        "## Simple Interactive Pipeline Conversion for Beam and Airflow\n",
        "\n",
        "You can convert a notebook to a pipeline via the following steps.<br>\n",
        "For any cells in your notebook that you do not want to export, use %%skip_for_export Jupyter magic command at the start of each cell.<br>\n",
        "1. Set the pipeline name and the orchestration tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Ys2oS-Nx40"
      },
      "source": [
        "# Alternative \"airflow\"\n",
        "runner_type = \"beam\"\n",
        "pipeline_name = \"consumer_complaints_beam\""
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8CckVKGRiU1"
      },
      "source": [
        "2. Set all the relevant file paths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7BAhboicCFu"
      },
      "source": [
        "import os"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jprX8RdHRhvT"
      },
      "source": [
        "notebook_file = chap_dir + \"Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow.ipynb\"\n",
        "\n",
        "# Pipeline inputs\n",
        "# Directories are all defined at the top of the notebook\n",
        "requirements_file = os.path.join(base_dir, \"requirements.txt\")\n",
        "module_file = base_dir + \"module.py\"\n",
        "serving_model_dir = out_dir + \"serving_model_dir/\"\n",
        "\n",
        "# Pipeline outputs\n",
        "output_base = os.path.join(out_dir, pipeline_name)\n",
        "serving_model_dir = os.path.join(output_base, pipeline_name)\n",
        "pipeline_root = os.path.join(output_base, \"pipeline_root\")\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdIl9nI6cDg6"
      },
      "source": [
        "3. List the components you wish to include in your pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofq5vjvb_GJ"
      },
      "source": [
        "components = [\n",
        "              example_gen,\n",
        "              statistics_gen,\n",
        "              schema_gen,\n",
        "              example_validator,\n",
        "              transform,\n",
        "              trainer,\n",
        "            #   model_resolver,\n",
        "              evaluator,\n",
        "              pusher\n",
        "    ]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwyy1OjhfPrg"
      },
      "source": [
        "4. Pipeline export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKEJDeCacU7w"
      },
      "source": [
        "pipeline_export_file = \"consumer_complaints_beam_export.py\"\n",
        "context.export_to_pipeline(\n",
        "    notebook_filepath=notebook_file,\n",
        "    export_filepath=pipeline_export_file,\n",
        "    runner_type=runner_type\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEbGRODYhVwe"
      },
      "source": [
        "This export command will generate a script that ca be run using Beam or Airflow depending on the *runner_type* you choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqC1oylahd1m"
      },
      "source": [
        "## Introduction to Apache Beam\n",
        "\n",
        "It this section, we will show how to orchestrate our example project using Beam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeyara26iFW4"
      },
      "source": [
        "### Orchestrating TFX Pipelines with Apache Beam\n",
        "\n",
        "Next to things mentioned in the beginning Beam can also be used to debugy our ML pipeline. By using Beam during your pipeline debugging and the nmoving to Airflow or Kubeflow Pipelines, you can rule out root causes of pipeline errors coming from the more complex Airflow or Kubeflow Pipeline setups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbrqRoqNhJvk"
      },
      "source": [
        "import absl\n",
        "from tfx.orchestration import metadata, pipeline"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OhCBQXWinCZ"
      },
      "source": [
        "def init_beam_pipeline(components, pipeline_root, direct_num_workers):\n",
        "    absl.logging.info(\"Pipeline root set to: {}\".format(pipeline_root))\n",
        "\n",
        "    beam_arg = [\n",
        "        # Beam lets you specify the number of workers\n",
        "        # A sensible default is half the numbers of avaiable CPUs if there is more tahn one CPU\n",
        "        \"--direct_num_workers={}\".format(direct_num_workers),\n",
        "        \"--requirements_file={}\".format(requirements_file)\n",
        "    ]\n",
        "\n",
        "    # This is where you define your pipeline object with a configuratio\n",
        "    p = pipeline.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=False, # We can set the cache to True if we would like to avoid rerunning components that have already finished\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
        "        metadata_pipeline_args=beam_arg\n",
        "    )\n",
        "\n",
        "    return p"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZHARQBULkb"
      },
      "source": [
        ""
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20PEupTj7k8"
      },
      "source": [
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7BSHeqUkBhK",
        "outputId": "0383e33d-3b4e-4bfc-8ab6-fe31becfc89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "direct_num_workers = int(os.cpu_count() / 2)\n",
        "direct_num_workers = 1 if direct_num_workers < 1 else direct_num_workers\n",
        "\n",
        "components = init_components(csv_data_dir, module_file, serving_model_dir,\n",
        "                            training_steps=1000, eval_steps=100)\n",
        "# print(components)\n",
        "pipe_line = init_beam_pipeline(components, pipeline_root, direct_num_workers)\n",
        "BeamDagRunner().run(pipe_line)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:12:27,927] {component.py:101} WARNING - The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
            "[2020-10-09 10:12:27,940] {component.py:88} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:12:27,950] {component.py:98} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:12:27,959] {component.py:96} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:12:27,970] {<ipython-input-41-c7e527094edd>:2} INFO - Pipeline root set to: drive/My Drive/Building ML Pipelines/Chapter 11/Outputs/consumer_complaints_beam/pipeline_root\n",
            "[2020-10-09 10:12:27,985] {pipeline.py:190} INFO - Missing pipeline option (runner). Executing pipeline using the default runner: DirectRunner.\n",
            "[2020-10-09 10:12:28,036] {beam_dag_runner.py:151} INFO - Component CsvExampleGen depends on [].\n",
            "[2020-10-09 10:12:28,066] {beam_dag_runner.py:165} INFO - Component CsvExampleGen is scheduled.\n",
            "[2020-10-09 10:12:28,082] {beam_dag_runner.py:151} INFO - Component ResolverNode.latest_blessed_model_resolver depends on [].\n",
            "[2020-10-09 10:12:28,109] {beam_dag_runner.py:165} INFO - Component ResolverNode.latest_blessed_model_resolver is scheduled.\n",
            "[2020-10-09 10:12:28,120] {beam_dag_runner.py:151} INFO - Component StatisticsGen depends on ['Run[CsvExampleGen]'].\n",
            "[2020-10-09 10:12:28,148] {beam_dag_runner.py:165} INFO - Component StatisticsGen is scheduled.\n",
            "[2020-10-09 10:12:28,158] {beam_dag_runner.py:151} INFO - Component SchemaGen depends on ['Run[StatisticsGen]'].\n",
            "[2020-10-09 10:12:28,185] {beam_dag_runner.py:165} INFO - Component SchemaGen is scheduled.\n",
            "[2020-10-09 10:12:28,195] {beam_dag_runner.py:151} INFO - Component ExampleValidator depends on ['Run[StatisticsGen]', 'Run[SchemaGen]'].\n",
            "[2020-10-09 10:12:28,229] {beam_dag_runner.py:165} INFO - Component ExampleValidator is scheduled.\n",
            "[2020-10-09 10:12:28,240] {beam_dag_runner.py:151} INFO - Component Transform depends on ['Run[SchemaGen]', 'Run[CsvExampleGen]'].\n",
            "[2020-10-09 10:12:28,267] {beam_dag_runner.py:165} INFO - Component Transform is scheduled.\n",
            "[2020-10-09 10:12:28,277] {beam_dag_runner.py:151} INFO - Component Trainer depends on ['Run[SchemaGen]', 'Run[Transform]'].\n",
            "[2020-10-09 10:12:28,305] {beam_dag_runner.py:165} INFO - Component Trainer is scheduled.\n",
            "[2020-10-09 10:12:28,315] {beam_dag_runner.py:151} INFO - Component Evaluator depends on ['Run[Trainer]', 'Run[ResolverNode.latest_blessed_model_resolver]', 'Run[CsvExampleGen]'].\n",
            "[2020-10-09 10:12:28,347] {beam_dag_runner.py:165} INFO - Component Evaluator is scheduled.\n",
            "[2020-10-09 10:12:28,364] {beam_dag_runner.py:151} INFO - Component Pusher depends on ['Run[Trainer]', 'Run[Evaluator]'].\n",
            "[2020-10-09 10:12:28,391] {beam_dag_runner.py:165} INFO - Component Pusher is scheduled.\n",
            "[2020-10-09 10:12:28,532] {translations.py:587} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f984621cd08> ====================\n",
            "[2020-10-09 10:12:28,547] {translations.py:587} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f984621ce18> ====================\n",
            "[2020-10-09 10:12:28,557] {translations.py:587} INFO - ==================== <function lift_combiners at 0x7f984621cea0> ====================\n",
            "[2020-10-09 10:12:28,570] {translations.py:587} INFO - ==================== <function expand_sdf at 0x7f984621cf28> ====================\n",
            "[2020-10-09 10:12:28,580] {translations.py:587} INFO - ==================== <function expand_gbk at 0x7f984621e048> ====================\n",
            "[2020-10-09 10:12:28,590] {translations.py:587} INFO - ==================== <function sink_flattens at 0x7f984621e158> ====================\n",
            "[2020-10-09 10:12:28,599] {translations.py:587} INFO - ==================== <function greedily_fuse at 0x7f984621e1e0> ====================\n",
            "[2020-10-09 10:12:28,613] {translations.py:587} INFO - ==================== <function read_to_impulse at 0x7f984621e268> ====================\n",
            "[2020-10-09 10:12:28,623] {translations.py:587} INFO - ==================== <function impulse_to_input at 0x7f984621e2f0> ====================\n",
            "[2020-10-09 10:12:28,632] {translations.py:587} INFO - ==================== <function sort_stages at 0x7f984621e510> ====================\n",
            "[2020-10-09 10:12:28,642] {translations.py:587} INFO - ==================== <function setup_timer_mapping at 0x7f984621e488> ====================\n",
            "[2020-10-09 10:12:28,652] {translations.py:587} INFO - ==================== <function populate_data_channel_coders at 0x7f984621e598> ====================\n",
            "[2020-10-09 10:12:28,665] {statecache.py:154} INFO - Creating state cache with size 100\n",
            "[2020-10-09 10:12:28,676] {worker_handlers.py:848} INFO - Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f98406db860> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, \"\\x1b[01mb''\\x1b[22m\")\n",
            "[2020-10-09 10:12:28,688] {fn_runner.py:489} INFO - Running (((((((ref_AppliedPTransform_CreateRoot/Impulse_3)+(ref_AppliedPTransform_CreateRoot/FlatMap(<lambda at core.py:2826>)_4))+(ref_AppliedPTransform_CreateRoot/Map(decode)_6))+(ref_AppliedPTransform_Run[CsvExampleGen]_7))+(ref_AppliedPTransform_Run[ResolverNode.latest_blessed_model_resolver]_8))+(ref_PCollection_PCollection_3/Write))+(ref_PCollection_PCollection_4/Write))+(ref_PCollection_PCollection_5/Write)\n",
            "[2020-10-09 10:12:28,767] {beam_dag_runner.py:87} INFO - Component CsvExampleGen is running.\n",
            "[2020-10-09 10:12:28,783] {base_component_launcher.py:195} INFO - Running driver for CsvExampleGen\n",
            "[2020-10-09 10:12:28,800] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:12:28,811] {utils.py:516} INFO - select span and version = (0, None)\n",
            "[2020-10-09 10:12:28,829] {utils.py:525} INFO - latest span and version = (0, None)\n",
            "[2020-10-09 10:12:28,939] {base_component_launcher.py:201} INFO - Running executor for CsvExampleGen\n",
            "[2020-10-09 10:12:28,952] {base_example_gen_executor.py:294} INFO - Generating examples.\n",
            "[2020-10-09 10:12:28,997] {executor.py:126} INFO - Processing input csv data drive/My Drive/Building ML Pipelines/CSV Data/* to TFExample.\n",
            "[2020-10-09 10:12:29,622] {translations.py:587} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f984621cd08> ====================\n",
            "[2020-10-09 10:12:29,649] {translations.py:587} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f984621ce18> ====================\n",
            "[2020-10-09 10:12:29,667] {translations.py:587} INFO - ==================== <function lift_combiners at 0x7f984621cea0> ====================\n",
            "[2020-10-09 10:12:29,682] {translations.py:587} INFO - ==================== <function expand_sdf at 0x7f984621cf28> ====================\n",
            "[2020-10-09 10:12:29,697] {translations.py:587} INFO - ==================== <function expand_gbk at 0x7f984621e048> ====================\n",
            "[2020-10-09 10:12:29,711] {translations.py:587} INFO - ==================== <function sink_flattens at 0x7f984621e158> ====================\n",
            "[2020-10-09 10:12:29,725] {translations.py:587} INFO - ==================== <function greedily_fuse at 0x7f984621e1e0> ====================\n",
            "[2020-10-09 10:12:29,742] {translations.py:587} INFO - ==================== <function read_to_impulse at 0x7f984621e268> ====================\n",
            "[2020-10-09 10:12:29,756] {translations.py:587} INFO - ==================== <function impulse_to_input at 0x7f984621e2f0> ====================\n",
            "[2020-10-09 10:12:29,767] {translations.py:587} INFO - ==================== <function sort_stages at 0x7f984621e510> ====================\n",
            "[2020-10-09 10:12:29,778] {translations.py:587} INFO - ==================== <function setup_timer_mapping at 0x7f984621e488> ====================\n",
            "[2020-10-09 10:12:29,789] {translations.py:587} INFO - ==================== <function populate_data_channel_coders at 0x7f984621e598> ====================\n",
            "[2020-10-09 10:12:29,809] {statecache.py:154} INFO - Creating state cache with size 100\n",
            "[2020-10-09 10:12:29,832] {worker_handlers.py:848} INFO - Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f983f9825c0> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, \"\\x1b[01mb''\\x1b[22m\")\n",
            "[2020-10-09 10:12:29,844] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_InputToRecord/ReadFromText/Read/_SDFBoundedSourceWrapper/Impulse_6)+(InputToRecord/ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(InputToRecord/ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "[2020-10-09 10:12:29,933] {fn_runner.py:489} INFO - Running (((((((ref_PCollection_PCollection_1_split/Read)+(InputToRecord/ReadFromText/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_InputToRecord/ParseCSVLine_8))+(ref_AppliedPTransform_InputToRecord/ExtractParsedCSVLines_9))+(ref_AppliedPTransform_InputToRecord/InferColumnTypes/KeyWithVoid_11))+(ref_PCollection_PCollection_4/Write))+(InputToRecord/InferColumnTypes/CombinePerKey/Precombine))+(InputToRecord/InferColumnTypes/CombinePerKey/Group/Write)\n",
            "[2020-10-09 10:12:35,625] {fn_runner.py:489} INFO - Running ((((InputToRecord/InferColumnTypes/CombinePerKey/Group/Read)+(InputToRecord/InferColumnTypes/CombinePerKey/Merge))+(InputToRecord/InferColumnTypes/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_InputToRecord/InferColumnTypes/UnKey_16))+(ref_PCollection_PCollection_8/Write)\n",
            "[2020-10-09 10:12:35,661] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_InputToRecord/InferColumnTypes/DoOnce/Impulse_18)+(ref_AppliedPTransform_InputToRecord/InferColumnTypes/DoOnce/FlatMap(<lambda at core.py:2826>)_19))+(ref_AppliedPTransform_InputToRecord/InferColumnTypes/DoOnce/Map(decode)_21))+(ref_AppliedPTransform_InputToRecord/InferColumnTypes/InjectDefault_22))+(ref_PCollection_PCollection_12/Write)\n",
            "[2020-10-09 10:12:35,700] {fn_runner.py:489} INFO - Running ((((((((((ref_PCollection_PCollection_4/Read)+(ref_AppliedPTransform_InputToRecord/ToTFExample_23))+(ref_AppliedPTransform_SplitData/ParDo(ApplyPartitionFnFn)/ParDo(ApplyPartitionFnFn)_26))+(ref_AppliedPTransform_WriteSplit[train]/MaybeSerialize_28))+(ref_AppliedPTransform_WriteSplit[eval]/MaybeSerialize_53))+(ref_AppliedPTransform_WriteSplit[train]/Shuffle/AddRandomKeys_30))+(ref_AppliedPTransform_WriteSplit[train]/Shuffle/ReshufflePerKey/Map(reify_timestamps)_32))+(WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Write))+(ref_AppliedPTransform_WriteSplit[eval]/Shuffle/AddRandomKeys_55))+(ref_AppliedPTransform_WriteSplit[eval]/Shuffle/ReshufflePerKey/Map(reify_timestamps)_57))+(WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Write)\n",
            "[2020-10-09 10:12:47,724] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/DoOnce/Impulse_40)+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_41))+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/DoOnce/Map(decode)_43))+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/InitializeWrite_44))+(ref_PCollection_PCollection_25/Write))+(ref_PCollection_PCollection_26/Write)\n",
            "[2020-10-09 10:12:47,772] {fn_runner.py:489} INFO - Running ((((((WriteSplit[train]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit[train]/Shuffle/ReshufflePerKey/FlatMap(restore_timestamps)_34))+(ref_AppliedPTransform_WriteSplit[train]/Shuffle/RemoveRandomKeys_35))+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_45))+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/WriteBundles_46))+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/Pair_47))+(WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:12:54,431] {fn_runner.py:489} INFO - Running ((WriteSplit[train]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/Extract_49))+(ref_PCollection_PCollection_31/Write)\n",
            "[2020-10-09 10:12:54,455] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/PreFinalize_50))+(ref_PCollection_PCollection_32/Write)\n",
            "[2020-10-09 10:12:54,494] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/DoOnce/Impulse_65)+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_66))+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/DoOnce/Map(decode)_68))+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/InitializeWrite_69))+(ref_PCollection_PCollection_42/Write))+(ref_PCollection_PCollection_43/Write)\n",
            "[2020-10-09 10:12:54,576] {fn_runner.py:489} INFO - Running ((((((WriteSplit[eval]/Shuffle/ReshufflePerKey/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit[eval]/Shuffle/ReshufflePerKey/FlatMap(restore_timestamps)_59))+(ref_AppliedPTransform_WriteSplit[eval]/Shuffle/RemoveRandomKeys_60))+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_70))+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/WriteBundles_71))+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/Pair_72))+(WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:12:58,893] {fn_runner.py:489} INFO - Running ((WriteSplit[eval]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/Extract_74))+(ref_PCollection_PCollection_48/Write)\n",
            "[2020-10-09 10:12:58,924] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/PreFinalize_75))+(ref_PCollection_PCollection_49/Write)\n",
            "[2020-10-09 10:12:58,954] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_42/Read)+(ref_AppliedPTransform_WriteSplit[eval]/Write/Write/WriteImpl/FinalizeWrite_76)\n",
            "[2020-10-09 10:12:58,981] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:12:59,098] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:12:59,127] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_25/Read)+(ref_AppliedPTransform_WriteSplit[train]/Write/Write/WriteImpl/FinalizeWrite_51)\n",
            "[2020-10-09 10:12:59,190] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:12:59,300] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:12:59,349] {base_example_gen_executor.py:311} INFO - Examples generated.\n",
            "[2020-10-09 10:12:59,361] {base_component_launcher.py:208} INFO - Running publisher for CsvExampleGen\n",
            "[2020-10-09 10:12:59,378] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:12:59,430] {beam_dag_runner.py:89} INFO - Component CsvExampleGen is finished.\n",
            "[2020-10-09 10:12:59,446] {beam_dag_runner.py:87} INFO - Component ResolverNode.latest_blessed_model_resolver is running.\n",
            "[2020-10-09 10:12:59,467] {base_component_launcher.py:195} INFO - Running driver for ResolverNode.latest_blessed_model_resolver\n",
            "[2020-10-09 10:12:59,509] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:12:59,593] {base_component_launcher.py:208} INFO - Running publisher for ResolverNode.latest_blessed_model_resolver\n",
            "[2020-10-09 10:12:59,621] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:12:59,640] {beam_dag_runner.py:89} INFO - Component ResolverNode.latest_blessed_model_resolver is finished.\n",
            "[2020-10-09 10:12:59,730] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_3/Read)+(ref_AppliedPTransform_Run[StatisticsGen]_9))+(ref_PCollection_PCollection_6/Write)\n",
            "[2020-10-09 10:12:59,759] {beam_dag_runner.py:87} INFO - Component StatisticsGen is running.\n",
            "[2020-10-09 10:12:59,783] {base_component_launcher.py:195} INFO - Running driver for StatisticsGen\n",
            "[2020-10-09 10:12:59,804] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:12:59,872] {base_component_launcher.py:201} INFO - Running executor for StatisticsGen\n",
            "[2020-10-09 10:12:59,905] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:12:59,919] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:12:59,951] {executor.py:138} INFO - Generating statistics for split train.\n",
            "[2020-10-09 10:13:00,296] {executor.py:150} INFO - Statistics for split train written to drive/My Drive/Building ML Pipelines/Chapter 11/Outputs/consumer_complaints_beam/pipeline_root/StatisticsGen/statistics/10/train.\n",
            "[2020-10-09 10:13:00,324] {executor.py:138} INFO - Generating statistics for split eval.\n",
            "[2020-10-09 10:13:00,665] {executor.py:150} INFO - Statistics for split eval written to drive/My Drive/Building ML Pipelines/Chapter 11/Outputs/consumer_complaints_beam/pipeline_root/StatisticsGen/statistics/10/eval.\n",
            "[2020-10-09 10:13:01,132] {translations.py:587} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f984621cd08> ====================\n",
            "[2020-10-09 10:13:01,170] {translations.py:587} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f984621ce18> ====================\n",
            "[2020-10-09 10:13:01,199] {translations.py:587} INFO - ==================== <function lift_combiners at 0x7f984621cea0> ====================\n",
            "[2020-10-09 10:13:01,227] {translations.py:587} INFO - ==================== <function expand_sdf at 0x7f984621cf28> ====================\n",
            "[2020-10-09 10:13:01,252] {translations.py:587} INFO - ==================== <function expand_gbk at 0x7f984621e048> ====================\n",
            "[2020-10-09 10:13:01,271] {translations.py:587} INFO - ==================== <function sink_flattens at 0x7f984621e158> ====================\n",
            "[2020-10-09 10:13:01,288] {translations.py:587} INFO - ==================== <function greedily_fuse at 0x7f984621e1e0> ====================\n",
            "[2020-10-09 10:13:01,322] {translations.py:587} INFO - ==================== <function read_to_impulse at 0x7f984621e268> ====================\n",
            "[2020-10-09 10:13:01,340] {translations.py:587} INFO - ==================== <function impulse_to_input at 0x7f984621e2f0> ====================\n",
            "[2020-10-09 10:13:01,356] {translations.py:587} INFO - ==================== <function sort_stages at 0x7f984621e510> ====================\n",
            "[2020-10-09 10:13:01,371] {translations.py:587} INFO - ==================== <function setup_timer_mapping at 0x7f984621e488> ====================\n",
            "[2020-10-09 10:13:01,386] {translations.py:587} INFO - ==================== <function populate_data_channel_coders at 0x7f984621e598> ====================\n",
            "[2020-10-09 10:13:01,456] {statecache.py:154} INFO - Creating state cache with size 100\n",
            "[2020-10-09 10:13:01,488] {worker_handlers.py:848} INFO - Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f984186b978> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, \"\\x1b[01mb''\\x1b[22m\")\n",
            "[2020-10-09 10:13:01,526] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/Impulse_105)+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_56_split/Write)\n",
            "[2020-10-09 10:13:01,615] {fn_runner.py:489} INFO - Running ((((((((((((((((ref_PCollection_PCollection_56_split/Read)+(TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead[eval]/RawRecordBeamSource/ReadRawRecords/FlattenPCollsFromPatterns_107))+(ref_AppliedPTransform_TFXIORead[eval]/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords_109))+(ref_AppliedPTransform_TFXIORead[eval]/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)_113))+(ref_AppliedPTransform_TFXIORead[eval]/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode_114))+(ref_AppliedPTransform_TFXIORead[eval]/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches_116))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/KeyWithVoid_119))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/ToTopKTuples_122))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/ParDo(SplitHotCold)/ParDo(SplitHotCold)_146))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoDiscarding_147))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
            "[2020-10-09 10:13:03,506] {fn_runner.py:489} INFO - Running (((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Map(StripNonce)_152))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoOriginal_153))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
            "[2020-10-09 10:13:03,590] {fn_runner.py:489} INFO - Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
            "[2020-10-09 10:13:03,638] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
            "[2020-10-09 10:13:03,701] {fn_runner.py:489} INFO - Running (((((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Rearrange_127))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_Keys_135))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_137))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
            "[2020-10-09 10:13:04,145] {fn_runner.py:489} INFO - Running (((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_ToFeatureValueCount_133))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_ToProto_134))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
            "[2020-10-09 10:13:04,223] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_142))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
            "[2020-10-09 10:13:04,259] {fn_runner.py:489} INFO - Running (GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
            "[2020-10-09 10:13:04,298] {fn_runner.py:489} INFO - Running ((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
            "[2020-10-09 10:13:04,392] {fn_runner.py:489} INFO - Running ((((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_164))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_167))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
            "[2020-10-09 10:13:04,490] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/DoOnce/Impulse_185)+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_186))+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/DoOnce/Map(decode)_188))+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/InitializeWrite_189))+(ref_PCollection_PCollection_103/Write))+(ref_PCollection_PCollection_104/Write)\n",
            "[2020-10-09 10:13:04,539] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_172))+(ref_PCollection_PCollection_95/Write)\n",
            "[2020-10-09 10:13:04,606] {fn_runner.py:489} INFO - Running (((((((ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Impulse_174)+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/FlatMap(<lambda at core.py:2826>)_175))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Map(decode)_177))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_178))+(ref_AppliedPTransform_GenerateStatistics[eval]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_179))+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_190))+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/WindowInto(WindowIntoFn)_191))+(WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:04,693] {fn_runner.py:489} INFO - Running ((WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/WriteBundles_193))+(ref_PCollection_PCollection_108/Write)\n",
            "[2020-10-09 10:13:04,756] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_103/Read)+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/PreFinalize_194))+(ref_PCollection_PCollection_109/Write)\n",
            "[2020-10-09 10:13:04,784] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/Impulse_8)+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_1_split/Write)\n",
            "[2020-10-09 10:13:04,839] {fn_runner.py:489} INFO - Running ((((((((((((((((ref_PCollection_PCollection_1_split/Read)+(TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIORead[train]/RawRecordBeamSource/ReadRawRecords/FlattenPCollsFromPatterns_10))+(ref_AppliedPTransform_TFXIORead[train]/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords_12))+(ref_AppliedPTransform_TFXIORead[train]/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)_16))+(ref_AppliedPTransform_TFXIORead[train]/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode_17))+(ref_AppliedPTransform_TFXIORead[train]/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches_19))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/KeyWithVoid_22))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/ToTopKTuples_25))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/ParDo(SplitHotCold)/ParDo(SplitHotCold)_49))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Write))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoDiscarding_50))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Transcode/0))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Write))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/0)\n",
            "[2020-10-09 10:13:08,389] {fn_runner.py:489} INFO - Running (((((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/CombineCountsAndWeights/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Rearrange_30))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Precombine))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_Keys_38))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Write))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/Uniques_CountPerFeatureName:PairWithVoid_40))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Write)\n",
            "[2020-10-09 10:13:09,305] {fn_runner.py:489} INFO - Running (((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_TopK/CombinePerKey(TopCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_ToFeatureValueCount_36))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Unweighted_ToProto_37))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/0)\n",
            "[2020-10-09 10:13:09,386] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_CountPerFeatureName/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/Uniques_ConvertToSingleFeatureStats_45))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Write/1)\n",
            "[2020-10-09 10:13:09,424] {fn_runner.py:489} INFO - Running (GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/TopKUniquesStatsGenerator/FlattenTopKUniquesFeatureStatsProtos/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/0)\n",
            "[2020-10-09 10:13:09,471] {fn_runner.py:489} INFO - Running (((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Map(StripNonce)_55))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoOriginal_56))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Write/1)\n",
            "[2020-10-09 10:13:09,564] {fn_runner.py:489} INFO - Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/Flatten/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Write)\n",
            "[2020-10-09 10:13:09,611] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/CombinePerKey(PostCombineFn)/ExtractOutputs))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Transcode/1))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Write/1)\n",
            "[2020-10-09 10:13:09,705] {fn_runner.py:489} INFO - Running ((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/FlattenFeatureStatistics/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Write)\n",
            "[2020-10-09 10:13:09,771] {fn_runner.py:489} INFO - Running ((((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MergeDatasetFeatureStatisticsProtos/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/AddSliceKeyToStatsProto_67))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/KeyWithVoid_70))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Precombine))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Write)\n",
            "[2020-10-09 10:13:09,868] {fn_runner.py:489} INFO - Running ((((GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Group/Read)+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/Merge))+(GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/UnKey_75))+(ref_PCollection_PCollection_40/Write)\n",
            "[2020-10-09 10:13:09,933] {fn_runner.py:489} INFO - Running (((((((ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Impulse_77)+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/FlatMap(<lambda at core.py:2826>)_78))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/DoOnce/Map(decode)_80))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/ToList/ToList/InjectDefault_81))+(ref_AppliedPTransform_GenerateStatistics[train]/RunStatsGenerators/GenerateSlicedStatisticsImpl/MakeDatasetFeatureStatisticsListProto_82))+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_93))+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/WindowInto(WindowIntoFn)_94))+(WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:10,022] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/DoOnce/Impulse_88)+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_89))+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/DoOnce/Map(decode)_91))+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/InitializeWrite_92))+(ref_PCollection_PCollection_48/Write))+(ref_PCollection_PCollection_49/Write)\n",
            "[2020-10-09 10:13:10,076] {fn_runner.py:489} INFO - Running ((WriteStatsOutput[train]/WriteStats/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/WriteBundles_96))+(ref_PCollection_PCollection_53/Write)\n",
            "[2020-10-09 10:13:10,135] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_48/Read)+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/PreFinalize_97))+(ref_PCollection_PCollection_54/Write)\n",
            "[2020-10-09 10:13:10,171] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_48/Read)+(ref_AppliedPTransform_WriteStatsOutput[train]/WriteStats/Write/WriteImpl/FinalizeWrite_98)\n",
            "[2020-10-09 10:13:10,205] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:10,320] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:10,350] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_103/Read)+(ref_AppliedPTransform_WriteStatsOutput[eval]/WriteStats/Write/WriteImpl/FinalizeWrite_195)\n",
            "[2020-10-09 10:13:10,383] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:10,500] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.11 seconds.\n",
            "[2020-10-09 10:13:10,541] {base_component_launcher.py:208} INFO - Running publisher for StatisticsGen\n",
            "[2020-10-09 10:13:10,570] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:10,606] {beam_dag_runner.py:89} INFO - Component StatisticsGen is finished.\n",
            "[2020-10-09 10:13:10,717] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_3/Read)+(ref_AppliedPTransform_Run[SchemaGen]_10))+(ref_PCollection_PCollection_7/Write)\n",
            "[2020-10-09 10:13:10,746] {beam_dag_runner.py:87} INFO - Component SchemaGen is running.\n",
            "[2020-10-09 10:13:10,770] {base_component_launcher.py:195} INFO - Running driver for SchemaGen\n",
            "[2020-10-09 10:13:10,797] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:10,862] {base_component_launcher.py:201} INFO - Running executor for SchemaGen\n",
            "[2020-10-09 10:13:10,885] {executor.py:94} INFO - Processing schema from statistics for split train.\n",
            "[2020-10-09 10:13:10,929] {executor.py:94} INFO - Processing schema from statistics for split eval.\n",
            "[2020-10-09 10:13:10,988] {executor.py:108} INFO - Schema written to drive/My Drive/Building ML Pipelines/Chapter 11/Outputs/consumer_complaints_beam/pipeline_root/SchemaGen/schema/11/schema.pbtxt.\n",
            "[2020-10-09 10:13:11,005] {base_component_launcher.py:208} INFO - Running publisher for SchemaGen\n",
            "[2020-10-09 10:13:11,019] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:11,079] {beam_dag_runner.py:89} INFO - Component SchemaGen is finished.\n",
            "[2020-10-09 10:13:11,139] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_3/Read)+(ref_AppliedPTransform_Run[Transform]_12))+(ref_PCollection_PCollection_9/Write)\n",
            "[2020-10-09 10:13:11,178] {beam_dag_runner.py:87} INFO - Component Transform is running.\n",
            "[2020-10-09 10:13:11,199] {base_component_launcher.py:195} INFO - Running driver for Transform\n",
            "[2020-10-09 10:13:11,227] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:11,304] {base_component_launcher.py:201} INFO - Running executor for Transform\n",
            "[2020-10-09 10:13:11,331] {executor.py:337} INFO - Analyze the 'train' split and transform all splits when splits_config is not set.\n",
            "[2020-10-09 10:13:11,371] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:13:11,384] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:13:11,396] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:13:11,416] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,436] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,452] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,470] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,487] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,513] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,530] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,539] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,554] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,580] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,591] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,840] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,868] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,886] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,901] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,926] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,939] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,949] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,959] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:11,978] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:11,992] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,005] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,340] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,372] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,384] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,395] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,405] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,415] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,423] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,431] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,439] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,446] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,455] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,465] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:13:12,472] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,482] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,492] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,501] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,512] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,521] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,527] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,535] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,543] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,553] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,561] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,573] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,583] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,591] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,601] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,611] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,621] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,655] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,663] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,671] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,681] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,689] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,698] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n",
            "[2020-10-09 10:13:12,704] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,715] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,725] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,735] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,746] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,752] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,763] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,771] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,780] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,791] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:12,849] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:12,859] {tf_example_record.py:55} INFO - We decided to produce LargeList and LargeBinary types.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "TFT beam APIs accept both the TFXIO format and the instance dict format now. There is no need to set use_tfxio any more and it will be removed soon.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:12,976] {decorators.py:433} WARNING - This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n",
            "[2020-10-09 10:13:13,459] {decorators.py:433} WARNING - This output type hint will be ignored and not used for type-checking purposes. Typically, output type hints for a PTransform are single (or nested) types wrapped by a PCollection, PDone, or None. Got: Tuple[Dict[str, Union[NoneType, _Dataset]], Union[Dict[str, Dict[str, PCollection]], NoneType]] instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n",
            "Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n",
            "Issue encountered when serializing tft_mapper_use.\n",
            "Type is unsupported, or the types of the items don't match field type in CollectionDef. Note this is a warning and probably safe to ignore.\n",
            "'Counter' object has no attribute 'name'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:16,052] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,081] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,097] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,115] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,130] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,151] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,159] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,170] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,181] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,194] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,205] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:16,370] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,382] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,391] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,402] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,416] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,426] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,521] {tensor_representation_util.py:233} INFO - Feature company has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,537] {tensor_representation_util.py:233} INFO - Feature company_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,549] {tensor_representation_util.py:233} INFO - Feature consumer_complaint_narrative has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,561] {tensor_representation_util.py:233} INFO - Feature issue has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,576] {tensor_representation_util.py:233} INFO - Feature product has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,586] {tensor_representation_util.py:239} INFO - Feature state has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,595] {tensor_representation_util.py:239} INFO - Feature sub_issue has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,604] {tensor_representation_util.py:239} INFO - Feature sub_product has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,612] {tensor_representation_util.py:233} INFO - Feature timely_response has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n",
            "[2020-10-09 10:13:16,621] {tensor_representation_util.py:239} INFO - Feature zip_code has no shape. Setting to VarLenSparseTensor.\n",
            "[2020-10-09 10:13:16,629] {tensor_representation_util.py:233} INFO - Feature consumer_disputed has a shape dim {\n",
            "  size: 1\n",
            "}\n",
            ". Setting to DenseTensor.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version (2.3.0) found. Note that Tensorflow Transform support for TF 2.0 is currently in beta, and features such as tf.function may not work as intended. \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:16,768] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,777] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,785] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,791] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,800] {typehints.py:1107} WARNING - Ignoring send_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:16,806] {typehints.py:1109} WARNING - Ignoring return_type hint: <class 'NoneType'>\n",
            "[2020-10-09 10:13:18,769] {translations.py:587} INFO - ==================== <function annotate_downstream_side_inputs at 0x7f984621cd08> ====================\n",
            "[2020-10-09 10:13:18,816] {translations.py:587} INFO - ==================== <function fix_side_input_pcoll_coders at 0x7f984621ce18> ====================\n",
            "[2020-10-09 10:13:18,851] {translations.py:587} INFO - ==================== <function lift_combiners at 0x7f984621cea0> ====================\n",
            "[2020-10-09 10:13:18,899] {translations.py:587} INFO - ==================== <function expand_sdf at 0x7f984621cf28> ====================\n",
            "[2020-10-09 10:13:18,936] {translations.py:587} INFO - ==================== <function expand_gbk at 0x7f984621e048> ====================\n",
            "[2020-10-09 10:13:18,975] {translations.py:587} INFO - ==================== <function sink_flattens at 0x7f984621e158> ====================\n",
            "[2020-10-09 10:13:19,008] {translations.py:587} INFO - ==================== <function greedily_fuse at 0x7f984621e1e0> ====================\n",
            "[2020-10-09 10:13:19,070] {translations.py:587} INFO - ==================== <function read_to_impulse at 0x7f984621e268> ====================\n",
            "[2020-10-09 10:13:19,093] {translations.py:587} INFO - ==================== <function impulse_to_input at 0x7f984621e2f0> ====================\n",
            "[2020-10-09 10:13:19,118] {translations.py:587} INFO - ==================== <function sort_stages at 0x7f984621e510> ====================\n",
            "[2020-10-09 10:13:19,137] {translations.py:587} INFO - ==================== <function setup_timer_mapping at 0x7f984621e488> ====================\n",
            "[2020-10-09 10:13:19,160] {translations.py:587} INFO - ==================== <function populate_data_channel_coders at 0x7f984621e598> ====================\n",
            "[2020-10-09 10:13:19,268] {statecache.py:154} INFO - Creating state cache with size 100\n",
            "[2020-10-09 10:13:19,282] {worker_handlers.py:848} INFO - Created Worker handler <apache_beam.runners.portability.fn_api_runner.worker_handlers.EmbeddedWorkerHandler object at 0x7f98402cb240> for environment ref_Environment_default_environment_1 (beam:env:embedded_python:v1, \"\\x1b[01mb''\\x1b[22m\")\n",
            "[2020-10-09 10:13:19,300] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/BindTensors/CreateSavedModel/Impulse_39)+(ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/BindTensors/CreateSavedModel/FlatMap(<lambda at core.py:2826>)_40))+(ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/BindTensors/CreateSavedModel/Map(decode)_42))+(ref_PCollection_PCollection_18/Write)\n",
            "[2020-10-09 10:13:19,344] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/Impulse_16)+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_5_split/Write)\n",
            "[2020-10-09 10:13:19,404] {fn_runner.py:489} INFO - Running (((((((((((((((((((((((((((((((((((((((((ref_PCollection_PCollection_5_split/Read)+(TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/ReadRawRecords/FlattenPCollsFromPatterns_18))+(ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords_20))+(ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)_24))+(ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode_25))+(ref_AppliedPTransform_TFXIOReadAndDecode[AnalysisIndex0]/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches_27))+(ref_AppliedPTransform_Analyze/ExtractInputForSavedModel[AnalysisIndex0]/Identity_51))+(ref_AppliedPTransform_Analyze/ApplySavedModel[Phase0][AnalysisIndex0]/ApplySavedModel_53))+(ref_AppliedPTransform_Analyze/TensorSource[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/ExtractKeys_55))+(ref_AppliedPTransform_Analyze/TensorSource[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/ExtractKeys_136))+(ref_AppliedPTransform_Analyze/TensorSource[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/ExtractKeys_217))+(ref_AppliedPTransform_Analyze/TensorSource[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/ExtractKeys_298))+(ref_AppliedPTransform_Analyze/TensorSource[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/ExtractKeys_379))+(ref_AppliedPTransform_Analyze/TensorSource[bucketize/quantiles][AnalysisIndex0]/ExtractKeys_460))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/FlattenTokensAndMaybeWeightsLabels_57))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CountPerToken:PairWithVoid_59))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/FlattenTokensAndMaybeWeightsLabels_138))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CountPerToken:PairWithVoid_140))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/FlattenTokensAndMaybeWeightsLabels_219))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CountPerToken:PairWithVoid_221))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/FlattenTokensAndMaybeWeightsLabels_300))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CountPerToken:PairWithVoid_302))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/FlattenTokensAndMaybeWeightsLabels_381))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CountPerToken:PairWithVoid_383))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Precombine))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Write))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/KeyWithVoid_463))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/ParDo(SplitHotCold)/ParDo(SplitHotCold)_466))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/WindowIntoDiscarding_467))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/Flatten/Transcode/0))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Precombine))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Write))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/Flatten/Write/0)\n",
            "[2020-10-09 10:13:22,299] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/DoOnce/Impulse_606)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_607))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/DoOnce/Map(decode)_609))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/InitializeWrite_610))+(ref_PCollection_PCollection_350/Write))+(ref_PCollection_PCollection_351/Write)\n",
            "[2020-10-09 10:13:22,346] {fn_runner.py:489} INFO - Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary][AnalysisIndex0]/FilterProblematicStrings_64))+(ref_AppliedPTransform_Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary/vocabulary]]/Flatten_66))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]][AnalysisIndex0]/Encode_523))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/WindowInto(WindowIntoFn)_611))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/WriteBundles_612))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/Pair_613))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:22,407] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/Extract_615))+(ref_PCollection_PCollection_356/Write)\n",
            "[2020-10-09 10:13:22,441] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_350/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/PreFinalize_616))+(ref_PCollection_PCollection_357/Write)\n",
            "[2020-10-09 10:13:22,480] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/DoOnce/Impulse_622)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_623))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/DoOnce/Map(decode)_625))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/InitializeWrite_626))+(ref_PCollection_PCollection_361/Write))+(ref_PCollection_PCollection_362/Write)\n",
            "[2020-10-09 10:13:22,521] {fn_runner.py:489} INFO - Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary][AnalysisIndex0]/FilterProblematicStrings_145))+(ref_AppliedPTransform_Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]]/Flatten_147))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]][AnalysisIndex0]/Encode_532))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/WindowInto(WindowIntoFn)_627))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/WriteBundles_628))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/Pair_629))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:22,591] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/Extract_631))+(ref_PCollection_PCollection_367/Write)\n",
            "[2020-10-09 10:13:22,622] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_361/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/PreFinalize_632))+(ref_PCollection_PCollection_368/Write)\n",
            "[2020-10-09 10:13:22,661] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_361/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex1]/Write/WriteImpl/FinalizeWrite_633)\n",
            "[2020-10-09 10:13:22,699] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:22,816] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.11 seconds.\n",
            "[2020-10-09 10:13:22,844] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/DoOnce/Impulse_638)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_639))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/DoOnce/Map(decode)_641))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/InitializeWrite_642))+(ref_PCollection_PCollection_372/Write))+(ref_PCollection_PCollection_373/Write)\n",
            "[2020-10-09 10:13:22,890] {fn_runner.py:489} INFO - Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary][AnalysisIndex0]/FilterProblematicStrings_226))+(ref_AppliedPTransform_Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]]/Flatten_228))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary]][AnalysisIndex0]/Encode_541))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/CountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/CountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/WindowInto(WindowIntoFn)_643))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/WriteBundles_644))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/Pair_645))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:22,954] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/Extract_647))+(ref_PCollection_PCollection_378/Write)\n",
            "[2020-10-09 10:13:22,981] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_372/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/PreFinalize_648))+(ref_PCollection_PCollection_379/Write)\n",
            "[2020-10-09 10:13:23,024] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_372/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex2]/Write/WriteImpl/FinalizeWrite_649)\n",
            "[2020-10-09 10:13:23,064] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:23,184] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.11 seconds.\n",
            "[2020-10-09 10:13:23,208] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/DoOnce/Impulse_654)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_655))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/DoOnce/Map(decode)_657))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/InitializeWrite_658))+(ref_PCollection_PCollection_383/Write))+(ref_PCollection_PCollection_384/Write)\n",
            "[2020-10-09 10:13:23,254] {fn_runner.py:489} INFO - Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary][AnalysisIndex0]/FilterProblematicStrings_307))+(ref_AppliedPTransform_Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]]/Flatten_309))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary]][AnalysisIndex0]/Encode_550))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/CountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/CountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/WindowInto(WindowIntoFn)_659))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/WriteBundles_660))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/Pair_661))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:23,570] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/Extract_663))+(ref_PCollection_PCollection_389/Write)\n",
            "[2020-10-09 10:13:23,603] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_383/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/PreFinalize_664))+(ref_PCollection_PCollection_390/Write)\n",
            "[2020-10-09 10:13:23,641] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_383/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex3]/Write/WriteImpl/FinalizeWrite_665)\n",
            "[2020-10-09 10:13:23,675] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:23,792] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:23,846] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/DoOnce/Impulse_670)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_671))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/DoOnce/Map(decode)_673))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/InitializeWrite_674))+(ref_PCollection_PCollection_394/Write))+(ref_PCollection_PCollection_395/Write)\n",
            "[2020-10-09 10:13:23,894] {fn_runner.py:489} INFO - Running (((((((((((Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Group/Read)+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/Merge))+(Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/CountPerToken/CombinePerKey(CountCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary][AnalysisIndex0]/FilterProblematicStrings_388))+(ref_AppliedPTransform_Analyze/FlattenCache[VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]]/Flatten_390))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary]][AnalysisIndex0]/Encode_559))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/CountPerToken/Precombine))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/CountPerToken/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/WindowInto(WindowIntoFn)_675))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/WriteBundles_676))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/Pair_677))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:23,973] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/Extract_679))+(ref_PCollection_PCollection_400/Write)\n",
            "[2020-10-09 10:13:24,005] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_394/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/PreFinalize_680))+(ref_PCollection_PCollection_401/Write)\n",
            "[2020-10-09 10:13:24,046] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_394/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex4]/Write/WriteImpl/FinalizeWrite_681)\n",
            "[2020-10-09 10:13:24,081] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:24,201] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:24,253] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/InstrumentAPI/CreateSoleAPIUse/Impulse_31)+(ref_AppliedPTransform_Analyze/InstrumentAPI/CreateSoleAPIUse/FlatMap(<lambda at core.py:2826>)_32))+(ref_AppliedPTransform_Analyze/InstrumentAPI/CreateSoleAPIUse/Map(decode)_34))+(ref_AppliedPTransform_Analyze/InstrumentAPI/CountAPIUse_35)\n",
            "[2020-10-09 10:13:24,308] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/Count/CreateSole/Impulse_45)+(ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_46))+(ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/Count/CreateSole/Map(decode)_48))+(ref_AppliedPTransform_Analyze/CreateSavedModelForAnalyzerInputs[Phase0]/Count/Count_49)\n",
            "[2020-10-09 10:13:24,350] {fn_runner.py:489} INFO - Running ((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/CountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyMerge[compute_and_apply_vocabulary/vocabulary]/SwapTokensAndCounts_72))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid_76))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric_93))+(Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/ParDo(_TopPerBundle)_95))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Flatten/Transcode/0))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Flatten/Write/0)\n",
            "[2020-10-09 10:13:24,393] {fn_runner.py:489} INFO - Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey_81))+(ref_PCollection_PCollection_38/Write)\n",
            "[2020-10-09 10:13:24,425] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Impulse_83)+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/FlatMap(<lambda at core.py:2826>)_84))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Map(decode)_86))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault_87))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary/vocabulary]/ToInt64_88))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/vocab_compute_and_apply_vocabulary_vocabulary_unpruned_vocab_size]/ToTensorBinding_90))+(ref_PCollection_PCollection_44/Write)\n",
            "[2020-10-09 10:13:24,472] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Create/Impulse_97)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Create/FlatMap(<lambda at core.py:2826>)_98))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Create/Map(decode)_100))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Flatten/Transcode/1))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Flatten/Write/1)\n",
            "[2020-10-09 10:13:24,519] {fn_runner.py:489} INFO - Running (Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/Flatten/Read)+(Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/GroupByKey/Write)\n",
            "[2020-10-09 10:13:24,548] {fn_runner.py:489} INFO - Running (((Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/Top(12)/ParDo(_MergeTopPerBundle)_103))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary/vocabulary]/ApplyThresholdsAndTopK/FlattenList_104))+(ref_PCollection_PCollection_53/Write)\n",
            "[2020-10-09 10:13:24,583] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/Impulse_704)+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_414_split/Write)\n",
            "[2020-10-09 10:13:24,638] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Impulse_278)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_279))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Map(decode)_281))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite_282))+(ref_PCollection_PCollection_156/Write))+(ref_PCollection_PCollection_157/Write)\n",
            "[2020-10-09 10:13:24,683] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Create/Impulse_259)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Create/FlatMap(<lambda at core.py:2826>)_260))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Create/Map(decode)_262))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Flatten/Transcode/1))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Flatten/Write/1)\n",
            "[2020-10-09 10:13:24,723] {fn_runner.py:489} INFO - Running ((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/CountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/CountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/CountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyMerge[compute_and_apply_vocabulary_2/vocabulary]/SwapTokensAndCounts_234))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid_238))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric_255))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/ParDo(_TopPerBundle)_257))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Flatten/Transcode/0))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Flatten/Write/0)\n",
            "[2020-10-09 10:13:24,789] {fn_runner.py:489} INFO - Running (Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/Flatten/Read)+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/GroupByKey/Write)\n",
            "[2020-10-09 10:13:24,813] {fn_runner.py:489} INFO - Running (((Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/Top(6)/ParDo(_MergeTopPerBundle)_265))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_2/vocabulary]/ApplyThresholdsAndTopK/FlattenList_266))+(ref_PCollection_PCollection_149/Write)\n",
            "[2020-10-09 10:13:24,848] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/Prepare/Impulse_269)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/Prepare/FlatMap(<lambda at core.py:2826>)_270))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/Prepare/Map(decode)_272))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/OrderElements_273))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_283))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_284))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:24,905] {fn_runner.py:489} INFO - Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/WriteBundles_286))+(ref_PCollection_PCollection_161/Write)\n",
            "[2020-10-09 10:13:24,946] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_156/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/PreFinalize_287))+(ref_PCollection_PCollection_162/Write)\n",
            "[2020-10-09 10:13:24,984] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_156/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite_288))+(ref_PCollection_PCollection_163/Write)\n",
            "[2020-10-09 10:13:25,014] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:25,135] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:25,189] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/CreatePath/Impulse_290)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/CreatePath/FlatMap(<lambda at core.py:2826>)_291))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/CreatePath/Map(decode)_293))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_2/vocabulary]/WaitForVocabularyFile_294))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_2/vocabulary/Placeholder]/ToTensorBinding_296))+(ref_PCollection_PCollection_168/Write)\n",
            "[2020-10-09 10:13:25,238] {fn_runner.py:489} INFO - Running ((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/CountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/CountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/CountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyMerge[compute_and_apply_vocabulary_4/vocabulary]/SwapTokensAndCounts_396))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid_400))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric_417))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/ParDo(_TopPerBundle)_419))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Flatten/Transcode/0))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Flatten/Write/0)\n",
            "[2020-10-09 10:13:25,289] {fn_runner.py:489} INFO - Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey_405))+(ref_PCollection_PCollection_230/Write)\n",
            "[2020-10-09 10:13:25,326] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Impulse_407)+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/FlatMap(<lambda at core.py:2826>)_408))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Map(decode)_410))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault_411))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_4/vocabulary]/ToInt64_412))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_4/vocabulary/vocab_compute_and_apply_vocabulary_4_vocabulary_unpruned_vocab_size]/ToTensorBinding_414))+(ref_PCollection_PCollection_236/Write)\n",
            "[2020-10-09 10:13:25,379] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Impulse_107)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/FlatMap(<lambda at core.py:2826>)_108))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/Prepare/Map(decode)_110))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/OrderElements_111))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_121))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_122))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:25,436] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Impulse_116)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_117))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Map(decode)_119))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite_120))+(ref_PCollection_PCollection_60/Write))+(ref_PCollection_PCollection_61/Write)\n",
            "[2020-10-09 10:13:25,484] {fn_runner.py:489} INFO - Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/WriteBundles_124))+(ref_PCollection_PCollection_65/Write)\n",
            "[2020-10-09 10:13:25,524] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_60/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/PreFinalize_125))+(ref_PCollection_PCollection_66/Write)\n",
            "[2020-10-09 10:13:25,560] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_60/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite_126))+(ref_PCollection_PCollection_67/Write)\n",
            "[2020-10-09 10:13:25,595] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:25,710] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:25,739] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Impulse_128)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/FlatMap(<lambda at core.py:2826>)_129))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/CreatePath/Map(decode)_131))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary/vocabulary]/WaitForVocabularyFile_132))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary/vocabulary/Placeholder]/ToTensorBinding_134))+(ref_PCollection_PCollection_72/Write)\n",
            "[2020-10-09 10:13:25,784] {fn_runner.py:489} INFO - Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey_243))+(ref_PCollection_PCollection_134/Write)\n",
            "[2020-10-09 10:13:25,822] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Impulse_245)+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/FlatMap(<lambda at core.py:2826>)_246))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Map(decode)_248))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault_249))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_2/vocabulary]/ToInt64_250))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_2/vocabulary/vocab_compute_and_apply_vocabulary_2_vocabulary_unpruned_vocab_size]/ToTensorBinding_252))+(ref_PCollection_PCollection_140/Write)\n",
            "[2020-10-09 10:13:25,872] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Impulse_359)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_360))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Map(decode)_362))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite_363))+(ref_PCollection_PCollection_204/Write))+(ref_PCollection_PCollection_205/Write)\n",
            "[2020-10-09 10:13:25,916] {fn_runner.py:489} INFO - Running ((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/CountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/CountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/CountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyMerge[compute_and_apply_vocabulary_3/vocabulary]/SwapTokensAndCounts_315))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid_319))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric_336))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/ParDo(_TopPerBundle)_338))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Flatten/Transcode/0))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Flatten/Write/0)\n",
            "[2020-10-09 10:13:25,968] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Create/Impulse_340)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Create/FlatMap(<lambda at core.py:2826>)_341))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Create/Map(decode)_343))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Flatten/Transcode/1))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Flatten/Write/1)\n",
            "[2020-10-09 10:13:26,006] {fn_runner.py:489} INFO - Running (Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/Flatten/Read)+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/GroupByKey/Write)\n",
            "[2020-10-09 10:13:26,033] {fn_runner.py:489} INFO - Running (((Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/Top(61)/ParDo(_MergeTopPerBundle)_346))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_3/vocabulary]/ApplyThresholdsAndTopK/FlattenList_347))+(ref_PCollection_PCollection_197/Write)\n",
            "[2020-10-09 10:13:26,069] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/Prepare/Impulse_350)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/Prepare/FlatMap(<lambda at core.py:2826>)_351))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/Prepare/Map(decode)_353))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/OrderElements_354))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_364))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_365))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:26,122] {fn_runner.py:489} INFO - Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/WriteBundles_367))+(ref_PCollection_PCollection_209/Write)\n",
            "[2020-10-09 10:13:26,161] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_204/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/PreFinalize_368))+(ref_PCollection_PCollection_210/Write)\n",
            "[2020-10-09 10:13:26,198] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_204/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite_369))+(ref_PCollection_PCollection_211/Write)\n",
            "[2020-10-09 10:13:26,236] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:26,348] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:26,382] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/CreatePath/Impulse_371)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/CreatePath/FlatMap(<lambda at core.py:2826>)_372))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/CreatePath/Map(decode)_374))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_3/vocabulary]/WaitForVocabularyFile_375))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_3/vocabulary/Placeholder]/ToTensorBinding_377))+(ref_PCollection_PCollection_216/Write)\n",
            "[2020-10-09 10:13:26,426] {fn_runner.py:489} INFO - Running (((((Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Group/Read)+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/Merge))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PreCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/Map(StripNonce)_472))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/WindowIntoOriginal_473))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/Flatten/Write/1)\n",
            "[2020-10-09 10:13:26,479] {fn_runner.py:489} INFO - Running ((Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/Flatten/Read)+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Precombine))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Write)\n",
            "[2020-10-09 10:13:26,514] {fn_runner.py:489} INFO - Running ((((Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Group/Read)+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/Merge))+(Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/CombinePerKey/CombinePerKey(PostCombineFn)/ExtractOutputs))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/UnKey_479))+(ref_PCollection_PCollection_277/Write)\n",
            "[2020-10-09 10:13:26,582] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/DoOnce/Impulse_686)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_687))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/DoOnce/Map(decode)_689))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/InitializeWrite_690))+(ref_PCollection_PCollection_405/Write))+(ref_PCollection_PCollection_406/Write)\n",
            "[2020-10-09 10:13:26,630] {fn_runner.py:489} INFO - Running ((((((((((((ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/DoOnce/Impulse_481)+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/DoOnce/FlatMap(<lambda at core.py:2826>)_482))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/DoOnce/Map(decode)_484))+(ref_AppliedPTransform_Analyze/CacheableCombineAccumulate[bucketize/quantiles][AnalysisIndex0]/InitialCombineGlobally/InjectDefault_485))+(ref_AppliedPTransform_Analyze/FlattenCache[CacheableCombineMerge[bucketize/quantiles]]/Flatten_487))+(ref_AppliedPTransform_Analyze/EncodeCache[CacheableCombineAccumulate[bucketize/quantiles]][AnalysisIndex0]/Encode_568))+(ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/KeyWithVoid_490))+(Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Precombine))+(Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Write))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/WindowInto(WindowIntoFn)_691))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/WriteBundles_692))+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/Pair_693))+(WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:26,705] {fn_runner.py:489} INFO - Running ((((Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Group/Read)+(Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/Merge))+(Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/UnKey_495))+(ref_PCollection_PCollection_286/Write)\n",
            "[2020-10-09 10:13:26,746] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Impulse_497)+(ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/FlatMap(<lambda at core.py:2826>)_498))+(ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/DoOnce/Map(decode)_500))+(ref_AppliedPTransform_Analyze/CacheableCombineMerge[bucketize/quantiles]/MergeCombinesGlobally/InjectDefault_501))+(ref_AppliedPTransform_Analyze/ExtractCombineMergeOutputs[bucketize/quantiles]/ExtractOutputs/FlatMap(extract_outputs)_504))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[bucketize/quantiles/Placeholder]/ToTensorBinding_506))+(ref_PCollection_PCollection_293/Write)\n",
            "[2020-10-09 10:13:26,795] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Impulse_197)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_198))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Map(decode)_200))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite_201))+(ref_PCollection_PCollection_108/Write))+(ref_PCollection_PCollection_109/Write)\n",
            "[2020-10-09 10:13:26,845] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Create/Impulse_178)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Create/FlatMap(<lambda at core.py:2826>)_179))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Create/Map(decode)_181))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Flatten/Transcode/1))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Flatten/Write/1)\n",
            "[2020-10-09 10:13:26,881] {fn_runner.py:489} INFO - Running ((((((((((Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerToken/Group/Read)+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerToken/Merge))+(Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/CountPerToken/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyMerge[compute_and_apply_vocabulary_1/vocabulary]/SwapTokensAndCounts_153))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/KeyWithVoid_157))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/FlattenToSingleMetric_174))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Precombine))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Write))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/ParDo(_TopPerBundle)_176))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Flatten/Transcode/0))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Flatten/Write/0)\n",
            "[2020-10-09 10:13:26,923] {fn_runner.py:489} INFO - Running (Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/Flatten/Read)+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/GroupByKey/Write)\n",
            "[2020-10-09 10:13:26,956] {fn_runner.py:489} INFO - Running (((Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/Top(46)/ParDo(_MergeTopPerBundle)_184))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_1/vocabulary]/ApplyThresholdsAndTopK/FlattenList_185))+(ref_PCollection_PCollection_101/Write)\n",
            "[2020-10-09 10:13:26,990] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Impulse_188)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/FlatMap(<lambda at core.py:2826>)_189))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/Prepare/Map(decode)_191))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/OrderElements_192))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_202))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_203))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:27,042] {fn_runner.py:489} INFO - Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/WriteBundles_205))+(ref_PCollection_PCollection_113/Write)\n",
            "[2020-10-09 10:13:27,081] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/PreFinalize_206))+(ref_PCollection_PCollection_114/Write)\n",
            "[2020-10-09 10:13:27,114] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_108/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite_207))+(ref_PCollection_PCollection_115/Write)\n",
            "[2020-10-09 10:13:27,151] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:27,271] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:27,298] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Impulse_209)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/FlatMap(<lambda at core.py:2826>)_210))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/CreatePath/Map(decode)_212))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_1/vocabulary]/WaitForVocabularyFile_213))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/Placeholder]/ToTensorBinding_215))+(ref_PCollection_PCollection_120/Write)\n",
            "[2020-10-09 10:13:27,350] {fn_runner.py:489} INFO - Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey_324))+(ref_PCollection_PCollection_182/Write)\n",
            "[2020-10-09 10:13:27,385] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Impulse_326)+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/FlatMap(<lambda at core.py:2826>)_327))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Map(decode)_329))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault_330))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_3/vocabulary]/ToInt64_331))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_3/vocabulary/vocab_compute_and_apply_vocabulary_3_vocabulary_unpruned_vocab_size]/ToTensorBinding_333))+(ref_PCollection_PCollection_188/Write)\n",
            "[2020-10-09 10:13:27,439] {fn_runner.py:489} INFO - Running ((((Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Group/Read)+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/Merge))+(Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/CombinePerKey/ExtractOutputs))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/UnKey_162))+(ref_PCollection_PCollection_86/Write)\n",
            "[2020-10-09 10:13:27,475] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Impulse_164)+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/FlatMap(<lambda at core.py:2826>)_165))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/DoOnce/Map(decode)_167))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/TotalVocabSize/CombineGlobally(CountCombineFn)/InjectDefault_168))+(ref_AppliedPTransform_Analyze/VocabularyCount[compute_and_apply_vocabulary_1/vocabulary]/ToInt64_169))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_1/vocabulary/vocab_compute_and_apply_vocabulary_1_vocabulary_unpruned_vocab_size]/ToTensorBinding_171))+(ref_PCollection_PCollection_92/Write)\n",
            "[2020-10-09 10:13:27,523] {fn_runner.py:489} INFO - Running ((((ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Create/Impulse_421)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Create/FlatMap(<lambda at core.py:2826>)_422))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Create/Map(decode)_424))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Flatten/Transcode/1))+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Flatten/Write/1)\n",
            "[2020-10-09 10:13:27,565] {fn_runner.py:489} INFO - Running (Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/Flatten/Read)+(Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/GroupByKey/Write)\n",
            "[2020-10-09 10:13:27,590] {fn_runner.py:489} INFO - Running (((Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/Top(91)/ParDo(_MergeTopPerBundle)_427))+(ref_AppliedPTransform_Analyze/VocabularyPrune[compute_and_apply_vocabulary_4/vocabulary]/ApplyThresholdsAndTopK/FlattenList_428))+(ref_PCollection_PCollection_245/Write)\n",
            "[2020-10-09 10:13:27,631] {fn_runner.py:489} INFO - Running ((((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/Prepare/Impulse_431)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/Prepare/FlatMap(<lambda at core.py:2826>)_432))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/Prepare/Map(decode)_434))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/OrderElements_435))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/Map(<lambda at iobase.py:1010>)_445))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/WindowInto(WindowIntoFn)_446))+(Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Write)\n",
            "[2020-10-09 10:13:27,682] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Impulse_440)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_441))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/DoOnce/Map(decode)_443))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/InitializeWrite_444))+(ref_PCollection_PCollection_252/Write))+(ref_PCollection_PCollection_253/Write)\n",
            "[2020-10-09 10:13:27,729] {fn_runner.py:489} INFO - Running ((Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/WriteBundles_448))+(ref_PCollection_PCollection_257/Write)\n",
            "[2020-10-09 10:13:27,769] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_252/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/PreFinalize_449))+(ref_PCollection_PCollection_258/Write)\n",
            "[2020-10-09 10:13:27,810] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_252/Read)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WriteToText/Write/WriteImpl/FinalizeWrite_450))+(ref_PCollection_PCollection_259/Write)\n",
            "[2020-10-09 10:13:27,847] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:27,976] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:28,024] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/CreatePath/Impulse_452)+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/CreatePath/FlatMap(<lambda at core.py:2826>)_453))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/CreatePath/Map(decode)_455))+(ref_AppliedPTransform_Analyze/VocabularyOrderAndWrite[compute_and_apply_vocabulary_4/vocabulary]/WaitForVocabularyFile_456))+(ref_AppliedPTransform_Analyze/CreateTensorBinding[compute_and_apply_vocabulary_4/vocabulary/Placeholder]/ToTensorBinding_458))+(ref_PCollection_PCollection_264/Write)\n",
            "[2020-10-09 10:13:28,070] {fn_runner.py:489} INFO - Running (((((((((((ref_AppliedPTransform_Analyze/CreateSavedModel/BindTensors/CreateSavedModel/Impulse_510)+(ref_AppliedPTransform_Analyze/CreateSavedModel/BindTensors/CreateSavedModel/FlatMap(<lambda at core.py:2826>)_511))+(ref_AppliedPTransform_Analyze/CreateSavedModel/BindTensors/CreateSavedModel/Map(decode)_513))+(ref_AppliedPTransform_Analyze/CreateSavedModel/BindTensors/ReplaceWithConstants_514))+(ref_AppliedPTransform_Analyze/ComputeDeferredMetadata[compat_v1=True]_576))+(ref_AppliedPTransform_Analyze/MakeCheapBarrier_577))+(ref_AppliedPTransform_WriteTransformFn/WriteTransformFnToTemp_594))+(ref_PCollection_PCollection_297/Write))+(ref_AppliedPTransform_WriteTransformFn/WriteMetadataToTemp/WriteMetadata_593))+(ref_PCollection_PCollection_333/Write))+(ref_PCollection_PCollection_342/Write))+(ref_PCollection_PCollection_343/Write)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_1:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_3_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_4_vocabulary\"\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:28,450] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/DoOnce/Impulse_762)+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_763))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/DoOnce/Map(decode)_765))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/InitializeWrite_766))+(ref_PCollection_PCollection_447/Write))+(ref_PCollection_PCollection_448/Write)\n",
            "[2020-10-09 10:13:28,497] {fn_runner.py:489} INFO - Running ((((((((((((((((ref_PCollection_PCollection_414_split/Read)+(TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/ReadRawRecords/FlattenPCollsFromPatterns_706))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords_708))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)_712))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode_713))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex0]/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches_715))+(ref_AppliedPTransform_Transform[TransformIndex0]/Transform_717))+(ref_AppliedPTransform_Transform[TransformIndex0]/ConvertAndUnbatch_718))+(ref_AppliedPTransform_Transform[TransformIndex0]/MakeCheapBarrier_719))+(ref_AppliedPTransform_EncodeAndSerialize[TransformIndex0]_726))+(ref_PCollection_PCollection_423/Write))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Values_757))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_767))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/WriteBundles_768))+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/Pair_769))+(Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Write)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_1:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_3_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_4_vocabulary\"\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:34,734] {fn_runner.py:489} INFO - Running ((Materialize[TransformIndex0]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/Extract_771))+(ref_PCollection_PCollection_453/Write)\n",
            "[2020-10-09 10:13:34,769] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_447/Read)+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/PreFinalize_772))+(ref_PCollection_PCollection_454/Write)\n",
            "[2020-10-09 10:13:34,807] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_447/Read)+(ref_AppliedPTransform_Materialize[TransformIndex0]/Write/Write/WriteImpl/FinalizeWrite_773)\n",
            "[2020-10-09 10:13:34,839] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:34,952] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:34,982] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/Impulse_733)+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/PairWithRestriction))+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/SplitAndSizeRestriction))+(ref_PCollection_PCollection_429_split/Write)\n",
            "[2020-10-09 10:13:35,037] {fn_runner.py:489} INFO - Running (((((ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/DoOnce/Impulse_780)+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/DoOnce/FlatMap(<lambda at core.py:2826>)_781))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/DoOnce/Map(decode)_783))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/InitializeWrite_784))+(ref_PCollection_PCollection_459/Write))+(ref_PCollection_PCollection_460/Write)\n",
            "[2020-10-09 10:13:35,087] {fn_runner.py:489} INFO - Running ((((((((((((((((ref_PCollection_PCollection_429_split/Read)+(TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/ReadFromTFRecord[0]/Read/_SDFBoundedSourceWrapper/ParDo(SDFBoundedSourceDoFn)/Process))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/ReadRawRecords/FlattenPCollsFromPatterns_735))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordBeamSource/CollectRawRecordTelemetry/ProfileRawRecords_737))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordToRecordBatch/RawRecordToRecordBatch/Batch/ParDo(_GlobalWindowsBatchingDoFn)_741))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordToRecordBatch/RawRecordToRecordBatch/Decode_742))+(ref_AppliedPTransform_TFXIOReadAndDecode[TransformIndex1]/RawRecordToRecordBatch/CollectRecordBatchTelemetry/ProfileRecordBatches_744))+(ref_AppliedPTransform_Transform[TransformIndex1]/Transform_746))+(ref_AppliedPTransform_Transform[TransformIndex1]/ConvertAndUnbatch_747))+(ref_AppliedPTransform_Transform[TransformIndex1]/MakeCheapBarrier_748))+(ref_AppliedPTransform_EncodeAndSerialize[TransformIndex1]_755))+(ref_PCollection_PCollection_438/Write))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Values_775))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/WindowInto(WindowIntoFn)_785))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/WriteBundles_786))+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/Pair_787))+(Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Write)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_1:0\\022-vocab_compute_and_apply_vocabulary_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_3:0\\022/vocab_compute_and_apply_vocabulary_1_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_5:0\\022/vocab_compute_and_apply_vocabulary_2_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_7:0\\022/vocab_compute_and_apply_vocabulary_3_vocabulary\"\n",
            "\n",
            "Expected binary or unicode string, got type_url: \"type.googleapis.com/tensorflow.AssetFileDef\"\n",
            "value: \"\\n\\013\\n\\tConst_9:0\\022/vocab_compute_and_apply_vocabulary_4_vocabulary\"\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:13:47,325] {fn_runner.py:489} INFO - Running ((Materialize[TransformIndex1]/Write/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/Extract_789))+(ref_PCollection_PCollection_465/Write)\n",
            "[2020-10-09 10:13:47,354] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_459/Read)+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/PreFinalize_790))+(ref_PCollection_PCollection_466/Write)\n",
            "[2020-10-09 10:13:47,388] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_459/Read)+(ref_AppliedPTransform_Materialize[TransformIndex1]/Write/Write/WriteImpl/FinalizeWrite_791)\n",
            "[2020-10-09 10:13:47,421] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:47,541] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:47,564] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/CreateSavedModel/Count/CreateSole/Impulse_517)+(ref_AppliedPTransform_Analyze/CreateSavedModel/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_518))+(ref_AppliedPTransform_Analyze/CreateSavedModel/Count/CreateSole/Map(decode)_520))+(ref_AppliedPTransform_Analyze/CreateSavedModel/Count/Count_521)\n",
            "[2020-10-09 10:13:47,603] {fn_runner.py:489} INFO - Running ((WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/GroupByKey/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/Extract_695))+(ref_PCollection_PCollection_411/Write)\n",
            "[2020-10-09 10:13:47,639] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_405/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/PreFinalize_696))+(ref_PCollection_PCollection_412/Write)\n",
            "[2020-10-09 10:13:47,678] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_405/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex5]/Write/WriteImpl/FinalizeWrite_697)\n",
            "[2020-10-09 10:13:47,716] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:47,834] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.10 seconds.\n",
            "[2020-10-09 10:13:47,889] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]][AnalysisIndex0]/Count/CreateSole/Impulse_526)+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_527))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]][AnalysisIndex0]/Count/CreateSole/Map(decode)_529))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary/vocabulary]][AnalysisIndex0]/Count/Count_530)\n",
            "[2020-10-09 10:13:47,927] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]][AnalysisIndex0]/Count/CreateSole/Impulse_535)+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_536))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]][AnalysisIndex0]/Count/CreateSole/Map(decode)_538))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_1/vocabulary]][AnalysisIndex0]/Count/Count_539)\n",
            "[2020-10-09 10:13:47,965] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary]][AnalysisIndex0]/Count/CreateSole/Impulse_544)+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_545))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary]][AnalysisIndex0]/Count/CreateSole/Map(decode)_547))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_2/vocabulary]][AnalysisIndex0]/Count/Count_548)\n",
            "[2020-10-09 10:13:48,005] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary]][AnalysisIndex0]/Count/CreateSole/Impulse_553)+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_554))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary]][AnalysisIndex0]/Count/CreateSole/Map(decode)_556))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_3/vocabulary]][AnalysisIndex0]/Count/Count_557)\n",
            "[2020-10-09 10:13:48,039] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary]][AnalysisIndex0]/Count/CreateSole/Impulse_562)+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_563))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary]][AnalysisIndex0]/Count/CreateSole/Map(decode)_565))+(ref_AppliedPTransform_Analyze/EncodeCache[VocabularyAccumulate[compute_and_apply_vocabulary_4/vocabulary]][AnalysisIndex0]/Count/Count_566)\n",
            "[2020-10-09 10:13:48,081] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/EncodeCache[CacheableCombineAccumulate[bucketize/quantiles]][AnalysisIndex0]/Count/CreateSole/Impulse_571)+(ref_AppliedPTransform_Analyze/EncodeCache[CacheableCombineAccumulate[bucketize/quantiles]][AnalysisIndex0]/Count/CreateSole/FlatMap(<lambda at core.py:2826>)_572))+(ref_AppliedPTransform_Analyze/EncodeCache[CacheableCombineAccumulate[bucketize/quantiles]][AnalysisIndex0]/Count/CreateSole/Map(decode)_574))+(ref_AppliedPTransform_Analyze/EncodeCache[CacheableCombineAccumulate[bucketize/quantiles]][AnalysisIndex0]/Count/Count_575)\n",
            "[2020-10-09 10:13:48,121] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Analyze/PrepareToClearSharedKeepAlives/Impulse_579)+(ref_AppliedPTransform_Analyze/PrepareToClearSharedKeepAlives/FlatMap(<lambda at core.py:2826>)_580))+(ref_AppliedPTransform_Analyze/PrepareToClearSharedKeepAlives/Map(decode)_582))+(ref_AppliedPTransform_Analyze/WaitAndClearSharedKeepAlives_583)\n",
            "[2020-10-09 10:13:48,161] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Transform[TransformIndex0]/PrepareToClearSharedKeepAlives/Impulse_721)+(ref_AppliedPTransform_Transform[TransformIndex0]/PrepareToClearSharedKeepAlives/FlatMap(<lambda at core.py:2826>)_722))+(ref_AppliedPTransform_Transform[TransformIndex0]/PrepareToClearSharedKeepAlives/Map(decode)_724))+(ref_AppliedPTransform_Transform[TransformIndex0]/WaitAndClearSharedKeepAlives_725)\n",
            "[2020-10-09 10:13:48,198] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_WriteMetadata/Create/Impulse_586)+(ref_AppliedPTransform_WriteMetadata/Create/FlatMap(<lambda at core.py:2826>)_587))+(ref_AppliedPTransform_WriteMetadata/Create/Map(decode)_589))+(ref_AppliedPTransform_WriteMetadata/WriteMetadata_590)\n",
            "[2020-10-09 10:13:48,253] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_IncrementPipelineMetrics/CreateSole/Impulse_4)+(ref_AppliedPTransform_IncrementPipelineMetrics/CreateSole/FlatMap(<lambda at core.py:2826>)_5))+(ref_AppliedPTransform_IncrementPipelineMetrics/CreateSole/Map(decode)_7))+(ref_AppliedPTransform_IncrementPipelineMetrics/Count_8)\n",
            "[2020-10-09 10:13:48,302] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_WriteTransformFn/CreateSole/Impulse_596)+(ref_AppliedPTransform_WriteTransformFn/CreateSole/FlatMap(<lambda at core.py:2826>)_597))+(ref_AppliedPTransform_WriteTransformFn/CreateSole/Map(decode)_599))+(ref_AppliedPTransform_WriteTransformFn/PublishMetadataAndTransformFn_600)\n",
            "[2020-10-09 10:13:48,352] {fn_runner.py:489} INFO - Running (ref_PCollection_PCollection_350/Read)+(ref_AppliedPTransform_WriteCache/Write[AnalysisIndex0][CacheKeyIndex0]/Write/WriteImpl/FinalizeWrite_617)\n",
            "[2020-10-09 10:13:48,390] {filebasedsink.py:310} INFO - Starting finalize_write threads with num_shards: 1 (skipped: 0), batches: 1, num_threads: 1\n",
            "[2020-10-09 10:13:48,509] {filebasedsink.py:355} INFO - Renamed 1 shards in 0.11 seconds.\n",
            "[2020-10-09 10:13:48,561] {fn_runner.py:489} INFO - Running (((ref_AppliedPTransform_Transform[TransformIndex1]/PrepareToClearSharedKeepAlives/Impulse_750)+(ref_AppliedPTransform_Transform[TransformIndex1]/PrepareToClearSharedKeepAlives/FlatMap(<lambda at core.py:2826>)_751))+(ref_AppliedPTransform_Transform[TransformIndex1]/PrepareToClearSharedKeepAlives/Map(decode)_753))+(ref_AppliedPTransform_Transform[TransformIndex1]/WaitAndClearSharedKeepAlives_754)\n",
            "[2020-10-09 10:13:48,683] {base_component_launcher.py:208} INFO - Running publisher for Transform\n",
            "[2020-10-09 10:13:48,700] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:48,732] {beam_dag_runner.py:89} INFO - Component Transform is finished.\n",
            "[2020-10-09 10:13:48,756] {fn_runner.py:489} INFO - Running ((ref_PCollection_PCollection_3/Read)+(ref_AppliedPTransform_Run[Trainer]_13))+(ref_PCollection_PCollection_10/Write)\n",
            "[2020-10-09 10:13:48,783] {beam_dag_runner.py:87} INFO - Component Trainer is running.\n",
            "[2020-10-09 10:13:48,798] {base_component_launcher.py:195} INFO - Running driver for Trainer\n",
            "[2020-10-09 10:13:48,814] {metadata_store.py:66} INFO - MetadataStore with DB connection initialized\n",
            "[2020-10-09 10:13:48,894] {base_component_launcher.py:201} INFO - Running executor for Trainer\n",
            "[2020-10-09 10:13:48,911] {fn_args_utils.py:105} INFO - Train on the 'train' split when train_args.splits is not set.\n",
            "[2020-10-09 10:13:48,920] {fn_args_utils.py:109} INFO - Evaluate on the 'eval' split when eval_args.splits is not set.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/beam/beam_dag_runner.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element, *signals)\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Signal PCollection should be empty.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/beam/beam_dag_runner.py\u001b[0m in \u001b[0;36m_run_component\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component %s is running.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_launcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component %s is finished.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/launcher/base_component_launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m                          \u001b[0mexecution_decision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                          execution_decision.exec_properties)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/launcher/in_process_component_launcher.py\u001b[0m in \u001b[0;36m_run_executor\u001b[0;34m(self, execution_id, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/components/trainer/executor.py\u001b[0m in \u001b[0;36mDo\u001b[0;34m(self, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mfn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetFnArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mtrainer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainer_fn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/components/util/udf_utils.py\u001b[0m in \u001b[0;36mget_fn\u001b[0;34m(exec_properties, fn_name)\u001b[0m\n\u001b[1;32m     42\u001b[0m       return import_utils.import_func_from_source(\n\u001b[0;32m---> 43\u001b[0;31m           exec_properties[_MODULE_FILE_KEY], fn_name)\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mhas_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/utils/import_utils.py\u001b[0m in \u001b[0;36mimport_func_from_source\u001b[0;34m(source_path, fn_name)\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'user_module' has no attribute 'trainer_fn'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-1030bc6dfd67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# print(components)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mpipe_line\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_beam_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipeline_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirect_num_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mBeamDagRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_line\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/beam/beam_dag_runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, tfx_pipeline)\u001b[0m\n\u001b[1;32m    163\u001b[0m                                    component_config, tfx_pipeline),\n\u001b[1;32m    164\u001b[0m                   *[beam.pvalue.AsIter(s) for s in signals_to_wait]))\n\u001b[0;32m--> 165\u001b[0;31m           \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component %s is scheduled.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 534\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    535\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m       \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_tempdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/direct/direct_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    117\u001b[0m       \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBundleBasedDirectRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     self._latest_run_result = self.run_via_runner_api(\n\u001b[0;32m--> 176\u001b[0;31m         pipeline.to_runner_api(default_environment=self._default_environment))\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_run_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_via_runner_api\u001b[0;34m(self, pipeline_proto)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;31m# TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m#   the teststream (if any), and all the stages).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_stages\u001b[0;34m(self, stage_context, stages)\u001b[0m\n\u001b[1;32m    342\u001b[0m           stage_results = self._run_stage(\n\u001b[1;32m    343\u001b[0m               \u001b[0mrunner_execution_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m               \u001b[0mbundle_context_manager\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m           )\n\u001b[1;32m    346\u001b[0m           monitoring_infos_by_stage[stage.name] = (\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self, runner_execution_context, bundle_context_manager)\u001b[0m\n\u001b[1;32m    521\u001b[0m               \u001b[0minput_timers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m               \u001b[0mexpected_timer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m               bundle_manager)\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m       \u001b[0mfinal_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36m_run_bundle\u001b[0;34m(self, runner_execution_context, bundle_context_manager, data_input, data_output, input_timers, expected_timer_output, bundle_manager)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m     result, splits = bundle_manager.process_bundle(\n\u001b[0;32m--> 561\u001b[0;31m         data_input, data_output, input_timers, expected_timer_output)\n\u001b[0m\u001b[1;32m    562\u001b[0m     \u001b[0;31m# Now we collect all the deferred inputs remaining from bundle execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[0;31m# Deferred inputs can be:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_unbounded_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       for result, split_result in executor.map(execute, zip(part_inputs,  # pylint: disable=zip-builtin-not-iterating\n\u001b[0;32m--> 945\u001b[0;31m                                                             timer_inputs)):\n\u001b[0m\u001b[1;32m    946\u001b[0m         \u001b[0msplit_result_list\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0msplit_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmerged_result\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    584\u001b[0m                     \u001b[0;31m# Careful not to keep a reference to the popped future\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                         \u001b[0;32myield\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    430\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 432\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    433\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/utils/thread_pool_executor.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m       \u001b[0;31m# If the future wasn't cancelled, then attempt to execute it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# Even though Python 2 futures library has #set_exection(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(part_map_input_timers)\u001b[0m\n\u001b[1;32m    939\u001b[0m           \u001b[0minput_timers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0mexpected_output_timers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 941\u001b[0;31m           dry_run)\n\u001b[0m\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mthread_pool_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_unbounded_instance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[1;32m    839\u001b[0m             \u001b[0mprocess_bundle_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             cache_tokens=[next(self._cache_token_generator)]))\n\u001b[0;32m--> 841\u001b[0;31m     \u001b[0mresult_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_bundle_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m     \u001b[0msplit_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    351\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uid_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'control_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uid_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mControlFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py\u001b[0m in \u001b[0;36mdo_instruction\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    481\u001b[0m       \u001b[0;31m# E.g. if register is set, this will call self.register(request.register))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m       return getattr(self, request_type)(\n\u001b[0;32m--> 483\u001b[0;31m           getattr(request, request_type), request.instruction_id)\n\u001b[0m\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/sdk_worker.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, request, instruction_id)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m           delayed_applications, requests_finalization = (\n\u001b[0;32m--> 518\u001b[0;31m               bundle_processor.process_bundle(instruction_id))\n\u001b[0m\u001b[1;32m    519\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbundle_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_cache_metrics_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, instruction_id)\u001b[0m\n\u001b[1;32m    981\u001b[0m           \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_fn_api_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m             input_op_by_transform_id[element.transform_id].process_encoded(\n\u001b[0;32m--> 983\u001b[0;31m                 element.data)\n\u001b[0m\u001b[1;32m    984\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m       \u001b[0;31m# Finish all operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/bundle_processor.py\u001b[0m in \u001b[0;36mprocess_encoded\u001b[0;34m(self, encoded_windowed_values)\u001b[0m\n\u001b[1;32m    217\u001b[0m       decoded_value = self.windowed_coder_impl.decode_from_stream(\n\u001b[1;32m    218\u001b[0m           input_stream, True)\n\u001b[0;32m--> 219\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_pcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/worker/operations.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/future/utils/__init__.py\u001b[0m in \u001b[0;36mraise_with_traceback\u001b[0;34m(exc, traceback)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mEllipsis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/apache_beam/runners/common.cpython-36m-x86_64-linux-gnu.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/beam/beam_dag_runner.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, element, *signals)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0msignal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m       \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Signal PCollection should be empty.'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_run_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/beam/beam_dag_runner.py\u001b[0m in \u001b[0;36m_run_component\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_run_component\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component %s is running.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_launcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mabsl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Component %s is finished.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_component_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/launcher/base_component_launcher.py\u001b[0m in \u001b[0;36mlaunch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    203\u001b[0m                          \u001b[0mexecution_decision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                          \u001b[0mexecution_decision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                          execution_decision.exec_properties)\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     absl.logging.info('Running publisher for %s',\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/orchestration/launcher/in_process_component_launcher.py\u001b[0m in \u001b[0;36m_run_executor\u001b[0;34m(self, execution_id, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[1;32m     65\u001b[0m         executor_context)  # type: ignore\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0mexecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/components/trainer/executor.py\u001b[0m in \u001b[0;36mDo\u001b[0;34m(self, input_dict, output_dict, exec_properties)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m     \u001b[0mfn_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetFnArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m     \u001b[0mtrainer_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mudf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'trainer_fn'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_pbtxt_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/components/util/udf_utils.py\u001b[0m in \u001b[0;36mget_fn\u001b[0;34m(exec_properties, fn_name)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m       return import_utils.import_func_from_source(\n\u001b[0;32m---> 43\u001b[0;31m           exec_properties[_MODULE_FILE_KEY], fn_name)\n\u001b[0m\u001b[1;32m     44\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mhas_fn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mfn_path_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexec_properties\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tfx/utils/import_utils.py\u001b[0m in \u001b[0;36mimport_func_from_source\u001b[0;34m(source_path, fn_name)\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0muser_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m       \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_module\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'user_module' has no attribute 'trainer_fn' [while running 'Run[Trainer]']"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnqUsPkxlUFV"
      },
      "source": [
        "This is a minimal setup that you can easily integrate with the rest of your infrastructure or example using a cron job. You can also scale this pipeline using <a href=\"https://flink.apache.org/\">Apache Flink</a> like in this <a href=\"https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py\">TFX Example</a> or <a href=\"https://spark.apache.org/\">Spark</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vz67jT1mheK"
      },
      "source": [
        "## Introduction to Apache Airflow\n",
        "\n",
        "Airflow is Apache's project for workflow automation. Airflow lets you represent workflow tasks thorugh DAGs (Direct Acylic Graphs) represented via Python code. Airflow also lets you schedule and monitor workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy8viiannD8p"
      },
      "source": [
        "### Installation and Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkdfr0jMGi7M"
      },
      "source": [
        "# !pip install apache-airflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d29H6f1gBli_"
      },
      "source": [
        "For a complete list of Airflow extensions and how to install them look into the <a href=\"\">Airflow documentation</a>.<br>\n",
        "With airflow now installed, you need to create an initial database where all the task status infromation will be stored:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCwseZiymatR",
        "outputId": "17d9c162-293e-4203-d551-31c2873022aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "!airflow initdb"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "DB: sqlite:////root/airflow/airflow.db\n",
            "[2020-10-09 10:14:46,564] {db.py:378} INFO - Creating tables\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "WARNI [airflow.models.crypto] cryptography not found - values will not be stored encrypted.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY9CM3BrHROI"
      },
      "source": [
        "Airflow will initiate and SQLite database if not configurations have been changed. This setup works for demo purposes and run smaller projects, but for larger projects look in the <a href=\"https://airflow.apache.org/docs/stable/howto/index.html\">Apache Airflow documentation</a>.<br>\n",
        "A minimal Airflow setup consists of the Airflow scheduler, which coordinates the tasks and task dependencies, as well as a web server, which provides a UI to start, stop and monitor the tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvrtdK0Qmama",
        "outputId": "997af130-48b7-4ae0-bca7-723ebe8219a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "!airflow scheduler"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
            "[2020-10-09 09:24:03,359] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1367} INFO - Starting the scheduler\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1375} INFO - Running execute loop for -1 seconds\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1376} INFO - Processing each file at most -1 times\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1379} INFO - Searching for files in /root/airflow/dags\n",
            "[2020-10-09 09:24:03,376] {scheduler_job.py:1381} INFO - There are 25 files in /root/airflow/dags\n",
            "[2020-10-09 09:24:03,377] {scheduler_job.py:1438} INFO - Resetting orphaned tasks for active dag runs\n",
            "[2020-10-09 09:24:03,386] {dag_processing.py:562} INFO - Launched DagFileProcessorManager with pid: 2939\n",
            "[2020-10-09 09:24:03,393] {settings.py:55} INFO - Configured default timezone <Timezone [UTC]>\n",
            "[2020-10-09 09:24:03,408] {dag_processing.py:776} WARNING - Because we cannot use more than 1 thread (max_threads = 2) when using sqlite. So we set parallelism to 1.\n",
            "1\n",
            "\n",
            "y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-309581388ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'airflow scheduler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   result = _run_command(\n\u001b[0;32m--> 440\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    441\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4aTmbl4mai8",
        "outputId": "087503b0-a665-4d9b-d170-bd05366cc085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start the Airflow web server\n",
        "# The command argument -p sets the port where your web browser can access the Airflow interface.\n",
        "!airflow webserver -p 8081"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
            "[2020-10-09 09:40:28,707] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:28,707] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "Running the Gunicorn Server with:\n",
            "Workers: 4 sync\n",
            "Host: 0.0.0.0:8081\n",
            "Timeout: 120\n",
            "Logfiles: - -\n",
            "=================================================================            \n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Starting gunicorn 20.0.4\n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Listening at: http://0.0.0.0:8081 (3750)\n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Using worker: sync\n",
            "[2020-10-09 09:40:29 +0000] [3754] [INFO] Booting worker with pid: 3754\n",
            "[2020-10-09 09:40:29 +0000] [3755] [INFO] Booting worker with pid: 3755\n",
            "[2020-10-09 09:40:29 +0000] [3756] [INFO] Booting worker with pid: 3756\n",
            "[2020-10-09 09:40:29 +0000] [3757] [INFO] Booting worker with pid: 3757\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:40:30,956] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:30,957] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,032] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,040] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,421] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,424] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,425] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,428] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:59 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:40:59 +0000] [3773] [INFO] Booting worker with pid: 3773\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:40:59,591] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:59,592] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:41:00 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:41:00 +0000] [3754] [INFO] Worker exiting (pid: 3754)\n",
            "[2020-10-09 09:41:29 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:41:29 +0000] [3781] [INFO] Booting worker with pid: 3781\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:41:30,049] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:41:30,050] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:41:30 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:41:30 +0000] [3755] [INFO] Worker exiting (pid: 3755)\n",
            "[2020-10-09 09:42:00 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:42:00 +0000] [3791] [INFO] Booting worker with pid: 3791\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:42:00,551] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:42:00,552] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:42:01 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:42:01 +0000] [3756] [INFO] Worker exiting (pid: 3756)\n",
            "[2020-10-09 09:42:30 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:42:30 +0000] [3799] [INFO] Booting worker with pid: 3799\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:42:31,099] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:42:31,100] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:42:31 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:42:31 +0000] [3757] [INFO] Worker exiting (pid: 3757)\n",
            "[2020-10-09 09:43:01 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:43:01 +0000] [3812] [INFO] Booting worker with pid: 3812\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:43:01,512] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:43:01,513] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:43:02 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:43:02 +0000] [3773] [INFO] Worker exiting (pid: 3773)\n",
            "[2020-10-09 09:43:31 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:43:31 +0000] [3823] [INFO] Booting worker with pid: 3823\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:43:32,026] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:43:32,027] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:43:32 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:43:32 +0000] [3781] [INFO] Worker exiting (pid: 3781)\n",
            "[2020-10-09 09:43:52 +0000] [3750] [INFO] Handling signal: int\n",
            "[2020-10-09 09:43:52,501] {cli.py:1127} INFO - Received signal: 2. Closing gunicorn.\n",
            "[2020-10-09 09:43:52 +0000] [3823] [INFO] Worker exiting (pid: 3823)\n",
            "[2020-10-09 09:43:52 +0000] [3799] [INFO] Worker exiting (pid: 3799)\n",
            "[2020-10-09 09:43:52 +0000] [3791] [INFO] Worker exiting (pid: 3791)\n",
            "[2020-10-09 09:43:52 +0000] [3812] [INFO] Worker exiting (pid: 3812)\n",
            "[2020-10-09 09:43:53 +0000] [3750] [INFO] Shutting down: Master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGr_B02rZe_"
      },
      "source": [
        "Go to: http://127.0.0.1:8081 and you should see an interface.\n",
        "\n",
        "**Airflow Configuration**\n",
        "The default settings of Airflow can be overwritten by chaning the relevant parameters in the Airflow configuration. The new settings have to be defined in ~/airflow/airflow.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqysLLW-r9so"
      },
      "source": [
        "### Basic Airflow Example\n",
        "\n",
        "Here we will not include any TFX scripts. Workflow pipelines are defined as Python scripts and Airflow expects the DAG definitions to be located in ~/airflow/dags. A basic pipeline consists of:\n",
        " - Project-Specific Configurations\n",
        " - Task Definitions\n",
        " - Definition of the Task Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohe36TsVsenC"
      },
      "source": [
        "#### Project-Specific Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFYKhzi3rZCJ"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI0AKOZ2mag8"
      },
      "source": [
        "# Location to define the project configuration\n",
        "project_cfg = {\n",
        "    \"owner\": \"airflow\",\n",
        "    \"email\": [\"jankezmann@t-online.de\"],\n",
        "    \"email_on_failure\": True,\n",
        "    \"start_date\": datetime(2019, 8, 1),\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(hours=1)\n",
        "}\n",
        "\n",
        "# The DAG object will be picked up by Airflow\n",
        "dag = DAG(\n",
        "    \"basic_pipeline\",\n",
        "    default_args=project_cfg,\n",
        "    schedule_interval=timedelta(days=1)\n",
        ")"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM8N8kIGtKsQ"
      },
      "source": [
        "#### Task Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmTS08xUmafh"
      },
      "source": [
        "from airflow.operators.python_operator import PythonOperator"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OquJGytCmacO"
      },
      "source": [
        "def example_task(_id, **kwargs):\n",
        "    print(\"task {}\".format(_id))\n",
        "    return \"completed task {}\".format(id)\n",
        "\n",
        "task_1 = PythonOperator(\n",
        "    task_id=\"task_1\",\n",
        "    provide_context=True,\n",
        "    python_callable=example_task,\n",
        "    op_kwargs={\"_id\": 1},\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "\n",
        "task_2 = PythonOperator(\n",
        "    task_id=\"task_2\",\n",
        "    provide_context=True,\n",
        "    python_callable=example_task,\n",
        "    op_kwargs={\"_id\": 2},\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9im6gKHjwlOg"
      },
      "source": [
        "In a TFX pipeline, you do not need to define these tasks because the TFX library takes care of it for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSk0IVndwsVR"
      },
      "source": [
        "#### Task dependencies\n",
        "\n",
        "Lets assume task_2 depends on task_1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0BxXFULwtAb"
      },
      "source": [
        "task_1.set_downstream(task_2)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXxe6xAExBGA"
      },
      "source": [
        "Airflow also offer as bit-shift operator to denote the task dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgDkMwQmwtRp",
        "outputId": "2b315abb-765c-4bcb-e08a-d13da9129e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "task_1 >> task_2"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(PythonOperator): task_2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qEcCIyxNIq"
      },
      "source": [
        "#### Putting it all together\n",
        "\n",
        "In your DAG folder in your AIRFLOW_HOME path, usually at ~/airflow/dags, create a new file basic_pipeline.py: See details pages 332f.<br>\n",
        "You can then test the pipeline setup by executing this command in your terminal\n",
        "\n",
        "```\n",
        "python ~/airflow/dags/basic_pipeline.py\n",
        "```\n",
        "\n",
        "The log file can be found at:\n",
        "```\n",
        "~/airflow/logs/NAME OF YOUR PIPELINE/TASK NAME/EXECTUION TIME/\n",
        "```\n",
        "\n",
        "If we want to inspect the result of the first task from our bsic pipeline, we have to investigate the log file:\n",
        "```\n",
        "cat ../logs/basic_pipeline/task_1/.../1.log\n",
        "```\n",
        "\n",
        "To test whether Airflow recognized the new pipeline you can execute:\n",
        "```\n",
        "!airflow list_dags\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksk7x9VrzFHT"
      },
      "source": [
        "## Orchestrating TFX Pipelines with Apache Airflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYW853_zI8i"
      },
      "source": [
        "### Pipeline Setup\n",
        "\n",
        "Instead of importing the BeamDagRunner, we will use the AirflowDAGRunner. Again the files for an Airflow pipeline need to be located in the ~/airflow/dags folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7XtPSwRwtWW"
      },
      "source": [
        "airflow_config = {\n",
        "    \"schedule_interval\": None,\n",
        "    \"start_date\": datetime(2020, 10, 9),\n",
        "    \"pipeline_name\": \"your_ml_pipeline\"\n",
        "}"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LctSUIx90aDs"
      },
      "source": [
        "from typing import Text"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5u0kASmwtMO"
      },
      "source": [
        "def init_pipeline(\n",
        "    components, pipeline_root: Text, direct_num_workers: int\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    beam_arg = [\n",
        "        f\"--direct_num_workers={direct_num_workers}\",\n",
        "    ]\n",
        "    p = pipeline.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_arg,\n",
        "    )\n",
        "    return p"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYoona7QwtKA"
      },
      "source": [
        "from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner, AirflowPipelineConfig\n",
        "# from base_pipeline import init_components # needs to be defined see book repo"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF904_AWwtIc",
        "outputId": "0956eecf-204d-47a1-9e12-1df175e9a474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "components = init_components(\n",
        "    data_dir,\n",
        "    module_file,\n",
        "    serving_model_dir,\n",
        "    training_steps=50000,\n",
        "    eval_steps=10000,\n",
        ")\n",
        "pipe_line = init_pipeline(components, pipeline_root, 0)\n",
        "DAG = AirflowDagRunner(AirflowPipelineConfig(airflow_config)).run(pipe_line)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:35:18,956] {component.py:101} WARNING - The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
            "[2020-10-09 10:35:18,980] {component.py:88} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:35:18,989] {component.py:98} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:35:18,997] {component.py:96} INFO - Excluding no splits because exclude_splits is not set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPDTc-KmTxG"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        " - <a href=\"https://flink.apache.org/\">Apache Flink</a>\n",
        " - <a href=\"https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py\">TFX Example</a>\n",
        " - <a href=\"https://spark.apache.org/\">Spark</a>\n",
        " - <a href=\"https://airflow.apache.org/\">Apache Airflow</a>"
      ]
    }
  ]
}