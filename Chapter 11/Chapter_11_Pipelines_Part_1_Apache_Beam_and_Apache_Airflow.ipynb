{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_UxA69aLg43"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xkTWvW5IcFs"
      },
      "source": [
        "# %cd drive/My\\ Drive/Building\\ ML\\ Pipelines/\n",
        "# !pip install -r requirements.txt\n",
        "# %cd ..\n",
        "# %cd .."
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpJj0bhJIkU5"
      },
      "source": [
        "# Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow\n",
        "\n",
        "In this and the next chapter, we will put all the components together and show how to run the full pipeline with three orchestrators:\n",
        " - Apache Beam\n",
        " - Apache Airflow\n",
        " - Kubeflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmNmEmYAQUKl"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_dir = \"drive/My Drive/Building ML Pipelines/\"\n",
        "chap_dir = base_dir + \"Chapter 11/\"\n",
        "data_dir = base_dir + \"Data/\"\n",
        "out_dir = chap_dir + \"Outputs/\"\n",
        "csv_data_dir = base_dir + \"CSV Data/\"\n",
        "csv_dir = csv_data_dir + \"consumer_complaints_with_narrative.csv\"\n",
        "eval_data_dir = base_dir + \"Chapter 6/Outputs/CsvExampleGen/examples/1/eval/data_tfrecord-00000-of-00001.gz\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KV6jpokmJBeH"
      },
      "source": [
        "## Which Orchestration Tool to Choose?\n",
        "\n",
        "You need to pick only one Tool to run the pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVMQOnULJdGd"
      },
      "source": [
        "### Apache Beam\n",
        "\n",
        "If your are looking for a minimal installation, reusing Beam to orchestrate is a logical choice. It is easy to set up and also allows you to use any existing distributed data processing infrastructure you might already be familiar with (e.g. Google Cloud Dataflow). It could also be used as an intermediate step to ensure the correctness of the pipeline.<br>\n",
        "However, Apache Beam is missing a variety of tools for scheduling your model updates or monitoring the process of a pipeline job, that's where the other two tools shine."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TXQk8iIKF6g"
      },
      "source": [
        "### Apache Airflow\n",
        "\n",
        "It is often used in companies for data-loading tasks. If you use Apache Airflow in combinatino with a production-ready database like PostgreSQL, you can take advantage of executing partial pipelines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJAA9_4KKcdB"
      },
      "source": [
        "### Kubeflow Pipelines\n",
        "\n",
        "If you already have experience with Kubernetes and access to a Kubernetes cluster, it makes sense to consider Kubeflow Pipelines. While the setup is more complicated, it opens up a variety of new opportunities, including the ability to view TFDV and TFMA visualizations, model lineage and the artifact collections. It is further an excellent infrastructure platform to deploy machine learning models. Inference routing through the tool Istio is currently state of the art in the field of machine learning infrastructure.<br>\n",
        "Setting up Kubernetes with a variety of cloud providers is also possible, which makes it very efficient and scalable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gk99G4jtLXXH"
      },
      "source": [
        "### Kubeflow Pipelines on AI Platform\n",
        "\n",
        "Kubeflow pipelines can run on Google's AI Platform, which is part of GCP. This takes care of much of the infrastructure and lets you load data more easily from Google Cloud Storage buckets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3kpYExZL-_B"
      },
      "source": [
        "## Converting Your Interactive TFX Pipeline to a Production Pipeline\n",
        "\n",
        "In order to automate our pipelines, we will need to write a Python script that will run all these components without any input from us.<br>\n",
        "Fortunately, we already have all the pieces of this script, here is a summary:\n",
        " - ExampleGen (Chapter 3)\n",
        " - StatisticsGen (Chapter 4)\n",
        " - SchemaGen (Chapter 4)\n",
        " - ExampleValidator (Chapter 4)\n",
        " - Transform (Chapter 5)\n",
        " - Trainer (chapter 6)\n",
        " - Resolver (Chapter 7)\n",
        " - Evaluator (Chapter 7)\n",
        " - Pusher (Chapter 7)\n",
        "\n",
        "Here is the full base pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHbnNdnzImKY"
      },
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "\n",
        "from tfx.components import CsvExampleGen, Evaluator, ExampleValidator, Pusher, ResolverNode, SchemaGen, StatisticsGen, Trainer, Transform\n",
        "from tfx.components.base import executor_spec\n",
        "from tfx.components.trainer.executor import GenericExecutor\n",
        "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
        "from tfx.proto import pusher_pb2, trainer_pb2\n",
        "from tfx.types import Channel\n",
        "from tfx.types.standard_artifacts import Model, ModelBlessing\n",
        "from tfx.utils.dsl_utils import external_input"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO3hd8VcNJZu"
      },
      "source": [
        "def init_components(csv_data_dir, module_file, serving_model_dir,\n",
        "                   training_steps=2000, eval_steps=200):\n",
        "    \n",
        "    \n",
        "    \n",
        "    examples = external_input(csv_data_dir)\n",
        "    \n",
        "    example_gen = CsvExampleGen(input=examples)\n",
        "    \n",
        "    statistics_gen = StatisticsGen(\n",
        "        examples=example_gen.outputs['examples']\n",
        "    )\n",
        "    \n",
        "    schema_gen = SchemaGen(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        infer_feature_shape=True\n",
        "    )\n",
        "    \n",
        "    example_validator = ExampleValidator(\n",
        "        statistics=statistics_gen.outputs['statistics'],\n",
        "        schema=schema_gen.outputs['schema']\n",
        "    )\n",
        "\n",
        "    transform = Transform(\n",
        "        examples=example_gen.outputs['examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        module_file=module_file\n",
        "    )\n",
        "    \n",
        "    trainer = Trainer(\n",
        "        module_file=module_file,\n",
        "        # Override the executor to load the run_fn() function\n",
        "        # custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "        examples=transform.outputs['transformed_examples'],\n",
        "        schema=schema_gen.outputs['schema'],\n",
        "        transform_graph=transform.outputs['transform_graph'],\n",
        "        train_args=trainer_pb2.TrainArgs(num_steps=training_steps),\n",
        "        eval_args=trainer_pb2.EvalArgs(num_steps=eval_steps)\n",
        "    )\n",
        "\n",
        "    model_resolver = ResolverNode(\n",
        "        instance_name=\"latest_blessed_model_resolver\",\n",
        "        resolver_class=latest_blessed_model_resolver.LatestBlessedModelResolver,\n",
        "        model=Channel(type=Model),\n",
        "        model_blessing=Channel(type=ModelBlessing)\n",
        "    )\n",
        "           \n",
        "    eval_config = tfma.EvalConfig(\n",
        "        model_specs=[tfma.ModelSpec(label_key=\"consumer_disputed\")],\n",
        "        slicing_specs=[\n",
        "            tfma.SlicingSpec(),\n",
        "            tfma.SlicingSpec(feature_keys=[\"product\"]),\n",
        "        ],\n",
        "        metrics_specs=[\n",
        "            tfma.MetricsSpec(\n",
        "                metrics=[\n",
        "                    tfma.MetricConfig(class_name=\"BinaryAccuracy\"),\n",
        "                    tfma.MetricConfig(class_name=\"ExampleCount\"),\n",
        "                    tfma.MetricConfig(class_name=\"AUC\"),\n",
        "                    tfma.MetricConfig(class_name='Precision'),\n",
        "                    tfma.MetricConfig(class_name='Recall')\n",
        "                ],\n",
        "                thresholds={\n",
        "                    \"AUC\": tfma.config.MetricThreshold(\n",
        "                        value_threshold=tfma.GenericValueThreshold(\n",
        "                            lower_bound={\"value\": 0.65}\n",
        "                        ),\n",
        "                        change_threshold=tfma.GenericChangeThreshold(\n",
        "                            direction=tfma.MetricDirection.HIGHER_IS_BETTER,\n",
        "                            absolute={\"value\": 0.01},\n",
        "                        ),\n",
        "                    )\n",
        "                },\n",
        "            )\n",
        "        ],\n",
        "    )\n",
        "\n",
        "\n",
        "    evaluator = Evaluator(\n",
        "        examples=example_gen.outputs[\"examples\"],\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        baseline_model=model_resolver.outputs[\"model\"],\n",
        "        eval_config=eval_config\n",
        "    )\n",
        "\n",
        "    pusher = Pusher(\n",
        "        model=trainer.outputs[\"model\"],\n",
        "        model_blessing=evaluator.outputs[\"blessing\"],\n",
        "        push_destination=pusher_pb2.PushDestination(\n",
        "            filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "                base_directory=serving_model_dir\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "    components = [\n",
        "                  example_gen,\n",
        "                  statistics_gen,\n",
        "                  schema_gen,\n",
        "                  example_validator,\n",
        "                  transform,\n",
        "                  trainer,\n",
        "                  model_resolver,\n",
        "                  evaluator,\n",
        "                  pusher\n",
        "    ]\n",
        "\n",
        "    return components"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaQyt44uNJuI"
      },
      "source": [
        "In the example project, we have split the component instantiation from the pipeline configuration to focus on the pipeline setup for the different orchestrators.<br>\n",
        "The *init_components* function instantiates the components, it requires three inputs in addition to the number of training steps and evaluation steps:\n",
        " - data_dir\n",
        " - module_file\n",
        " - serving_model_dir\n",
        "\n",
        "Besides the minor tweaks to the Google Cloud setup we will discuss in the next Chapter, the component setup will be identical for each orchestration platform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kfebwIlQ7Ml"
      },
      "source": [
        "## Simple Interactive Pipeline Conversion for Beam and Airflow\n",
        "\n",
        "You can convert a notebook to a pipeline via the following steps.<br>\n",
        "For any cells in your notebook that you do not want to export, use %%skip_for_export Jupyter magic command at the start of each cell.<br>\n",
        "1. Set the pipeline name and the orchestration tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_Ys2oS-Nx40"
      },
      "source": [
        "# Alternative \"airflow\"\n",
        "runner_type = \"beam\"\n",
        "pipeline_name = \"consumer_complaints_beam\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8CckVKGRiU1"
      },
      "source": [
        "2. Set all the relevant file paths:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7BAhboicCFu"
      },
      "source": [
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jprX8RdHRhvT"
      },
      "source": [
        "notebook_file = chap_dir + \"Chapter 11: Pipelines Part 1: Apache Beam and Apache Airflow.ipynb\"\n",
        "\n",
        "# Pipeline inputs\n",
        "# Directories are all defined at the top of the notebook\n",
        "requirements_file = os.path.join(base_dir, \"requirements.txt\")\n",
        "module_file = base_dir + \"module.py\"\n",
        "serving_model_dir = out_dir + \"serving_model_dir/\"\n",
        "\n",
        "# Pipeline outputs\n",
        "output_base = os.path.join(out_dir, pipeline_name)\n",
        "serving_model_dir = os.path.join(output_base, pipeline_name)\n",
        "pipeline_root = os.path.join(output_base, \"pipeline_root\")\n",
        "metadata_path = os.path.join(pipeline_root, \"metadata.sqlite\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdIl9nI6cDg6"
      },
      "source": [
        "3. List the components you wish to include in your pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mofq5vjvb_GJ"
      },
      "source": [
        "components = [\n",
        "              example_gen,\n",
        "              statistics_gen,\n",
        "              schema_gen,\n",
        "              example_validator,\n",
        "              transform,\n",
        "              trainer,\n",
        "            #   model_resolver,\n",
        "              evaluator,\n",
        "              pusher\n",
        "    ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gwyy1OjhfPrg"
      },
      "source": [
        "4. Pipeline export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKEJDeCacU7w"
      },
      "source": [
        "pipeline_export_file = \"consumer_complaints_beam_export.py\"\n",
        "context.export_to_pipeline(\n",
        "    notebook_filepath=notebook_file,\n",
        "    export_filepath=pipeline_export_file,\n",
        "    runner_type=runner_type\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEbGRODYhVwe"
      },
      "source": [
        "This export command will generate a script that ca be run using Beam or Airflow depending on the *runner_type* you choose."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqC1oylahd1m"
      },
      "source": [
        "## Introduction to Apache Beam\n",
        "\n",
        "It this section, we will show how to orchestrate our example project using Beam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeyara26iFW4"
      },
      "source": [
        "### Orchestrating TFX Pipelines with Apache Beam\n",
        "\n",
        "Next to things mentioned in the beginning Beam can also be used to debugy our ML pipeline. By using Beam during your pipeline debugging and the nmoving to Airflow or Kubeflow Pipelines, you can rule out root causes of pipeline errors coming from the more complex Airflow or Kubeflow Pipeline setups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbrqRoqNhJvk"
      },
      "source": [
        "import absl\n",
        "from tfx.orchestration import metadata, pipeline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OhCBQXWinCZ"
      },
      "source": [
        "def init_beam_pipeline(components, pipeline_root, direct_num_workers):\n",
        "    absl.logging.info(\"Pipeline root set to: {}\".format(pipeline_root))\n",
        "\n",
        "    beam_arg = [\n",
        "        # Beam lets you specify the number of workers\n",
        "        # A sensible default is half the numbers of avaiable CPUs if there is more tahn one CPU\n",
        "        \"--direct_num_workers={}\".format(direct_num_workers),\n",
        "        \"--requirements_file={}\".format(requirements_file)\n",
        "    ]\n",
        "\n",
        "    # This is where you define your pipeline object with a configuratio\n",
        "    p = pipeline.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=False, # We can set the cache to True if we would like to avoid rerunning components that have already finished\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(metadata_path),\n",
        "        metadata_pipeline_args=beam_arg\n",
        "    )\n",
        "\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cZHARQBULkb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20PEupTj7k8"
      },
      "source": [
        "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7BSHeqUkBhK"
      },
      "source": [
        "direct_num_workers = int(os.cpu_count() / 2)\n",
        "direct_num_workers = 1 if direct_num_workers < 1 else direct_num_workers\n",
        "\n",
        "components = init_components(csv_data_dir, module_file, serving_model_dir,\n",
        "                            training_steps=1000, eval_steps=100)\n",
        "# print(components)\n",
        "pipe_line = init_beam_pipeline(components, pipeline_root, direct_num_workers)\n",
        "BeamDagRunner().run(pipe_line)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnqUsPkxlUFV"
      },
      "source": [
        "This is a minimal setup that you can easily integrate with the rest of your infrastructure or example using a cron job. You can also scale this pipeline using <a href=\"https://flink.apache.org/\">Apache Flink</a> like in this <a href=\"https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py\">TFX Example</a> or <a href=\"https://spark.apache.org/\">Spark</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Vz67jT1mheK"
      },
      "source": [
        "## Introduction to Apache Airflow\n",
        "\n",
        "Airflow is Apache's project for workflow automation. Airflow lets you represent workflow tasks thorugh DAGs (Direct Acylic Graphs) represented via Python code. Airflow also lets you schedule and monitor workflows."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy8viiannD8p"
      },
      "source": [
        "### Installation and Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkdfr0jMGi7M"
      },
      "source": [
        "# !pip install apache-airflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d29H6f1gBli_"
      },
      "source": [
        "For a complete list of Airflow extensions and how to install them look into the <a href=\"\">Airflow documentation</a>.<br>\n",
        "With airflow now installed, you need to create an initial database where all the task status infromation will be stored:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCwseZiymatR",
        "outputId": "17d9c162-293e-4203-d551-31c2873022aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "!airflow initdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "DB: sqlite:////root/airflow/airflow.db\n",
            "[2020-10-09 10:14:46,564] {db.py:378} INFO - Creating tables\n",
            "INFO  [alembic.runtime.migration] Context impl SQLiteImpl.\n",
            "INFO  [alembic.runtime.migration] Will assume non-transactional DDL.\n",
            "WARNI [airflow.models.crypto] cryptography not found - values will not be stored encrypted.\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oY9CM3BrHROI"
      },
      "source": [
        "Airflow will initiate and SQLite database if not configurations have been changed. This setup works for demo purposes and run smaller projects, but for larger projects look in the <a href=\"https://airflow.apache.org/docs/stable/howto/index.html\">Apache Airflow documentation</a>.<br>\n",
        "A minimal Airflow setup consists of the Airflow scheduler, which coordinates the tasks and task dependencies, as well as a web server, which provides a UI to start, stop and monitor the tasks."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvrtdK0Qmama",
        "outputId": "997af130-48b7-4ae0-bca7-723ebe8219a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        }
      },
      "source": [
        "!airflow scheduler"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
            "[2020-10-09 09:24:03,359] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1367} INFO - Starting the scheduler\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1375} INFO - Running execute loop for -1 seconds\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1376} INFO - Processing each file at most -1 times\n",
            "[2020-10-09 09:24:03,373] {scheduler_job.py:1379} INFO - Searching for files in /root/airflow/dags\n",
            "[2020-10-09 09:24:03,376] {scheduler_job.py:1381} INFO - There are 25 files in /root/airflow/dags\n",
            "[2020-10-09 09:24:03,377] {scheduler_job.py:1438} INFO - Resetting orphaned tasks for active dag runs\n",
            "[2020-10-09 09:24:03,386] {dag_processing.py:562} INFO - Launched DagFileProcessorManager with pid: 2939\n",
            "[2020-10-09 09:24:03,393] {settings.py:55} INFO - Configured default timezone <Timezone [UTC]>\n",
            "[2020-10-09 09:24:03,408] {dag_processing.py:776} WARNING - Because we cannot use more than 1 thread (max_threads = 2) when using sqlite. So we set parallelism to 1.\n",
            "1\n",
            "\n",
            "y\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-309581388ff6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'airflow scheduler'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    438\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m   result = _run_command(\n\u001b[0;32m--> 440\u001b[0;31m       shell.var_expand(cmd, depth=2), clear_streamed_output=False)\n\u001b[0m\u001b[1;32m    441\u001b[0m   \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_INTERRUPTED_SIGNALS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    220\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    267\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    270\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4aTmbl4mai8",
        "outputId": "087503b0-a665-4d9b-d170-bd05366cc085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Start the Airflow web server\n",
        "# The command argument -p sets the port where your web browser can access the Airflow interface.\n",
        "!airflow webserver -p 8081"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "  ____________       _____________\n",
            " ____    |__( )_________  __/__  /________      __\n",
            "____  /| |_  /__  ___/_  /_ __  /_  __ \\_ | /| / /\n",
            "___  ___ |  / _  /   _  __/ _  / / /_/ /_ |/ |/ /\n",
            " _/_/  |_/_/  /_/    /_/    /_/  \\____/____/|__/\n",
            "[2020-10-09 09:40:28,707] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:28,707] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "Running the Gunicorn Server with:\n",
            "Workers: 4 sync\n",
            "Host: 0.0.0.0:8081\n",
            "Timeout: 120\n",
            "Logfiles: - -\n",
            "=================================================================            \n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Starting gunicorn 20.0.4\n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Listening at: http://0.0.0.0:8081 (3750)\n",
            "[2020-10-09 09:40:29 +0000] [3750] [INFO] Using worker: sync\n",
            "[2020-10-09 09:40:29 +0000] [3754] [INFO] Booting worker with pid: 3754\n",
            "[2020-10-09 09:40:29 +0000] [3755] [INFO] Booting worker with pid: 3755\n",
            "[2020-10-09 09:40:29 +0000] [3756] [INFO] Booting worker with pid: 3756\n",
            "[2020-10-09 09:40:29 +0000] [3757] [INFO] Booting worker with pid: 3757\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:40:30,956] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:30,957] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,032] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,040] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,421] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,424] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:31,425] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:31,428] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:40:59 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:40:59 +0000] [3773] [INFO] Booting worker with pid: 3773\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:40:59,591] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:40:59,592] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:41:00 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:41:00 +0000] [3754] [INFO] Worker exiting (pid: 3754)\n",
            "[2020-10-09 09:41:29 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:41:29 +0000] [3781] [INFO] Booting worker with pid: 3781\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:41:30,049] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:41:30,050] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:41:30 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:41:30 +0000] [3755] [INFO] Worker exiting (pid: 3755)\n",
            "[2020-10-09 09:42:00 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:42:00 +0000] [3791] [INFO] Booting worker with pid: 3791\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:42:00,551] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:42:00,552] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:42:01 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:42:01 +0000] [3756] [INFO] Worker exiting (pid: 3756)\n",
            "[2020-10-09 09:42:30 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:42:30 +0000] [3799] [INFO] Booting worker with pid: 3799\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:42:31,099] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:42:31,100] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:42:31 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:42:31 +0000] [3757] [INFO] Worker exiting (pid: 3757)\n",
            "[2020-10-09 09:43:01 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:43:01 +0000] [3812] [INFO] Booting worker with pid: 3812\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:43:01,512] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:43:01,513] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:43:02 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:43:02 +0000] [3773] [INFO] Worker exiting (pid: 3773)\n",
            "[2020-10-09 09:43:31 +0000] [3750] [INFO] Handling signal: ttin\n",
            "[2020-10-09 09:43:31 +0000] [3823] [INFO] Booting worker with pid: 3823\n",
            "/usr/local/lib/python3.6/dist-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
            "  \"\"\")\n",
            "[2020-10-09 09:43:32,026] {__init__.py:50} INFO - Using executor SequentialExecutor\n",
            "[2020-10-09 09:43:32,027] {dagbag.py:417} INFO - Filling up the DagBag from /root/airflow/dags\n",
            "[2020-10-09 09:43:32 +0000] [3750] [INFO] Handling signal: ttou\n",
            "[2020-10-09 09:43:32 +0000] [3781] [INFO] Worker exiting (pid: 3781)\n",
            "[2020-10-09 09:43:52 +0000] [3750] [INFO] Handling signal: int\n",
            "[2020-10-09 09:43:52,501] {cli.py:1127} INFO - Received signal: 2. Closing gunicorn.\n",
            "[2020-10-09 09:43:52 +0000] [3823] [INFO] Worker exiting (pid: 3823)\n",
            "[2020-10-09 09:43:52 +0000] [3799] [INFO] Worker exiting (pid: 3799)\n",
            "[2020-10-09 09:43:52 +0000] [3791] [INFO] Worker exiting (pid: 3791)\n",
            "[2020-10-09 09:43:52 +0000] [3812] [INFO] Worker exiting (pid: 3812)\n",
            "[2020-10-09 09:43:53 +0000] [3750] [INFO] Shutting down: Master\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AGr_B02rZe_"
      },
      "source": [
        "Go to: http://127.0.0.1:8081 and you should see an interface.\n",
        "\n",
        "**Airflow Configuration**\n",
        "The default settings of Airflow can be overwritten by chaning the relevant parameters in the Airflow configuration. The new settings have to be defined in ~/airflow/airflow.cfg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqysLLW-r9so"
      },
      "source": [
        "### Basic Airflow Example\n",
        "\n",
        "Here we will not include any TFX scripts. Workflow pipelines are defined as Python scripts and Airflow expects the DAG definitions to be located in ~/airflow/dags. A basic pipeline consists of:\n",
        " - Project-Specific Configurations\n",
        " - Task Definitions\n",
        " - Definition of the Task Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ohe36TsVsenC"
      },
      "source": [
        "#### Project-Specific Configurations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFYKhzi3rZCJ"
      },
      "source": [
        "from airflow import DAG\n",
        "from datetime import datetime, timedelta"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rI0AKOZ2mag8"
      },
      "source": [
        "# Location to define the project configuration\n",
        "project_cfg = {\n",
        "    \"owner\": \"airflow\",\n",
        "    \"email\": [\"jankezmann@t-online.de\"],\n",
        "    \"email_on_failure\": True,\n",
        "    \"start_date\": datetime(2019, 8, 1),\n",
        "    \"retries\": 1,\n",
        "    \"retry_delay\": timedelta(hours=1)\n",
        "}\n",
        "\n",
        "# The DAG object will be picked up by Airflow\n",
        "dag = DAG(\n",
        "    \"basic_pipeline\",\n",
        "    default_args=project_cfg,\n",
        "    schedule_interval=timedelta(days=1)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM8N8kIGtKsQ"
      },
      "source": [
        "#### Task Definitions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmTS08xUmafh"
      },
      "source": [
        "from airflow.operators.python_operator import PythonOperator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OquJGytCmacO"
      },
      "source": [
        "def example_task(_id, **kwargs):\n",
        "    print(\"task {}\".format(_id))\n",
        "    return \"completed task {}\".format(id)\n",
        "\n",
        "task_1 = PythonOperator(\n",
        "    task_id=\"task_1\",\n",
        "    provide_context=True,\n",
        "    python_callable=example_task,\n",
        "    op_kwargs={\"_id\": 1},\n",
        "    dag=dag,\n",
        ")\n",
        "\n",
        "\n",
        "task_2 = PythonOperator(\n",
        "    task_id=\"task_2\",\n",
        "    provide_context=True,\n",
        "    python_callable=example_task,\n",
        "    op_kwargs={\"_id\": 2},\n",
        "    dag=dag,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9im6gKHjwlOg"
      },
      "source": [
        "In a TFX pipeline, you do not need to define these tasks because the TFX library takes care of it for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSk0IVndwsVR"
      },
      "source": [
        "#### Task dependencies\n",
        "\n",
        "Lets assume task_2 depends on task_1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0BxXFULwtAb"
      },
      "source": [
        "task_1.set_downstream(task_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXxe6xAExBGA"
      },
      "source": [
        "Airflow also offer as bit-shift operator to denote the task dependencies:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgDkMwQmwtRp",
        "outputId": "2b315abb-765c-4bcb-e08a-d13da9129e00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "task_1 >> task_2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Task(PythonOperator): task_2>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6qEcCIyxNIq"
      },
      "source": [
        "#### Putting it all together\n",
        "\n",
        "In your DAG folder in your AIRFLOW_HOME path, usually at ~/airflow/dags, create a new file basic_pipeline.py: See details pages 332f.<br>\n",
        "You can then test the pipeline setup by executing this command in your terminal\n",
        "\n",
        "```\n",
        "python ~/airflow/dags/basic_pipeline.py\n",
        "```\n",
        "\n",
        "The log file can be found at:\n",
        "```\n",
        "~/airflow/logs/NAME OF YOUR PIPELINE/TASK NAME/EXECTUION TIME/\n",
        "```\n",
        "\n",
        "If we want to inspect the result of the first task from our bsic pipeline, we have to investigate the log file:\n",
        "```\n",
        "cat ../logs/basic_pipeline/task_1/.../1.log\n",
        "```\n",
        "\n",
        "To test whether Airflow recognized the new pipeline you can execute:\n",
        "```\n",
        "!airflow list_dags\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksk7x9VrzFHT"
      },
      "source": [
        "## Orchestrating TFX Pipelines with Apache Airflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OYW853_zI8i"
      },
      "source": [
        "### Pipeline Setup\n",
        "\n",
        "Instead of importing the BeamDagRunner, we will use the AirflowDAGRunner. Again the files for an Airflow pipeline need to be located in the ~/airflow/dags folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7XtPSwRwtWW"
      },
      "source": [
        "airflow_config = {\n",
        "    \"schedule_interval\": None,\n",
        "    \"start_date\": datetime(2020, 10, 9),\n",
        "    \"pipeline_name\": \"your_ml_pipeline\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LctSUIx90aDs"
      },
      "source": [
        "from typing import Text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5u0kASmwtMO"
      },
      "source": [
        "def init_pipeline(\n",
        "    components, pipeline_root: Text, direct_num_workers: int\n",
        ") -> pipeline.Pipeline:\n",
        "\n",
        "    beam_arg = [\n",
        "        f\"--direct_num_workers={direct_num_workers}\",\n",
        "    ]\n",
        "    p = pipeline.Pipeline(\n",
        "        pipeline_name=pipeline_name,\n",
        "        pipeline_root=pipeline_root,\n",
        "        components=components,\n",
        "        enable_cache=True,\n",
        "        metadata_connection_config=metadata.sqlite_metadata_connection_config(\n",
        "            metadata_path\n",
        "        ),\n",
        "        beam_pipeline_args=beam_arg,\n",
        "    )\n",
        "    return p"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYoona7QwtKA"
      },
      "source": [
        "from tfx.orchestration.airflow.airflow_dag_runner import AirflowDagRunner, AirflowPipelineConfig\n",
        "# from base_pipeline import init_components # needs to be defined see book repo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TF904_AWwtIc",
        "outputId": "0956eecf-204d-47a1-9e12-1df175e9a474",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "components = init_components(\n",
        "    data_dir,\n",
        "    module_file,\n",
        "    serving_model_dir,\n",
        "    training_steps=50000,\n",
        "    eval_steps=10000,\n",
        ")\n",
        "pipe_line = init_pipeline(components, pipeline_root, 0)\n",
        "DAG = AirflowDagRunner(AirflowPipelineConfig(airflow_config)).run(pipe_line)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-10-09 10:35:18,956] {component.py:101} WARNING - The \"input\" argument to the CsvExampleGen component has been deprecated by \"input_base\". Please update your usage as support for this argument will be removed soon.\n",
            "[2020-10-09 10:35:18,980] {component.py:88} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:35:18,989] {component.py:98} INFO - Excluding no splits because exclude_splits is not set.\n",
            "[2020-10-09 10:35:18,997] {component.py:96} INFO - Excluding no splits because exclude_splits is not set.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWPDTc-KmTxG"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        " - <a href=\"https://flink.apache.org/\">Apache Flink</a>\n",
        " - <a href=\"https://github.com/tensorflow/tfx/blob/master/tfx/examples/chicago_taxi_pipeline/taxi_pipeline_portable_beam.py\">TFX Example</a>\n",
        " - <a href=\"https://spark.apache.org/\">Spark</a>\n",
        " - <a href=\"https://airflow.apache.org/\">Apache Airflow</a>"
      ]
    }
  ]
}