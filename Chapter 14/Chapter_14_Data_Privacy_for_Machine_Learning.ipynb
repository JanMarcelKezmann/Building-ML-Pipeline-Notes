{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 14: Data Privacy for Machine Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NIQhFX3ZEjsP"
      },
      "source": [
        "# Chapter 14: Data Privacy for Machine Learning\n",
        "\n",
        "Three main methods for privacy-preserving machine learning will be covered:\n",
        " - Differential Privacy\n",
        " - Federated Privacy\n",
        " - Encrypted Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkkOZzl9UA4-"
      },
      "source": [
        "## Data Privacy Issues\n",
        "\n",
        "Answer the following questions to decide which privacy-preserving mahine learning method to choose:\n",
        " - Who are you trying to keep the data private from?\n",
        " - Which parts of the system can be private and which can be exposed to the world?\n",
        " - Who are the trusted parties that can view the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etO5yVI5Wrhi"
      },
      "source": [
        "### The Simplest Way to Increase Privacy\n",
        "\n",
        "Only collect data that is necessary and only give the model the data that it really needs for a good prediction, with that many fields like name, gender or similary can be deleted without any negative effects. But be careful about potential biases that could result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_NRaTgXncn"
      },
      "source": [
        "### What Data needs to be kept Private?\n",
        "\n",
        " - Personally Identifying Information (PII) is data that can directly identify a single person\n",
        " - Sensitive data, often defined as data that could harm someone if it were released (Health Data, Financial Data, ...)\n",
        " - quasi-identifying data, i.e. data that could uniqely identify somenone if enough data is known."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDQKaYcuah7Q"
      },
      "source": [
        "## Differential Privacy (DP)\n",
        "\n",
        "DP is a formalization of the idea that a query or a transformation of a dataset should not reveal whether a person is in that dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RIpI2i6bI3B"
      },
      "source": [
        "### Local and Global Differential Privacy\n",
        "\n",
        "In local DP noise or randomness is added at the individual level, so privacy is maintained between an individual and the collector of the data.<br>\n",
        "In global DP noise is added to a transformation on the entire dataset. The data collector is trusted with the raw data, but the result of the transformation does not reveal data about an individual."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOXVDC35brx7"
      },
      "source": [
        "### Epsilon, Delta and the Privacy Budget\n",
        "\n",
        "See for details page 404."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7sgd6-pcCGQ"
      },
      "source": [
        "### Differential Privacy for Machine Learning\n",
        "\n",
        "Places DP can be used:\n",
        " - Federated Learning System\n",
        "  - Either local or global DP\n",
        " -TensorFlwo Privacy Library\n",
        "  - Global DP (Raw data is available for model training)\n",
        " - Private Aggregation of Teacher Ensembles (PATE) approach"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2L_Jdr3ct5L"
      },
      "source": [
        "## Introduction to TensorFlow Privacy\n",
        "\n",
        "TensorFlow Privacy (TFP) adds DP to an optimizer during model training. The type of DP used in TFP is an example of global DP, i.e. noise is added during trainign so that private data is not exposed in a model's prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDJ5W6qVdDmh"
      },
      "source": [
        "### Training with a Differentially Private Optimizer\n",
        "\n",
        "TFP can be installed with pip, but does currently only work with TensorFlow version 1.x:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjK6D-f5Ei-_"
      },
      "source": [
        "!pip install tensorflow_privacy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWg3O_WSEeW5"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMMr1pfTdqQR"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
        "])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfEZe2Aed1Pk"
      },
      "source": [
        "The differentially private optimizer requires that we set two extra hyperparameters compared to a normal tf.keras model:\n",
        " - Noise Multiplier\n",
        " - L2 Norm Clip\n",
        "\n",
        "It's best to tune these to suit your dataset and measure their impact on $\\epsilon$:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zem9eUlddzqI"
      },
      "source": [
        "# DP parameters\n",
        "NOISE_MULTIPLIER = 1.1\n",
        "NUM_MICROBATCHES = 32 # The batch size must be exaclty divisible by the number of microbatches\n",
        "LEARNING_RATE = 0.1\n",
        "POPULATION_SIZE = 1000 # The number of examples in the training set\n",
        "L2_NORM_CLIP = 1.0\n",
        "BATCH_SIZE = 32 # The population size must be exactly divisible by the batch size\n",
        "EPOCHS = 1"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCHX45LYe1LM"
      },
      "source": [
        "Initialize the differentially private optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJyoBgWde7ti"
      },
      "source": [
        "from tensorflow_privacy.privacy.optimizers.dp_optimizer import DPGradientDescentGaussianOptimizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00egPC6qfAuI"
      },
      "source": [
        "optimizer = DPGradientDescentGaussianOptimizer(\n",
        "    l2_norm_clip=L2_NORM_CLIP,\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    num_microbatches=NUM_MICROBATCHES,\n",
        "    learning_rate=LEARNING_RATE\n",
        ")\n",
        "\n",
        "# Loss must be calculated on a per-example basis rather than over an entire mini-batch\n",
        "loss = tf.keras.losses.BinaryCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=tf.losses.Reduction.NONE\n",
        ")"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSaPohh_fcoy"
      },
      "source": [
        "Training a private model is just like training a normal tf.keras model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8JWgProfYh7"
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(X_train,\n",
        "          y_train,\n",
        "          epochs=EPOCHS,\n",
        "          validation_data=(X_text, y_test),\n",
        "          batch_size=BATCH_SIZE\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqBdTWNof70U"
      },
      "source": [
        "### Calculating Epsilon\n",
        "\n",
        "Now, we calculate the differential privacy parameters for our model and our choice of noise multiplier and gradient clip:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NOSZ-DlgFaO"
      },
      "source": [
        "from tensorflow_privacy.privacy.analysis import compute_dp_sgd_privacy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu494RktgKnw",
        "outputId": "4f0a4016-bc79-46f0-ebd6-461d16b13308",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "compute_dp_sgd_privacy.compute_dp_sgd_privacy(\n",
        "    n=POPULATION_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    noise_multiplier=NOISE_MULTIPLIER,\n",
        "    epochs=EPOCHS,\n",
        "    delta=1e-4 # The value of delta is set to 1/(set size of the dataset), round to the nearest order of magnitude\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "DP-SGD with sampling rate = 3.2% and noise_multiplier = 1.1 iterated over 32 steps satisfies differential privacy with eps = 1.72 and delta = 0.0001.\n",
            "The optimal RDP order is 8.0.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.715653096511666, 8.0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HwDoXEygzy1"
      },
      "source": [
        "## Federated Learning (FL)\n",
        "\n",
        "FL is a protocol where the training of a machine learning model is distributed accross many different devices and the trained model is combined on a central server. The key point is that the raw data never leaves the separate devices and is never pooled in one place. This is very different from the traditional architecture of gathering a dataset in a central location and then training a model.<br>\n",
        "Useful applications:\n",
        " - Mobile phones\n",
        " - User's browser\n",
        " - Sharing of sensitive data that is distributed accross multiple data owners.\n",
        "\n",
        "FL is most useful in use cases that share the following characteristics:\n",
        " - The data required for the model can only be collected from distributed sources\n",
        " - The number of data sources is large\n",
        " - The data is sensitive in some way\n",
        " - The data does not require extra labelling - the labels are provided directly by the user and do not leave the source\n",
        " - Ideally, the data is drwan from close to identical distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnpNOvbjkkeS"
      },
      "source": [
        "### Federeated Learning in TensorFlow\n",
        "\n",
        "TensorFlow Federated (TFF) simulates the distributed setup of FL and contains a version of SGD that can calculate updates on distributed data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kZNiqNBlGk4"
      },
      "source": [
        "## Encrypted Machine Learning\n",
        "\n",
        "Encrypted Machien Learning leans on technology and research from the cryptographic community and applies these techniques to machine learning. The major methods that have been adopted so far are homomorphic encryption (HE) and secure multiparty computation (SMPC).<br>\n",
        "There are two ways to using these techniques:\n",
        " - Encrypting a model that has already been trained on plain text data\n",
        " - Encrypting an entire system (if the data must stay encrypted during training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VunTWJ5PmAXQ"
      },
      "source": [
        "### Encrypted Model Training\n",
        "\n",
        "Useful when training models on encrypted data. This is useful when the raw data needs to be kept private from the data scientist training the model or when two or more parties own the raw data and want to train a model using all parties' data, but don't want to share the raw data.<br>\n",
        "TFE can be used to train an encrypted model for this use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGL51ryxgzIX"
      },
      "source": [
        "!pip install tf_encrypted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRLOxcNvmu_f"
      },
      "source": [
        "First step in building a TFE model is to define a class that yields training data in batches. This class is implemented locally by the data owner(s). It is converted to encrypted data using a decorator:\n",
        "    \n",
        "    @tfe.local_computation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ewQruP7gzKK"
      },
      "source": [
        "import tf_encrypted as tfe"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYXuMpgegzQJ"
      },
      "source": [
        "model = tfe.keras.Sequential()\n",
        "model.add(tfe.keras.layers.Dense(1, batch_input_shape=[batch_size, num_features]))\n",
        "model.add(tfe.keras.layers.Activation(\"sigmoid\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuV9G2IDnhVH"
      },
      "source": [
        "### Converting a Trained Model to Serve Encrypted Predictions\n",
        "\n",
        "The second scenario where TFE is useful is when you would like to serve encrypted model that have been trained on plain-text data. In this case you have full access to the unencrypted training data, but you want the users of your application to be able to receive private predictions. This provides privacy to the users, who upload encrypted data and receive an enrypted prediction.<br>\n",
        "Keras models can be converted to TFE models via:\n",
        "\n",
        "    tfe_model = tfe.keras.models.clone_model(model)\n",
        "\n",
        "In this scenario, the following steps need to be carried out:\n",
        " - Load and preprocess the data locally on the client\n",
        " - Encrypt the data on the client\n",
        " - Send the encrypted data to the servers\n",
        " - Make a prediction on the encrypted data\n",
        " - Send the encrypted prediction to the client\n",
        " - Decrypt the predcition on the client and show the result to the user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gG0WR9mngkFM"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        " - <a href=\"https://github.com/tensorflow/privacy\">TensorFlow Privacy Github</a>\n",
        " - <a href=\"https://medium.com/dropoutlabs/introducing-pysyft-tensorflow-cc361ac75137\">PySyft TensorFlow Medium Article</a>\n",
        " - <a href=\"https://tf-encrypted.io/\">TensorFlow Encrypted</a>\n",
        " - <a href=\"https://github.com/tf-encrypted/tf-encrypted/tree/master/examples/notebooks/keras-training\">TensorFlow Encrypted Github</a>\n",
        " - <a href=\"https://github.com/tf-encrypted/tf-encrypted/tree/master/examples/notebooks\">TFE Notebook examples for serving private predictions</a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLKnUqrNge6x"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}