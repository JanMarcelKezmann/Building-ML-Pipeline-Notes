{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 12: Pipelines Part 2: Kubeflow Pipelines.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n21G2fNe7-OD"
      },
      "source": [
        "# Chapter 12: Pipelines Part 2: Kubeflow Pipelines\n",
        "\n",
        "Even though the installation is harder than the ones of Apache Airflow or Apache Beam, it provides great features including *Pipeline Lineage Browser*, *TensorBoard Integration* and the ability to view TFDV and TFMA visualizations. Further more it leverages the advantages of Kubernetes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN2Kddmx93Qm"
      },
      "source": [
        "## Introduction to Kubeflow Pipelines\n",
        "\n",
        "While Apache Airflow was designed for ETL (Extract, Transform, Load) processes, Kubeflow Pipelines has the end-to-end execution of machine learning pipelines at its heart. Kubeflow further provide its own Software Development Kit (SDK) to build Docker containers for pipeline runs or to orchestrate containers.<br>\n",
        "When you install Kubeflow Pipelines, Kubeflow Pipelines will install a variety of tools, including the UI, the workflow controller, a MySQL database instance and the ML MetadataStore."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJHhlIgj9Dri"
      },
      "source": [
        "### Installation and Initial Setup\n",
        "\n",
        "Details on the installation, the initial setup and the accessing of your Kubeflow Pipelines Installation are on the pages 345 - 349.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2dKBC550Tjh"
      },
      "source": [
        "## Orchestrating TFX Pipelines with Kubeflow Pipelines\n",
        "\n",
        "TFX KubeflowRunner will convert our Python TFX scripts with all the component specifications to Argo instructions, which can the be executed with Kubeflow Piplines. Argo will spin up each TFX component as its own Kubernentes pod and run the TFX Executor for the specific component in the container."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bl2ZNtk42Y1x"
      },
      "source": [
        "### Pipeline Setup\n",
        "\n",
        "Details on the pipeline setup can be found on the pages 351-357.<br>\n",
        "There are three important arguments to configure the TFX setup in our Kubeflow Pipelines setup:\n",
        " - kubeflow_metedata_config\n",
        " - tfx_image\n",
        " - pipeline_operator_funcs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyLA0kSh4BLI"
      },
      "source": [
        "### Executing the Pipeline\n",
        "\n",
        "Again it is easier to follow the images and documentation in the book, see pages 357-365."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cQX9R5z42sl"
      },
      "source": [
        "### Usefule Features of Kubeflow Pipelines\n",
        "\n",
        " - Restart failed pipelines\n",
        " - Recurring runs\n",
        " - Collaborating and reviewing pipeline runs\n",
        " - Auditing the pipeline lineage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9Fs7npe5h71"
      },
      "source": [
        "## Pipelines Based on Google Cloud AI Platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "415SfxKm5tZa"
      },
      "source": [
        "Again see for pages 371-382."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Jrl2PJy0YT9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}