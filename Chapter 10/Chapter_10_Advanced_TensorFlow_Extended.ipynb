{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 10: Advanced TensorFlow Extended.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAFXJ-oOFjHC"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eEtciw6FmV9"
      },
      "source": [
        "%cd drive/My\\ Drive/Building\\ ML\\ Pipelines/\n",
        "!pip install -r requirements.txt\n",
        "%cd ..\n",
        "%cd .."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5QiyGQlZtNA"
      },
      "source": [
        "# Chapter 10: Advanced TensorFlow Extended\n",
        "\n",
        "This chapter focuses on building your own TFX components or more compelx pipeline graphs. Furthermore advanced concepts of pipeline structures will be introduced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_7My-RZaN5d"
      },
      "source": [
        "## Advanced Pipeline Concepts\n",
        "\n",
        "Three additional concepts will be discussed:\n",
        " - Training multiple models simultaneously\n",
        " - Exporting models for mobile deployments\n",
        " - Warm starting model training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmXp1Do2ayOq"
      },
      "source": [
        "### Training Multiple Models Simultaneously\n",
        "\n",
        "You can assembly a graph with mutliple models with TFX by defining multiple Trainer components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoKL4KUlaxAq"
      },
      "source": [
        "# Function to instantiate the Trainer efficiently\n",
        "def set_trainer(module_file, instance_name, train_steps=5000, eval_steps=100):\n",
        "    return Trainer(module_file=trainer_file,\n",
        "                   custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "                   examples=transform.outputs['transformed_examples'],\n",
        "                   schema=schema_gen.outputs['schema'],\n",
        "                   transform_graph=transform.outputs['transform_graph'],\n",
        "                   train_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
        "                   eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS),\n",
        "                   instance_name=instance_name\n",
        "    )\n",
        "\n",
        "# Load moduel for each Trainer\n",
        "prod_module_file = os.path.join(pipeline_dir, \"prod_module.py\")\n",
        "trial_module_file = os.path.join(pipeline_dir, \"trial_module.py\")\n",
        "\n",
        "# Instantiate a Trainer component for each graph branch\n",
        "trainer_prod_model = set_trainer(module_file, \"production_model\")\n",
        "trainer_trial_model = set_trainer(trial_module_file, \"trial_model\", train_steps=10000, eval_steps=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRsx3lV4cWlQ"
      },
      "source": [
        "Each instantiated training component needs to be consumed by its own Evaluator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t15uN5_FccI2"
      },
      "source": [
        "evaluator_prod_model = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer_prod_model.outputs[\"model\"],\n",
        "    eval_config=eval_config_prod_model,\n",
        "    instance_name=\"production_model\"\n",
        ")\n",
        "\n",
        "evaluator_trial_model = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=trainer_trial_model.outputs[\"model\"],\n",
        "    eval_config=eval_config_trial_model,\n",
        "    instance_name=\"trial_model\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC7XV4szc0Sr"
      },
      "source": [
        "### Exporting TFLite Models\n",
        "\n",
        "Very few changes are required for mobile deployment compared to deployment to model servers, like discussed in Chapter 8.\n",
        "\n",
        "**TFLite Limitations**\n",
        "Because of hardware limitations of mobile and edge devices, TFLite doesn't support all TensorFlow operations. Therefore not every model can be converted to a TFLite-compatible model. For more information see the <a href=\"https://www.tensorflow.org/lite\">TFLite Website</a>\n",
        "\n",
        "We can use the branch strategy of the above section and amend the run_fn function of the module file to rewrite the saved models to a TFLite-compatible format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzb8KHizdhXB"
      },
      "source": [
        "from tfx.components.trainer.executor import TrainerFnArgs\n",
        "from tfx.components.trainer.rewriting import converters, rewriter, rewriter_factory"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2Y3bxaddhei"
      },
      "source": [
        "def fun_fn(fn_args: TrainerFnArgs):\n",
        "    ...\n",
        "    temp_saving_model_dir = os.path.join(fn_args.serving_model_dir, \"temp\")\n",
        "    # Export the model as saved model\n",
        "    model.save(temp_saving_model_dir, save_format=\"tf\", signatures=signatures)\n",
        "\n",
        "    # Instantiate the TFLite rewriter\n",
        "    tfrw = rewriter_factory.create_rewriter(\n",
        "        rewriter_factory.TFLITE_REWRITER,\n",
        "        name=\"tflite_rewriter\",\n",
        "        enable_experimental_new_converter=True\n",
        "    )\n",
        "\n",
        "    # Convert the model to TFLite format\n",
        "    converters.rewrite_saved_model(\n",
        "        temp_saving_model_dir,\n",
        "        fn_args.serving_model_dir,\n",
        "        tfrw,\n",
        "        rewriter.ModelType.TFLITE_MODEL\n",
        "    )\n",
        "\n",
        "    # Delete the saved model after conversion\n",
        "    tf.io.gfile.rmtree(temp_saving_model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJzRrZwe0P7"
      },
      "source": [
        "Evaluate the TFLite-compliant model, which is helpful in detecting whether the model optimizations (e.g. quantization) have led to a degradation of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-_mNmL_fZv9"
      },
      "source": [
        "import tensorflow_model_analysis as tfma\n",
        "from tfx.components import Evaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2wY2eKodhbH"
      },
      "source": [
        "eval_config = tfma.EvalConfig(\n",
        "    models_specs=[tfma.ModelSpec(label_key=\"my_label\", model_type=tfma.TF_LITE)],\n",
        "    ...\n",
        ")\n",
        "\n",
        "evaluator = Evaluator(\n",
        "    examples=example_gen.outputs[\"examples\"],\n",
        "    model=traininer_mobile_model.outputs[\"model\"],\n",
        "    eval_config=eval_config,\n",
        "    instance_name=\"tflite_model\"\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xnZlVWpfqkf"
      },
      "source": [
        "With this presented pipeline setup, we can now produce models for mobile deployment automatically and push them in the artifact stores for model deployment in mobiel apps.<br>\n",
        "For example a Pusher component could ship the produced TFLite model to a cloud bucket where a obile developer could pick up the model and deploy it with <a href=\"https://developers.google.com/ml-kit\">Google's ML Kit</a> in an iOS or Android mobile app.\n",
        "\n",
        "**Converting Models to TensorFlow.js**\n",
        "This new feature allows to deploy model to web browser and Node.js runtime environments. For details see page 287."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q59Pqpxmgvt2"
      },
      "source": [
        "### Warm Starting Model Training\n",
        "\n",
        "In a TFX pipeline, warm start training requires the Resolver component that we introduced in Chapter 7. The Resolver picks up the details of the latest trained model and passes them on to the Trainer component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlVonm0bh3Xi"
      },
      "source": [
        "latest_model_resolver = ResolverNode(\n",
        "    instance_name=\"latest_model_resolver\",\n",
        "    resolver_class=latest_artifacts_resolver.LatestArtifactsResolver,\n",
        "    latest_model=Channel(type=Model)\n",
        ")"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "in6rsIvFiMYf"
      },
      "source": [
        "The latest model is then passed to the Trainer using the base_model argument."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcMpmWteiEnk"
      },
      "source": [
        "trainer = Trainer(\n",
        "    module_file=trainer_file,\n",
        "    transformed_examples=transform.outputs[\"transformed_examples\"],\n",
        "    custom_executor_spec=executor_spec.ExecutorClassSpec(GenericExecutor),\n",
        "    examples=transform.outputs['transformed_examples'],\n",
        "    schema=schema_gen.outputs['schema'],\n",
        "    base_model=latest_model_resolver.outputs[\"latest_model\"],\n",
        "    transform_graph=transform.outputs['transform_graph'],\n",
        "    rain_args=trainer_pb2.TrainArgs(num_steps=TRAINING_STEPS),\n",
        "    eval_args=trainer_pb2.EvalArgs(num_steps=EVALUATION_STEPS),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrRAUVbMikLA"
      },
      "source": [
        "## Human in the Loop\n",
        "\n",
        "The idea is to let a human review the trained model after the automatic model analysis, to spot check the trained model or to gain confidence in the automated pipeline setup. This would mean that the human generate the *blessing* which allows the Pusher component to push the model. This whole setup works via Slack, for dtails see pages 289ff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4uOyh6xjcIj"
      },
      "source": [
        "### Slack Component Setup\n",
        "\n",
        "See pages 291f."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32Eo1yjtjjr0"
      },
      "source": [
        "### How to use The Slack Component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KGgdMJDjpPd"
      },
      "source": [
        "from slack_component.component import SlackComponent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ezny9a9OjpY2"
      },
      "source": [
        "slack_validator = SlackComponent(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    model_blessing=model_validator.outputs[\"blessing\"],\n",
        "    # Load the Slack token from your environment\n",
        "    slack_token=os.environ[\"SLACK_BOT_TOEKN\"],\n",
        "    # Specify the channel where the message should appear\n",
        "    slack_channel_id=\"my-channel-id\",\n",
        "    timeout_sec=3600 # in seconds\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxoJq1SKkM_m"
      },
      "source": [
        "When executed, the component will post a message and wait up to an hour (defined in timeout_sec) for an answer. During this time, a data scientist can evaluate the model and respond with their approval or rejection. The downstream component (e.g. a Pusher component) can consume the result from the Slack component, as shown in the following code:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXIZ43B8jpVg"
      },
      "source": [
        "pusher = Pusher(\n",
        "    model=trainer.outputs[\"model\"],\n",
        "    # Model blessing provided by the Slack component\n",
        "    model_blessing=slack_validator.outputs[\"slack_blessing\"],\n",
        "    push_destination=pusher_pb2.PushDestination(\n",
        "        filesystem=pusher_pb2.PushDestination.Filesystem(\n",
        "            base_directory=serving_model_dir\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWN4js9ns-Ge"
      },
      "source": [
        "**Slack API Standards**\n",
        "The Implementation of the Slack component relies on the Real Time Messaging (RTM) protocol. This protocol is deprecated and might be replaced by a new protocol standard, which would affect the component's functionality."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J52AX_Q6tUzg"
      },
      "source": [
        "## Custom TFX Components\n",
        "\n",
        "Remember from Chapter 2, that each component consits of three parts:\n",
        " - driver\n",
        " - executor\n",
        " - publisher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovZd3m-lvMnV"
      },
      "source": [
        "### Use Cases of Custom Components\n",
        "\n",
        "Custom components can be applied anywhere along the machine learning pipeline. The concepts discussed in the following provide the highest flexibility to customize your machine learning pipelines to your needs. Some ideas might be:\n",
        " - Ingesting data from your custom database\n",
        " - Sending an email with the generated data statistics to the data science team\n",
        " - Notifying the DevOps team if a new model was exported\n",
        " - Kicking off a post-export build process for Docker containers\n",
        " - Tracking additional information in your machine learning audit trail"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9OXy1XNv9OG"
      },
      "source": [
        "### Writing a Custom Component from Scratch\n",
        "\n",
        "We will need to implement a few component pieces:\n",
        " - 1. Define the inputs and outputs of our component as a ComponentSpec\n",
        " - 2. Define our component executor, which defines how the input data should be processed and how the output data is generated\n",
        "  - 2.1 If the component requires inputs that are not registered in the metadata store, we will need to write a custom component driver, e.g. this happens when we want to register an image path in the component and the artifact has not been registered in the metadata store previously.\n",
        "\n",
        "**Try to Reuse Components**\n",
        "Try to reuse already existing TFX components and change the executor instead, this is explained further down below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yksY98YQxaLA"
      },
      "source": [
        "#### Component Specifications\n",
        "\n",
        "The component specifications or ComponentSpec, define how components communicate with each other. They describe the component inputs, outputs and potential component parameters that are required during the component execution.\n",
        "The following example shows a definition of our component specifications for our image ingestion component:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePWCh22DjpTn"
      },
      "source": [
        "from tfx.types.component_spec import ChannelParameter, ExecutionParameter\n",
        "from tfx.types import standard_artifacts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1qnaOYyjpM9"
      },
      "source": [
        "class ImageIngestComponentSpec(types.ComponentSpec):\n",
        "    \"\"\"\n",
        "    ComponentSpec for a Custom TFX Image Ingestion Component.\n",
        "    \"\"\"\n",
        "    PARAMETERS = {\n",
        "        \"name\": ExecutionParameter(type=Text),\n",
        "    }\n",
        "    INPUTS = {\n",
        "        # Using ExternalArtifact to allow new input paths\n",
        "        \"input\": ChannelParameter(type=standard_artifacts.ExternalArtifact)\n",
        "    }\n",
        "    OUTPUTS = {\n",
        "        # Exporting Examples\n",
        "        \"examples\": ChannelParameter(type=standard_artifacts.Examples)\n",
        "    }"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1wviKUAztKk"
      },
      "source": [
        "#### Component Channels\n",
        "\n",
        "The standard types are usually the ones used above: *ExternalArtifact* and *Examples*. Here is a small list of availabe types:\n",
        " - ExampleStatistics\n",
        " - Model\n",
        " - ModelBlessing\n",
        " - Bytes\n",
        " - String\n",
        " - Integer\n",
        " - Float\n",
        " - Hyperparameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdnvFpup0RBX"
      },
      "source": [
        "#### Component Executors\n",
        "\n",
        "This defines the processes inside the component, including how inputs are used to generate the component outputs. Here we will write it from Scratch, but we can rely on TFX classes to inherit function patterns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir5CBt5X0nvM"
      },
      "source": [
        "from tfx.components.base import base_executor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zb5-xd_w1Fhj"
      },
      "source": [
        "**Artifacts contain References**\n",
        "The information provided via the input_dict and output_dict contain the information stored in the metadata store. These are the references to the artifacts, not the underlying data itself, e.g. the input_dict will contain a protocol buffer with the file location information instead of the data, this allows us to process the data efficiently with programs like Apache Beam."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWfNTFYl0ogF"
      },
      "source": [
        "class Executor(base_executor.BaseExecutor):\n",
        "    \"\"\"\n",
        "    Executor for Image Ingestion Component.\n",
        "    \"\"\"\n",
        "    def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n",
        "           output_dict: Dict[Text, List[types.Artifact]],\n",
        "           exe_properties: Dict[Text, Any]) -> None:\n",
        "        ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jZGnBAKj1vwD"
      },
      "source": [
        "# Reuse the implentation that we discussed in Image Data for Computer Vision Problems\n",
        "# to convert images to TFRecord data structures.\n",
        "def convert_image_to_TFExample(image_filename, tf_writer, input_base_uri):\n",
        "    # Assemble the complete image path\n",
        "    image_path = os.path.join(input_base_uri, image_filename)\n",
        "\n",
        "    # Determine the label for each image base on the file path\n",
        "    lowered filename = image_path.lower()\n",
        "    if \"dog\" in lowered_filename:\n",
        "        label = 0\n",
        "    elif \"cat\" in lowered_filename:\n",
        "        label = 1\n",
        "    else:\n",
        "        raise NotImplementedError(\"Found unknown image\")\n",
        "\n",
        "    # Read the image\n",
        "    raw_file = tf.io.read_file(image_path)\n",
        "\n",
        "    # Create the TensorFlow Example data structures\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            features={\n",
        "                \"image_raw\": _bytes_features(raw_file.numpy()),\n",
        "                \"label\": _int64_feature(label)\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Write the tf.Example to TFRecord files\n",
        "    writer.write(example.SerializeToString())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF2SnFqx28m7"
      },
      "source": [
        "We now want our very basic component to load our images, convert them to tf.Examples and return two image sets for training and evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEcNoEwi2xLS"
      },
      "source": [
        "class ImageIngestionExecutor(base_executor.BaseExecutor):\n",
        "    def Do(self, input_dict: Dict[Text, List[types.Artifact]],\n",
        "           output_dict: Dict[Text, List[types.Artifact]],\n",
        "           exe_properties: Dict[Text, Any]) -> None:\n",
        "        # Log arguments\n",
        "        self._log_startup(input_dict, output_dict, exec_properties)\n",
        "\n",
        "        # Get the folder path from the artifact\n",
        "        input_base_uri = artifact_utils.get_single_uri(input_dict[\"input\"])\n",
        "\n",
        "        # Obtain all the filenames\n",
        "        image_files = tf.io.gfile.listdir(input_base_uri)\n",
        "        random.shuffle(image_files)\n",
        "        splits = get_splits(images)\n",
        "\n",
        "        for split_name, images in splits:\n",
        "            # Set the split Uniform Resource Identifier (URI)\n",
        "            output_dir = artifact_utils.get_split_uri(\n",
        "                output_dict[\"examples\"],\n",
        "                split_name\n",
        "            )\n",
        "\n",
        "            tfrecord_filename = os.path.join(output_dir, \"images.tfrecord\")\n",
        "            options = tf.io.TFRecordOptions(compression_type=None)\n",
        "\n",
        "    # Create a TFRecord writer instace with options\n",
        "    writer = tf.io.TFRecordWriter(tfrecord_filename, options=options)\n",
        "    for images in images:\n",
        "        # Write an image to a file containing the TFRecord data structures\n",
        "        convert_image_to_TFExample(image, tf_writer, input_base_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0prikP84k0Y"
      },
      "source": [
        "For more details and information about dynamically setting data splits, see page 303."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qYTm8zF4uZ1"
      },
      "source": [
        "#### Component Drivers\n",
        "\n",
        "In our case we want to ingest data from a disk and we are reading the data for the first time in our pipeline, therefore the data isn't passed down from a different component and we need to register the data sources in the metadata store. If this would not be done, that the component with the executor we have defined so far would encounter a TFX error.\n",
        "\n",
        "**Custom Drivers Are Rare**\n",
        "If you can reuse the input/output architecture of an existing TFX component or if the inputs are already registered with the metadata store, you will not need to write a custom driver and you can skip this step.\n",
        "\n",
        "See the explicit code on pages 304f."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SuZFKLhu52oj"
      },
      "source": [
        "class ImageIngestDriver(base_driver.BaseDriver):\n",
        "    \"\"\"\n",
        "    Custom driver for ImageIngest.\n",
        "    \"\"\"\n",
        "    def resolve_input_artifacts(self, ...):\n",
        "        ..."
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnOPIr9B5wD2"
      },
      "source": [
        "#### Assembling the custom component\n",
        "\n",
        "To define the actual component, we need to define the specification, executor and driver classes. We can do this by setting SPEC_CLASS, EXECUTOR_SPEC and DRIVER_CLASS. As the final step, we need to instantiate our ComponentSecs with the component's arguemtns (e.g. input and output exaples and the provided name) and pass it to the instantiated ImageIngestComponent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TKy7LpIB0odO"
      },
      "source": [
        "from tfx.components.base import base_component\n",
        "from tfx import types\n",
        "from tfx.types import channel_utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3x_K3C-0oaX"
      },
      "source": [
        "class ImageIngestComponent(base_component.BaseComponent):\n",
        "    \"\"\"\n",
        "    Custom ImageIngestWorld Component\n",
        "    \"\"\"\n",
        "    SPEC_CLASS = ImageIngestComponentSpec\n",
        "    EXECUTOR_SPEC = executor_spec.ExecutorClassSpec(ImageIngestExecutor)\n",
        "    DRIVER_CLASS = ImageIngestDriver\n",
        "\n",
        "    def __init__(self, input, output_data=None, name=None):\n",
        "        if not output_data:\n",
        "            examples_artifact = standard_artifacts.Examples()\n",
        "            examples_artifact.split_names = artifact_utils.encode_split_names([\"train\", \"eval\"])\n",
        "            output_data = channel_utils.as_channel([examples_artifcat])\n",
        "\n",
        "        spec = ImageIngestComponentSpec(\n",
        "            input=input,\n",
        "            examples=output_data,\n",
        "            name=name\n",
        "        )\n",
        "        super(ImageIngestComponent, self).__init__(spec=spec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YmLTzWA7veC"
      },
      "source": [
        "#### Using our basic custom component"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhEenBgo0oXH"
      },
      "source": [
        "import os\n",
        "\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
        "from image_ingestion_component.component import ImageIngestComponent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZ8cL9Ds0oUS"
      },
      "source": [
        "context = InteractiveContext()\n",
        "\n",
        "image_file_path = \"/path/to/files\"\n",
        "examples = external_input(dataimage_file_path_root)\n",
        "example_gen = ImageIngestComponent(input=\"examples\", name=u\"ImageIngestComponent\")\n",
        "\n",
        "context.run(example_gen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8LEioMb0nr0"
      },
      "source": [
        "# The above components output can than be consumend by downstream components like StatisticsGen\n",
        "from tfx.components import StatisticsGen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN1H6mc88fPo"
      },
      "source": [
        "statistics_gen = StatisticsGen(\n",
        "    examples=example_gen:outputs[\"examples\"]\n",
        ")\n",
        "\n",
        "context.run(statistics_gen)\n",
        "context.show(statistics_gen.outputs[\"statistics\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQW5CGbJ8sPv"
      },
      "source": [
        "**Very Basic Implementation**\n",
        "Caution: The discussed implementation only provides basic functionality and is not production ready. For details see the following section. For a product-ready implementation, see the updated component implementation in the next sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqQgrfuk9Wuw"
      },
      "source": [
        "#### Implementation Review\n",
        "\n",
        "While the component is functioning, it is missing some key functionality that we discussed in Chapter 3, e.g. dynamic split names or split ratios). Furter it required a lot of boiler-plate code, e.g. setting up the component driver. The ingestion should be handled in a more efficient way. This can be achieved by using Apache Beam under the hood of TFX components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpz8yigK-49u"
      },
      "source": [
        "### Reusing Existing Components\n",
        "\n",
        "Instead of writing everything from scratch, we can inherit an existing component and customize it by overwriting the executor functionality.\n",
        "Similarly to the Avro or Parquet examples from Chapter 3, we can simply focus on developing our custom executor and making it more flexible as our previous basic component. By reusing existing code infrastructure, we can also piggyback on existing Apache Beam implementations.<br>\n",
        "TFX and Apache Beam provide classes and function decorators to ingest the data via Apache Beam pipelines. We will use the function decorator *@beam.ptransform_fn* which allows us to define Apache Beam transformation (PTransform). The following code example is an updated version of the previous conversion function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYXWecdQ8fYt"
      },
      "source": [
        "# Only the file path is needed\n",
        "def convert_image_to_TFExample(image_path):\n",
        "    # Determine the label for each image base on the file path\n",
        "    lowered filename = image_path.lower()\n",
        "    if \"dog\" in lowered_filename:\n",
        "        label = 0\n",
        "    elif \"cat\" in lowered_filename:\n",
        "        label = 1\n",
        "    else:\n",
        "        raise NotImplementedError(\"Found unknown image\")\n",
        "\n",
        "    # Read the image\n",
        "    raw_file = tf.io.read_file(image_path)\n",
        "\n",
        "    # Create the TensorFlow Example data structures\n",
        "    example = tf.train.Example(\n",
        "        features=tf.train.Features(\n",
        "            features={\n",
        "                \"image_raw\": _bytes_features(raw_file.numpy()),\n",
        "                \"label\": _int64_feature(label)\n",
        "            }\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # The function returns examples instead of writing them to a disk.\n",
        "    return example"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpeL3d-qBSZe"
      },
      "source": [
        "With the updated conversion function in place, we can now focus on implementing the core executor functionality."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNPuo3me8fVZ"
      },
      "source": [
        "@beam.ptransform_fn\n",
        "def image_to_example(\n",
        "    pipeline: beam.Pipeline,\n",
        "    input_dict: Dict[Text, List[types.Artifact]],\n",
        "    exec_properties: Dict[Text, Any],\n",
        "    split_pattern: Text) -> beam.pvalue.PCollection:\n",
        "    input_base_uri = artifact_utils.get_single_uri(input_dict[\"input\"])\n",
        "    image_pattern = os.path.join(input_base_uri, split_pattern)\n",
        "    absl.logging.info(\n",
        "        \"Processing input image data {} to tf.Example.\".format(image_pattern)\n",
        "    )\n",
        "\n",
        "    # Convert the list of files present in the ingestion paths\n",
        "    image_files = tf.io.gfile.glob(image_pattern)\n",
        "    if not image_files:\n",
        "        raise RuntimeError(\n",
        "            \"Split pattern {} did not match any valid path.\".format(image_pattern)\n",
        "        )\n",
        "\n",
        "    p_collection = (\n",
        "        pipeline\n",
        "        # Convert the list to a Beam PCollection\n",
        "        | beam.Create(image_files)\n",
        "        # Apply the conversion to every image\n",
        "        | \"ConvertImageToTFRecords\" >> beam.Map(lambda image: convert_image_to_TFExample(image))\n",
        "    )\n",
        "\n",
        "    return p_collection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msWi8RXmC-Id"
      },
      "source": [
        "Now as the final step, overwrite the GetInputSourceToExamplePTransform of the BaseExampleGenExecutor with our image_to_example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GZT2A8NC-3w"
      },
      "source": [
        "class ImageExampleGenExecutor(BaseExampleGenExecutor):\n",
        "\n",
        "    @beam.ptransform_fn\n",
        "    def image_to_example(...):\n",
        "        ...\n",
        "\n",
        "    def GetInputSourceToExamplePTransform(self) -> beam.PTransform:\n",
        "        return image_to_example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4UqqaigbDZv1"
      },
      "source": [
        "#### Using our custom executor\n",
        "\n",
        "We can now follow the same patterns we discusseed for the Avro ingestion in Chapter 3 and specify the custom_executor_spec. By doing this, we can use the entire functionality of ingestion components.<br>\n",
        "Complete example of using our custom component:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKV3oQFyD4vq"
      },
      "source": [
        "from tfx.components import FileBaseExampleGen\n",
        "from tfx.utils.dsl_utils import external_input\n",
        "\n",
        "from image_ingestion_component.executor import ImageExapleGenExecutor\n",
        "\n",
        "input_config = example_gen_pb2.Input(\n",
        "    splits=[\n",
        "            example_gen_pb2.Input.Split(\n",
        "                name=\"images\",\n",
        "                pattern=\"sub-directory/if/needed/*.jpg\"\n",
        "            )\n",
        "    ]\n",
        ")\n",
        "\n",
        "output = example_gen_pb2.Output(\n",
        "    split_config=example_gen_pb2.SplitConfig(\n",
        "        splits=[\n",
        "                example_gen_pb2.SplitConfig.Split(\n",
        "                    name=\"train\",\n",
        "                    hash_buckets=4\n",
        "                ),\n",
        "                example_gen_pb2.SplitConfig.Split(\n",
        "                    name=\"eval\",\n",
        "                    hash_buckets=1\n",
        "                )\n",
        "        ]\n",
        "    )\n",
        ")\n",
        "\n",
        "example_gen = FileBasedExampleGen(\n",
        "    input=external_input(\"/path/to/images/\"),\n",
        "    input_config=input_config,\n",
        "    output_config=output,\n",
        "    custom_executor_spec=executor_spech.ExecutorClassSpec(ImageExampleGenExecutor)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYIUVfzSdcx2"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        "- <a href=\"https://www.tensorflow.org/lite\">TFLite Website</a>\n",
        "- <a href=\"https://developers.google.com/ml-kit\">Google's ML Kit</a>\n",
        "- <a href=\"https://api.slack.com/\">Slack API</a>\n",
        "- <a href=\"https://www.tensorflow.org/tfx/guide\">TensorFlow Extended Guide</a>"
      ]
    }
  ]
}