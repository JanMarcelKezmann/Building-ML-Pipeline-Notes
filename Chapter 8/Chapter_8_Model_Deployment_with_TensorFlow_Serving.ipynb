{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chapter 8: Model Deployment with TensorFlow Serving.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCGDRQuVW30g"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Glvw5xmoeAx_",
        "outputId": "10ebb708-5b9d-4954-ed79-8823d572f1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "!apt-get install tree"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  tree\n",
            "0 upgraded, 1 newly installed, 0 to remove and 6 not upgraded.\n",
            "Need to get 40.7 kB of archives.\n",
            "After this operation, 105 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 tree amd64 1.7.0-5 [40.7 kB]\n",
            "Fetched 40.7 kB in 1s (38.5 kB/s)\n",
            "Selecting previously unselected package tree.\n",
            "(Reading database ... 144617 files and directories currently installed.)\n",
            "Preparing to unpack .../tree_1.7.0-5_amd64.deb ...\n",
            "Unpacking tree (1.7.0-5) ...\n",
            "Setting up tree (1.7.0-5) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1dKb_MFXH_q"
      },
      "source": [
        "# Chapter 8: Model Deployment with TensorFlow Serving\n",
        "\n",
        "This Chapter bridges the gap between DevOps engineers and Data Scientists.\n",
        "\n",
        "Machine Learning Models can be deployed in three main ways:\n",
        " - Model Server\n",
        " - User's Browser\n",
        " - Edge Device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eIx94Hb5mP7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "base_dir = \"drive/My Drive/Building ML Pipelines/\"\n",
        "chap_dir = base_dir + \"Chapter 8/\"\n",
        "data_dir = base_dir + \"Data/\"\n",
        "out_dir = chap_dir + \"Outputs/\"\n",
        "csv_data_dir = base_dir + \"CSV Data/\"\n",
        "csv_dir = csv_data_dir + \"consumer_complaints_with_narrative.csv\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7R5oTgsWdYAa"
      },
      "source": [
        "## A Simple Model Server\n",
        "\n",
        "Most introductions to deploying machine learning models follow roughly the same workflow:\n",
        " - Create a web app with Python (with Flask or Django)\n",
        " - Create an API endpoint in the web app\n",
        " - Load the model structure and its weights\n",
        " - Call the predict method on the loaded model\n",
        " - Return the prediction results as an HTTP request"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXvxbSneXHPM"
      },
      "source": [
        "import json\n",
        "from flask import Flask, request\n",
        "from tensorflow.keras.models import load_model\n",
        "# from utils import preprocess"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzZrpQttgujV"
      },
      "source": [
        "Only for demonstration purposes, it is not recommended to use such a code in production."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPzAh3eyd3UD"
      },
      "source": [
        "# Load your trained model\n",
        "model = load_model(\"model.h5\")\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/classify\", methods=[\"POST\"])\n",
        "def classify():\n",
        "    complaint_data = request.form[\"complaint_data\"]\n",
        "    preprocessed_complaint_data = preprocess(complaint_data)\n",
        "    # Perform the prediction\n",
        "    prediction = model.predict([preprocessed_complaint_data])\n",
        "\n",
        "    # Return the prediction in an HTTP response\n",
        "    return json.dumpy({\"score\": prediction})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqZfwxP1fipb"
      },
      "source": [
        "# The Downside of Model Deployments with Python-Based APIs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY7dlIqkfquE"
      },
      "source": [
        "### Lack of Code Separation\n",
        "\n",
        "Lack of code separation can lead to many problems between the Data Scientists creating the model and the API team deploying it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpXnjj8CgLH_"
      },
      "source": [
        "### Lack of Model Version Control\n",
        "\n",
        "Creating Model Versions requires extra attention to keep all endpoints structurally the same, which itself requires a lot of boiler plate code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbEqups9geTW"
      },
      "source": [
        "### Inefficient Model Inference\n",
        "\n",
        "This happens because, e.g. in Flask each request is preprocessed and inferred individually. Like in training you can run inference in batches, which will make it alot more efficient, especially when run on GPUs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XTFIMp9hGD5"
      },
      "source": [
        "## TensorFlow Serving\n",
        "\n",
        "TensorFlow Serving allows to deploy any TensorFlow graph and you can make predictions from the graph through its standardized endpoints.\n",
        "TensorFlow Serving will further handle the model and version management for you."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NyB15xbHCnN"
      },
      "source": [
        "## TensorFlow Architecture Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1NyqBKThfe3"
      },
      "source": [
        "## Exporting Models for TensorFlow Serving\n",
        "\n",
        "Depending on the type of TensorFlow model, the export steps are slightly different.<br>\n",
        "For Keras models you can use:\n",
        "\n",
        "    saved_model_path = model.save(file_path=\"./saved_models\", save_format=\"tf\")\n",
        "\n",
        "\n",
        "**Add a Timestamp to Your Export Path:**<br>\n",
        "If model is saved manually it is recommend to add the timestamp to the model path.\n",
        "\n",
        "    import time\n",
        "\n",
        "    ts = int(time.time())\n",
        "    file_path = \"./saved_models/{}\".format(ts)\n",
        "    save_model_path = model.save(file_path=file_path, save_format=\"tf\")\n",
        "\n",
        "\n",
        "For TensorFlow Estimator models, you need to first declare a receiver function:\n",
        "\n",
        "    import tensorflow as tf\n",
        "\n",
        "    def serving_input_receiver_fn():\n",
        "        # an example input feature\n",
        "        input_feature = tf.compat.v1.placeholder(\n",
        "            dtype=tf.string,\n",
        "            shape=[None, 1],\n",
        "            name=\"input\"\n",
        "            )\n",
        "\n",
        "        fn = tf.estimator.export.build_raw_serving_input_receiver_fn(\n",
        "            features={\"input_feature\": input_feature}\n",
        "        )\n",
        "\n",
        "        return fn\n",
        "\n",
        "Export the Estimator model with the *export_saved_model* method for the Estimator:\n",
        "\n",
        "    estimator = tf.estimator.Estimator(model_fn, \"model\", params={})\n",
        "    estimator.export_saved_model(\n",
        "        export_dir_base=\"saved_models/\",\n",
        "        serving_input_receiver_fn=sering_input_receiver_fn\n",
        "        )\n",
        "        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w5mCot4IpsD"
      },
      "source": [
        "## Model Signatures\n",
        "\n",
        "Model signatures identify the model graph's inut and outupts as well as the method of the graph signature. This allows us to map serving inputs to a given graph node for the inference, which is useful if we want to update the model without changing the requests to the model server."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfVjoM5e2GHA"
      },
      "source": [
        "### Signature Methods\n",
        "\n",
        "There are three supported signature methods:\n",
        " - predict\n",
        " - classify\n",
        " - regress\n",
        "\n",
        "Predict is the default method.<br>\n",
        "Predict:\n",
        " - Inputs: Keys\n",
        " - Outputs: Scores (Values)\n",
        "\n",
        "Classify:\n",
        " - Inputs: Keys\n",
        " - Outputs: Classes and Score\n",
        "\n",
        "Regress:\n",
        " - Inputs: Keys\n",
        " - Value: Scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD4Hse5Z3Naw"
      },
      "source": [
        "## Inspecting Exported Models\n",
        "\n",
        "You can install the TensorFlow Serving API by running the following command:\n",
        "\n",
        "    pip install tensorflow-serving-api\n",
        "\n",
        "Now you have a useful command-line tool called SavedModel Command Line Interface (CLI), which lets you:\n",
        " - Inspect the signatures of exported models\n",
        " - Test the exported models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "INy5gnCA3wz9"
      },
      "source": [
        "### Inspecting the Model\n",
        "\n",
        "*saved_model_cli* helps you understand the model dependencies without inspecting the original graph code:\n",
        "\n",
        "    saved_model_cli show --dir saved_models/\n",
        "\n",
        "*Output*<br>\n",
        "  The given SavedModel contains the following tag-sets:<br>\n",
        "  - *serve*\n",
        "\n",
        "Once you know the tag_set you want to inspect, add it as an argument and saved_model_cli will provide you the available model signatures. Run:\n",
        "\n",
        "    saved_model_cli show --dir saved_models/ --tag_set serve\n",
        "\n",
        "This returns the SignatureDefs with its keys.<br>\n",
        "With the tag_set and signature_def information you can now inspect the model's inputs and outputs. Therfore add the signature_def to the CLI arguments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xL_IrHJLBjju"
      },
      "source": [
        "# !pip install tensorflow_serving_api"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Z53TmP5IqY",
        "outputId": "296b6f70-9f23-48ef-f455-ae600e60731d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "saved_models_dir = \"drive/My Drive/Building ML Pipelines/Chapter 7/Outputs/Trainer/model/18/serving_model_dir\"\n",
        "print(saved_models_dir)\n",
        "\n",
        "# converting string to raw string \n",
        "def to_raw(string):\n",
        "    r_s = \"\"\n",
        "    for s in string:\n",
        "        if s == \" \":\n",
        "            r_s += \"\\\\\"\n",
        "        r_s += s\n",
        "        \n",
        "    return r_s \n",
        "raw_saved_models_dir = to_raw(saved_models_dir)\n",
        "print(raw_saved_models_dir)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Building ML Pipelines/Chapter 7/Outputs/Trainer/model/18/serving_model_dir\n",
            "drive/My\\ Drive/Building\\ ML\\ Pipelines/Chapter\\ 7/Outputs/Trainer/model/18/serving_model_dir\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EbYLz-aiCTHB",
        "outputId": "09464c6d-d9ae-479c-d056-ef2f7da80d9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!saved_model_cli show --dir {raw_saved_models_dir}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel contains the following tag-sets:\n",
            "serve\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clUtVTUQIiHe",
        "outputId": "af4d5441-c977-41cc-8a77-06a3fa723f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "!saved_model_cli show --dir {raw_saved_models_dir} --tag_set serve"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel MetaGraphDef contains SignatureDefs with the following keys:\n",
            "SignatureDef key: \"__saved_model_init_op\"\n",
            "SignatureDef key: \"serving_default\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yNLHR4RDXI7"
      },
      "source": [
        "The following example signature is taken from the model defined an trained in Chapter 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxtdptSjBnOp",
        "outputId": "dac1e32b-74a1-4b9c-8e71-572bda666491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "!saved_model_cli show --dir {raw_saved_models_dir} --tag_set serve --signature_def serving_default"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The given SavedModel SignatureDef contains the following input(s):\n",
            "  inputs['examples'] tensor_info:\n",
            "      dtype: DT_STRING\n",
            "      shape: (-1)\n",
            "      name: serving_default_examples:0\n",
            "The given SavedModel SignatureDef contains the following output(s):\n",
            "  outputs['outputs'] tensor_info:\n",
            "      dtype: DT_FLOAT\n",
            "      shape: (-1, 1)\n",
            "      name: StatefulPartitionedCall_2:0\n",
            "Method name is: tensorflow/serving/predict\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i-vyD0HDhWc"
      },
      "source": [
        "If you want to see all signatures regardless of the tag_set and signature_def run the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3X_EeZf9Bnqc",
        "outputId": "3ce09d07-19e7-454b-cdea-566f487d9106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!saved_model_cli show --dir {raw_saved_models_dir} --all"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
            "\n",
            "signature_def['__saved_model_init_op']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['__saved_model_init_op'] tensor_info:\n",
            "        dtype: DT_INVALID\n",
            "        shape: unknown_rank\n",
            "        name: NoOp\n",
            "  Method name is: \n",
            "\n",
            "signature_def['serving_default']:\n",
            "  The given SavedModel SignatureDef contains the following input(s):\n",
            "    inputs['examples'] tensor_info:\n",
            "        dtype: DT_STRING\n",
            "        shape: (-1)\n",
            "        name: serving_default_examples:0\n",
            "  The given SavedModel SignatureDef contains the following output(s):\n",
            "    outputs['outputs'] tensor_info:\n",
            "        dtype: DT_FLOAT\n",
            "        shape: (-1, 1)\n",
            "        name: StatefulPartitionedCall_2:0\n",
            "  Method name is: tensorflow/serving/predict\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W1007 12:35:10.979345 139644275099520 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling __init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "2020-10-07 12:35:14.076932: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-10-07 12:35:14.130143: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "2020-10-07 12:35:14.130209: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (122c1c697a3c): /proc/driver/nvidia/version does not exist\n",
            "2020-10-07 12:35:14.157582: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
            "2020-10-07 12:35:14.157912: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x564a602be840 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-10-07 12:35:14.157955: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "\n",
            "Defined Functions:\n",
            "  Function Name: '__call__'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'inputs/0'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'inputs/1'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'inputs/2'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'inputs/3'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'inputs/4'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'inputs/5'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'inputs/6'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'product_xf'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'sub_product_xf'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'company_response_xf'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'state_xf'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'issue_xf'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'zip_code_xf'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'consumer_complaint_narrative_xf'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'inputs/0'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'inputs/1'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'inputs/2'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'inputs/3'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'inputs/4'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'inputs/5'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'inputs/6'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'product_xf'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'sub_product_xf'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'company_response_xf'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'state_xf'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'issue_xf'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'zip_code_xf'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'consumer_complaint_narrative_xf'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "\n",
            "  Function Name: '_default_save_signature'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'product_xf'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'sub_product_xf'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'company_response_xf'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'state_xf'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'issue_xf'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'zip_code_xf'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'consumer_complaint_narrative_xf'), \b\b]\n",
            "\n",
            "  Function Name: 'call_and_return_all_conditional_losses'\n",
            "    Option #1\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'product_xf'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'sub_product_xf'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'company_response_xf'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'state_xf'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'issue_xf'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'zip_code_xf'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'consumer_complaint_narrative_xf'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #2\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'product_xf'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'sub_product_xf'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'company_response_xf'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'state_xf'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'issue_xf'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'zip_code_xf'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'consumer_complaint_narrative_xf'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #3\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'inputs/0'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'inputs/1'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'inputs/2'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'inputs/3'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'inputs/4'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'inputs/5'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'inputs/6'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: False\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n",
            "    Option #4\n",
            "      Callable with:\n",
            "        Argument #1\n",
            "          DType: list\n",
            "          Value: [TensorSpec(shape=(None, 12), dtype=tf.float32, name=u'inputs/0'), TensorSpec(shape=(None, 46), dtype=tf.float32, name=u'inputs/1'), TensorSpec(shape=(None, 6), dtype=tf.float32, name=u'inputs/2'), TensorSpec(shape=(None, 61), dtype=tf.float32, name=u'inputs/3'), TensorSpec(shape=(None, 91), dtype=tf.float32, name=u'inputs/4'), TensorSpec(shape=(None, 11), dtype=tf.float32, name=u'inputs/5'), TensorSpec(shape=(None, 1), dtype=tf.string, name=u'inputs/6'), \b\b]\n",
            "        Argument #2\n",
            "          DType: bool\n",
            "          Value: True\n",
            "        Argument #3\n",
            "          DType: NoneType\n",
            "          Value: None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mxqxoFdD423"
      },
      "source": [
        "### Testing the Model\n",
        "\n",
        "*saved_model_cli* also lets you test the export model with sample input data.<br>\n",
        "You have three different ways to submit the sample input data for the model test inference:\n",
        " - --inputs (for arguments pointing at NumPy files)\n",
        " - --input_exprs (for arguments pointing at Python expressions to specify the input data)\n",
        " - --input_examples (for arguments pointing to data formatted as tf.Example data structure)\n",
        "\n",
        "For testing the model, you can specify exactly one of the input arguments.<br>\n",
        "Furhtermore, *saved_model_cli* provides three optional arguments:\n",
        " - --outdir (target_directory, else file is written to stdout)\n",
        " - --overwrite (if output is written to a file you can specify if it can be overwritten)\n",
        " - --tf_debug (for further model inspection)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jZXe37PBnuO"
      },
      "source": [
        "!saved_model_cli run --dir {raw_saved_models_dir} --tag_set serve --signature_def serving_default --input_example \"examples=[{'company': ['HSBC']}]\""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4U3Jv-hFpHr"
      },
      "source": [
        "## Setting Up TensorFlow Serving\n",
        "\n",
        "TensorFlow Serving can either run on Docker or if you run an Ubuntu OS you can install the Ubuntu package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_wZmt-NF-Um"
      },
      "source": [
        "### Docker Installation\n",
        "\n",
        "Install TensorFlow Serving by downloading the prebuilt Docker image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rrI3UqXBoDt"
      },
      "source": [
        "!docker pull tensorflow/serving"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCtMNxfHm9i"
      },
      "source": [
        "If you run the Docker container on an instance with GPUs available, you will need to download the latest build with GPU support."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjZ8KQCqBn9Q"
      },
      "source": [
        "!docker pull tensorflow/serving:latest-gpu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpBVbjkBIK3F"
      },
      "source": [
        "For the Native Ubuntu Installation see page 214 of the Book."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abDpPN0BIVfA"
      },
      "source": [
        "### Building TensorFlow Serving from Source\n",
        "\n",
        "Currently this option is only build for Linux Operating systems and the build tool *bazel* is required.<br>\n",
        "You can find detailed instructions in the <a href=\"https://www.tensorflow.org/tfx/serving/setup#building_from_source\">TensorFlow Serving Documentation</a>.<br>\n",
        "\n",
        "**Optimize Your TensorFlow Serving Instances**\n",
        "If you build TensorFlow Serving from scratch, it is highly recommend compiling the Serving versino for the specific TensorFlow versino of your models and available hardware of your serving instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CD-QL0IrJKnu"
      },
      "source": [
        "## Configuring a TensorFlow Server\n",
        "\n",
        "There are two different modes a TensorFlow Serving model can run:\n",
        " - Single Model Configuration\n",
        " - Multiple Model Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8TnVyK9JgvR"
      },
      "source": [
        "### Single Model Configuration\n",
        "\n",
        "Preferred when you want to run TF Serving by loading a single model and switching to newer model versions when they are available.\n",
        "If run in an Docker environment, then execute:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_h86Pt_DBn6l"
      },
      "source": [
        "!docker run -p 8500:8500 \\ # specify the default ports\n",
        "            -p 8501:8501 \\\n",
        "            --mount type=bind,source=/tmp/models,target=/models/my_model \\ # mount the model directory\n",
        "            -e MODEL_NAME=my_model \\ # specify your model\n",
        "            -e MODEL_BASE_PATH=/models/my_model \\\n",
        "            -t tensorflow/serving # specify the docker image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvFq-wqGKceI"
      },
      "source": [
        "for more information see p.216 ff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP7XXV1cKhUP"
      },
      "source": [
        "If you want to run the Docker image prebuilt for GPU images, you need to swap out the name of the docker image to the latest GPU build with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZvYg9tfBn5W"
      },
      "source": [
        "!docker run -p 8500:8500 \\ # specify the default ports\n",
        "            -p 8501:8501 \\\n",
        "            --mount type=bind,source=/tmp/models,target=/models/my_model \\ # mount the model directory\n",
        "            -e MODEL_NAME=my_model \\ # specify your model\n",
        "            -e MODEL_BASE_PATH=/models/my_model \\\n",
        "            -t tensorflow/serving:latest-gpu # specify the docker image for GPU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqA9CATUK09D"
      },
      "source": [
        "In the output you should see that the server loaded our model *my_model* successfully and that created two endpoints:\n",
        " - 1 REST endpoint and \n",
        " - 1 gRPC endpoint\n",
        "\n",
        "One great advantage of TensorFlow Serving is the *hot swap*, which automatically unloads the existing model and load the newer model for inferencing if a new model is uploaded. In general the model with the highest version number will be loaded by TensorFlow Serving. The same procedure works for rolling back a model version, which works by deleting the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNlzx2_TNHzG"
      },
      "source": [
        "### Multiple Model Configuration\n",
        "\n",
        "You can also configure TensorFlow Serving to load multiple models at the same time by running the following command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OKbDU9DBn3y"
      },
      "source": [
        "model_config_list {\n",
        "    config {\n",
        "        name: \"my_model\"\n",
        "        base_path: \"models/my_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "    }\n",
        "    config {\n",
        "        name: \"another_model\"\n",
        "        base_path \"models/another_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QmI09W90N-_h"
      },
      "source": [
        "The run Docker as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2l9EgGzBn2l"
      },
      "source": [
        "!docker run -p 8500:8500 \\ # specify the default ports\n",
        "            -p 8501:8501 \\\n",
        "            --mount type=bind,source=/tmp/models,target=/models/my_model \\ # mount the model directory\n",
        "            --mount type=bind,source=/tmp/modles_config,target=/models/model_config \\ # mmount the configuration file\n",
        "            -e MODEL_NAME=my_model \\ # specify your model\n",
        "            -t tensorflow/serving:latest-gpu \\ # specify the docker image for GPU\n",
        "            --model_config_file=/models/model_config # specify the model configuration file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2P8lxFcOehF"
      },
      "source": [
        "If TensorFlow Serving is used outside of a Docker container, you can point the model server to the configuration file with the argument *model_config_file*, which loads the configuration from the file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Cz0f_PeBny3"
      },
      "source": [
        "!tensorflow_model_server --port=8500 \\\n",
        "                         --rest_api_port=8501 \\\n",
        "                         --model_config_file=/models/model_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bU3xdtrqPLj9"
      },
      "source": [
        "**Running specific model versions:**\n",
        "\n",
        "If you want to load a set of available model version, e.g. for A/B Testing, you can extend the model configuration file with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3RbsqH4PK9s"
      },
      "source": [
        "model_config_list {\n",
        "    config {\n",
        "        name: \"my_model\"\n",
        "        base_path: \"models/my_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "        model_version_policy: {all: {}}\n",
        "    }\n",
        "    config {\n",
        "        name: \"another_model\"\n",
        "        base_path \"models/another_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_tHSvJKPeJz"
      },
      "source": [
        "If you want to specify specific model version, you can define them as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47w4JytbPLGU"
      },
      "source": [
        "model_config_list {\n",
        "    config {\n",
        "        name: \"my_model\"\n",
        "        base_path: \"models/my_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "        model_version_policy {\n",
        "            specific {\n",
        "                versions: 1556250435\n",
        "                versions: 1556251435\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    config {\n",
        "        name: \"another_model\"\n",
        "        base_path \"models/another_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O0ekQjq-Ptex"
      },
      "source": [
        "You can even give the model version labels, which comes in handy for making predictions. This is only available through TensorFlow Serving's gRPC endpoints."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gx8pvEpePLDC"
      },
      "source": [
        "model_config_list {\n",
        "    config {\n",
        "        name: \"my_model\"\n",
        "        base_path: \"models/my_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "        model_version_policy {\n",
        "            specific {\n",
        "                versions: 1556250435\n",
        "                versions: 1556251435\n",
        "            }\n",
        "        }\n",
        "        version_labels {\n",
        "            key: \"stable\"\n",
        "            value: 1556250435\n",
        "        }\n",
        "        \n",
        "        version_labels {\n",
        "            key: \"testing\"\n",
        "            value: 1556251435\n",
        "        }\n",
        "    }\n",
        "    config {\n",
        "        name: \"another_model\"\n",
        "        base_path \"models/another_model\"\n",
        "        model_platform: \"tensorflow\"\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtyJLJY3QIm9"
      },
      "source": [
        "With that you can for example run a model A/B test.\n",
        "Starting with TensorFlow Serving 2.3, the *version_label* functionality will be able for REST endpoints too."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MfEhjcmQcbb"
      },
      "source": [
        "## REST versus gRPC\n",
        "\n",
        "### REST\n",
        "\n",
        "REST is a communication \"protocol\" used by today's web services.\n",
        "\n",
        "### gRPC\n",
        "\n",
        "gRPC is a remote procedure protocol defined by Google\n",
        "\n",
        "#### Which protocol to use?\n",
        "\n",
        "REST is very easy to use and is already widely available for all sorts of clients. gRPC APIs have a higher burden of entry, but come in handy because they often lead to significant performance improvements depending on the data structures required for the model inference.\n",
        "<br>\n",
        "Internally TensorFlow Serving converts JSON data structures submitted via REST to tf.Example data structures, and this can lead to slower performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA1ESaQ_Rw4d"
      },
      "source": [
        "## Making Predictions from the Model Server\n",
        "\n",
        "All following code examples concerning REST or gRPC requests are executed on the client side."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rywPPSm9R-kN"
      },
      "source": [
        "### Getting Model Predictiosn via REST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkvSUIZFPKjg"
      },
      "source": [
        "import requests"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBQRzrVASaIq"
      },
      "source": [
        "An example schowcase for a POST request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TZRZPgiBnnc"
      },
      "source": [
        "url = \"http://some-domain.abc\"\n",
        "payload = {\"key_1\": \"value_1\"}\n",
        "# Submit the request\n",
        "r = requests.post(url, json=payload)\n",
        "# View the HTTP response\n",
        "print(r.json())"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJXjzLEfSkML"
      },
      "source": [
        "### URL structure\n",
        "\n",
        "The URL for your HTTP request to the model server contains informatio about which model and which version you would like to infer:\n",
        "\n",
        "    http://{HOST}:{PORT}/v1/models/{MODEL_NAME}:{VERB}\n",
        "\n",
        "For Details see page 225 in the book.\n",
        "If you want to specify the model version for a prediction, you will need to extend the URL with the model version identifier.\n",
        "\n",
        "    http://{HOST}:{PORT}/v1/models/{MODEL_NAME}[/versions/${MODEL_VERSION}]:{VERB}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwwBvRQATdlb"
      },
      "source": [
        "### Payloads\n",
        "\n",
        "TensorFlow Serving expects the input data as a JSON data structure.\n",
        "\n",
        "    {\n",
        "        \"signature_name\": <string> # not necessary\n",
        "        \"instances\": <value> # list of input values or objects\n",
        "    }\n",
        "\n",
        "To submit multiple data samples, you can submit them as a list under the instances key, but if you want to submit one data example for the inference, you can use inputs and list all input values as a list.\n",
        "\n",
        "    {\n",
        "        \"signature_name\": <string> # not necessary\n",
        "        \"inputs\": <value>\n",
        "    }\n",
        "\n",
        "**Either \"inputs\" or \"instances\" must be provieded but not both at the same time!**\n",
        "\n",
        "Example model prediction request with a Python client:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hU_S6LNqSdp8"
      },
      "source": [
        "import requests"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjKYycFISdla"
      },
      "source": [
        "def get_rest_request(text, model_name=\"my_model\"):\n",
        "    # Exchange localhost with an IP address if the server is not running on the same machine\n",
        "    url = \"http://localhost:8501/v1/models/{}:predict\".format(model_name)\n",
        "    # Add more examples to the instances list if you want to infer more samples\n",
        "    payload = {\"instances\": [text]}\n",
        "    response = requests.post(url=url, json=payload)\n",
        "    \n",
        "    return reponse\n",
        "\n",
        "rs_rest = get_rest_request(text=\"classify my text\")\n",
        "rs_rest.json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBChupvxVPMo"
      },
      "source": [
        "### Using TensorFlow Serving via gRPC\n",
        "\n",
        "First you need to establisch a gRPC channel, which provides the connection to the gRPC server at a given host address and over a given port. Once the channel is established, you will create a stub, i.e. a local object which replicates the available methods from the server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byDNq2arSdjv"
      },
      "source": [
        "import grpc\n",
        "import tensorflow as tf\n",
        "from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFiEVCC_SdiD"
      },
      "source": [
        "def create_grpc_stub(host, port=8500):\n",
        "    hostport = \"{}:{}\".format(host, port)\n",
        "    channel = grpc.insecure_channel(hostport)\n",
        "    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
        "\n",
        "    return stub"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZjnNyn1UY7v"
      },
      "source": [
        "Once the gRPC stub is created, we can set the model and the signature to access predictions from the correct model and submit our data for the inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gyA7m-GoSdgG"
      },
      "source": [
        "def grpc_request(stub, data_sample, model_name=\"my_model\", signature_name=\"classification\"):\n",
        "    request = predict_pb2.PredictRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.model_spec.signature_name = signature_name\n",
        "    # inputs is the name of the input of our nerual network\n",
        "    request.inputs[\"inputs\"].CopyFrom(tf.make_tensor_proto(data_sample, shape=[1, 1]))\n",
        "    # 10 is the max time in seconds before the function times out\n",
        "    result_future = stub.Predict.future(request, 10)\n",
        "    \n",
        "    return result_future"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzAJ_jJiVFwQ"
      },
      "source": [
        "Infer the example dataset, with the two function calls:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKkwBUN8Sded"
      },
      "source": [
        "stub = create_grpc_stub(host, port=8500)\n",
        "rs_grpc = grpc_request(stub, data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv4CZuK4VQZA"
      },
      "source": [
        "**Secure Conncetions**\n",
        "\n",
        "The grpc library also provides functionality to connect securely with the gRPC enpoints, from the client side:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2cz6IO1SdcX"
      },
      "source": [
        "cert = open(client_cert_file, \"rb\").read()\n",
        "key = open(clien_key_file, \"rb\").read()\n",
        "ca_cert = open(ca_cert_file, \"rb\").read() if ca_cert_file else \"\"\n",
        "credentials = grpc.ssl_channel_credentials(\n",
        "    ca_cert,\n",
        "    key,\n",
        "    credits\n",
        ")\n",
        "\n",
        "channel = implementations.secure_channel(hostport, credentials)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4jrIjeDV3nI"
      },
      "source": [
        "TF Serving can terminate secure connections if SSL is configured, therefore follow the example on page 229 in the book.<br>\n",
        "Once the configuration file is created, you can pass the file to the TensorFlow Serving argument --ssl_config_file during the start of TensorFlow Serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgEx44TFSdab"
      },
      "source": [
        "!tensorflow_model_server --port=8500 \\\n",
        "                         --rest_api_port=8501 \\\n",
        "                         --model_name=my_model \\\n",
        "                         --model_base_path=/models/my_model \\\n",
        "                         --ssl_config_file=\"<path_to_config_file>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J02FvfbqWtQT"
      },
      "source": [
        "#### Getting predictions from classification and regression models\n",
        "\n",
        "Therefore you can use the gRPC API like follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8lHUj1WSdY2"
      },
      "source": [
        "def grpc_request(stub, data_sample, model_name=\"my_model\", signature_name=\"classification\"):\n",
        "    request = classification_pb2.ClassificationRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.model_spec.signature_name = signature_name\n",
        "    # inputs is the name of the input of our nerual network\n",
        "    request.inputs[\"inputs\"].CopyFrom(tf.make_tensor_proto(data_sample, shape=[1, 1]))\n",
        "    # 10 is the max time in seconds before the function times out\n",
        "    result_future = stub.Predict.future(request, 10)\n",
        "    \n",
        "    return result_future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0WKN9h4XOvA"
      },
      "source": [
        "For predictions from a regression model, do:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AlYN-ehZSdWG"
      },
      "source": [
        "from tensorflow_serving.apis import regression_pb2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzQ5T2nASdR6"
      },
      "source": [
        "def grpc_request(stub, data_sample, model_name=\"my_model\", signature_name=\"classification\"):\n",
        "    request = regression_pb2.RegressionRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.model_spec.signature_name = signature_name\n",
        "    # inputs is the name of the input of our nerual network\n",
        "    request.inputs[\"inputs\"].CopyFrom(tf.make_tensor_proto(data_sample, shape=[1, 1]))\n",
        "    # 10 is the max time in seconds before the function times out\n",
        "    result_future = stub.Predict.future(request, 10)\n",
        "    \n",
        "    return result_future"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuuDvYdiXioP"
      },
      "source": [
        "#### Payloads\n",
        "\n",
        "gRPC API uses Protocol Buffers as the data strucutres for the API request, which use less bandwidth compared to JSON payloads. Depending on the model input data structure, you can even get faster predictions as with REST endpoints. For the conversion of the data, TensorFlow provides you the function *tf.make_tensor_proto*, which allows various data formats, including scalars, lists, NumPy scalars and NumPy arrays."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwQ_2RrYYNpZ"
      },
      "source": [
        "## Model A/B Testing with TensorFlow Serving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GMP2Cw65ygl"
      },
      "source": [
        "from random import random"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_eN7iZXher"
      },
      "source": [
        "def get_rest_url(model_name, host=\"localhost\", port=8501, verb=\"predict\", version=None):\n",
        "    url = \"http://{}:{}/v1/models/{}/\".format(host, port, model_name)\n",
        "    if version:\n",
        "        url += \"versions/{}/\".format(version)\n",
        "    url += \":{}\".format(verb)\n",
        "    \n",
        "    return url"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lk5PEWkfXiDG"
      },
      "source": [
        "# Submit 10% of all requests from this client to version 1.\n",
        "# 90 % of the requests should go to the default models.\n",
        "threshold = 0.1\n",
        "# If version = None, TensorFlow Serving will infer with the default version.\n",
        "version = 1 if random() < threshold else None\n",
        "\n",
        "url = get_rest_url(model_name=\"complaints_classification\", version=version)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RIlyqJKb24CO"
      },
      "source": [
        "If you would like to extend theses capabilities by performing the random routing of the model inference on the server side, we highly recommend routing tools like <a href=\"https://istio.io/\">Istio</a> for this purpose. Istio was originally deigned for web traffic and can be used to route traffic to specific models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoAFg7xg3oN1"
      },
      "source": [
        "## Requesting Model Metadata from the Model Server\n",
        "\n",
        "A critical component of the continuous life cycle is gegenerating accuracy or general performance feedback about your model versions. Therefore we have to know which model version performed a correct are false prediction to improved it afterwards."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr3EphYb4Fa2"
      },
      "source": [
        "### REST Requests for Model Metadata\n",
        "\n",
        "TensorFlow Serving provides you an endpoint for model metadata:\n",
        "\n",
        "    http://{HOST}:{PORT}/v1/models/{MODEL_NAME}[/versions/${MODEL_VERSION}]/metadata\n",
        "\n",
        "Similar to the REST API inference requests, you have the option to specify the model verion in the request URL. We can request model metadata with a single GET request."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5Jhp8SnXh_F"
      },
      "source": [
        "import requests"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oA6uL_TwXh9F"
      },
      "source": [
        "def metadata_rest_request(model_name, host=\"localhost\", port=8501, version=None):\n",
        "    url = \"http://{}:{}/v1/models/{}/\".format(host, port, model_name)\n",
        "    if version:\n",
        "        url += \"versions/{}/\".format(version)\n",
        "    # Append /metadata for model information\n",
        "    url += \"/metadata\"\n",
        "    # Perform GET request\n",
        "    reponse = request.get(url=url)\n",
        "    \n",
        "    return response"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7YXzNtL_Y2f"
      },
      "source": [
        "The return will look like the specifications dictionary as in the model_config_lists above, but with a *model_spec* and *metadata* dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNSlwhQg_0oN"
      },
      "source": [
        "### gRPC for Model Metadata\n",
        "\n",
        "Here you file a GetModelMetadataRequest, add the model name to the specifications and submit the request via the GetModelMetadata method of the stub."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BgNc-NljXhaW"
      },
      "source": [
        "def get_model_version(model_name, stub):\n",
        "    request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.metadat_field.append(\"signature_def\")\n",
        "    # 5 is the max time in seconds before the function times out\n",
        "    response = stub.GetModelMetadata(request, 5)\n",
        "    \n",
        "    return response\n",
        "\n",
        "model_name = \"complaints_classficiation\"\n",
        "stub = create_grpc_stub(\"localhost\")\n",
        "get_model_version(model_name, stub)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fjvv636eAxlx"
      },
      "source": [
        "The gRPC response contains a ModelSpech object that contains the version number of the loaded model. Even more useful is the obtaining of the model signature information of the loaded model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzIoIlDDXhYW"
      },
      "source": [
        "from tensorflow_serving.apis import get_model_metadata_pb2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTjNK26ZXhW1"
      },
      "source": [
        "def get_model_metadata(model_name, stub):\n",
        "    request = get_model_metadata_pb2.GetModelMetadataRequest()\n",
        "    request.model_spec.name = model_name\n",
        "    request.metadat_field.append(\"signature_def\")\n",
        "    # 5 is the max time in seconds before the function times out\n",
        "    response = stub.GetModelMetadata(request, 5)\n",
        "    \n",
        "    return response.metadata[\"signature_def\"]\n",
        "\n",
        "\n",
        "model_name = \"complaints_classficiation\"\n",
        "stub = create_grpc_stub(\"localhost\")\n",
        "get_model_metadata(model_name, stub)\n",
        "\n",
        "# The information needs to be serialized to be human readable\n",
        "# Use SerializeToString function to convert the protocol buffer information\n",
        "print(meta.SerializeToString().decode(\"utf-8\", \"ignore\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7LzilzdB0-x"
      },
      "source": [
        "## Batching Inferene Requests\n",
        "\n",
        "Without batching like in training, we under-utilize the available memory of the CPU or GPU, because we handle each request for inference individually. Multiple clients can request model predictions and the server batches the different client requests into one \"batch\" to compoute."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwevNejXClRb"
      },
      "source": [
        "## Configuring Batch Predictions\n",
        "\n",
        "In TensorFlow Serving you have five configuration options for batching predictions:\n",
        " - max_batch_size (Int)\n",
        " - batch_timeout_micros (Int)\n",
        " - num_batch_threads (Int)\n",
        " - max_enqueued_batches (Int)\n",
        " - pad_variable_length_inputs (Bool)\n",
        "\n",
        "Setting parameters for optimal batcing requires some tuning and is application dependet, see for an initial help page 239. TensorFlow Serving will make predictions on the batch when either the max_batch_size or the timeout is reached.<br>\n",
        "For CPU predictions tune num_batch_threads to configure the number of CPU cores and for GPU predictios set max_batch_size to get an optimial utilization of the GPU memory.<br>\n",
        "The parameters can be set in a test_file like:\n",
        "\n",
        "    max_batch_size { value: 32 }\n",
        "    batch_timeout_micros { value: 5000 }\n",
        "    pad_variable_length_inputs: true\n",
        "\n",
        "For enabling batching you need to pass two additional parameters to the Docker container running TensorFlow Serving:\n",
        " - enable_batching = True\n",
        " - batching_paramters_file = \"path/of/the/batching/configuration/file/inside/of/the/container\"\n",
        "\n",
        "**Complete Example of the *docker run* command:**\n",
        "\n",
        "    !docker run -p 8500:8500 \\ # specify the default ports\n",
        "                -p 8501:8501 \\\n",
        "                --mount type=bind,source=/tmp/models,target=/models/my_model \\ # mount the model directory\n",
        "                --mount type=bind,source=/path/to/batch_config,target=/server_config \\ # mount the configuration file\n",
        "                -e MODEL_NAME=my_model\\ # specify your model\n",
        "                -t tensorflow/serving:latest-gpu \\ # specify the docker image for GPU\n",
        "                --enable_batching=true\n",
        "                --batching_parameters_file=/server_config/batching_parameters.txt\n",
        "\n",
        "It is highly recommend to make use of the TensorFlow Serving features, because it is especially useful for inferring a large number of data sampes with offline batch processes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKa_llH5GiHI"
      },
      "source": [
        "## Other TensorFlow Serving Optimizations\n",
        "\n",
        " - --file_system_poll_wait_seconds=1\n",
        " - --tensorflow_session_parallelism=0\n",
        " - --tensorflow_intra_op_parallelism=0\n",
        " - --tensorflow_inter_op_parallelism=0\n",
        "\n",
        "Similar to above you can pass this arguments to the docker run command to improve performance and avoid unnecessary cloud provider charges."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMjeDCCMG9Bu"
      },
      "source": [
        "## TensorFlow Serving Alternatives\n",
        "\n",
        " - <a href=\"https://docs.bentoml.org/en/latest/\">BentoML</a>\n",
        " - <a href=\"https://www.seldon.io/tech/products/core/\">Seldon</a>\n",
        " - <a href=\"https://oracle.github.io/graphpipe/#/\">GraphPipe</a>\n",
        " - <a href=\"https://stfs.readthedocs.io/en/latest/\">Simple TensorFlow Serving</a>\n",
        " - <a href=\"https://mlflow.org/\">MLflow</a>\n",
        " - <a href=\"https://ray.io/\">Ray Serve</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rkPOQyxH2I2"
      },
      "source": [
        "## Deploying with Cloud Providers\n",
        "\n",
        "Up till now every model server solution have to be managed by you personally.<br>\n",
        "But all primary cloud providers - Google Cloud, AWS and Microsoft Azure - offer machine learning products, including hosting of machine learning models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2JOrfL8OdrC"
      },
      "source": [
        "### Use Cases\n",
        "\n",
        " - Seamless model deployment\n",
        " - No scaling problems\n",
        "\n",
        "But the advantages comes at a cost and further have the downside of deploying via their own software development kits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4edALyAO9uf"
      },
      "source": [
        "### Example Deployment with GCP\n",
        "\n",
        "Instead of writing configuration files and executing temrinal commands, we can set up model endpoints through a web UI.\n",
        "\n",
        "**Limits of Model Size on GCP's AI Platform**\n",
        "\n",
        "GCP's endpoints are limited to model sizes up to 500 MB. However there are options to increase that up to 2GB with the compute engines of type N1.\n",
        "\n",
        "#### Model Deployment\n",
        "\n",
        "The deployment consists of three steps:\n",
        " - Make the model accessible on Google Cloud\n",
        " - Create a new model instance with Google Cloud's AI Platform\n",
        " - Create a new version with the model instance\n",
        "\n",
        "For the details see pages 246 - 252 in the book.\n",
        "\n",
        "#### Model Inference\n",
        "\n",
        "To connect to the Google Cloud API, you will need to install the library google-api-python-client with:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRy3UFWLXhU9"
      },
      "source": [
        "!pip install google-api-python-client"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GauoHk5WQ7lG"
      },
      "source": [
        "All Google services can be connected via a service object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__U9qQRtB0lU"
      },
      "source": [
        "import googleapiclient.discovery"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcgMYffnB0jZ"
      },
      "source": [
        "def _connect_service():\n",
        "    return googleapiclient.discovery.build(serviceName=\"ml\", version=\"v1\")"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCoaptE4RQF8"
      },
      "source": [
        "Similar to the REST and gRPC examples, we nest our inference data under a fixed instances key, which carries a list of input dictionaries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eu0-kzbkB0hk"
      },
      "source": [
        "def _generate_payload(sentence):\n",
        "    return {\"instances\": [{\"sentence\": sentence}]}"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_lQlAHC7Rlp0"
      },
      "source": [
        "Now request the prediction from the Google Cloud-hosted machine learning model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIJ1cxkNB0gC"
      },
      "source": [
        "project = \"yourGCPProjectName\"\n",
        "model_name = \"demo_model\"\n",
        "version_name = \"v1\"\n",
        "request = service.projects().predict(\n",
        "    name=\"projects/{}/models/{}/versions/{}\".format(\n",
        "        project,\n",
        "        model_name,\n",
        "        version_name\n",
        "    ),\n",
        "    body=_generate_payload(sentence)\n",
        ")\n",
        "response = request.execute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yKe-YQSSHCE"
      },
      "source": [
        "The Google Cloud AI Platform response contains the predict scores for the different categories similar to a REST response fro a TensorFlow Serving instance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NI2bnPtnSWQp"
      },
      "source": [
        "# Model Deployment with TFX Pipelines\n",
        "\n",
        "See page 255."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gkw0qYaH1IH"
      },
      "source": [
        "# References and Additional Resources\n",
        "\n",
        " - <a href=\"https://github.com/NVIDIA/nvidia-docker#quick-start\">Nvidia Container Toolkit</a> to use Docker with GPU support\n",
        " - <a href=\"https://istio.io/\">Istio</a> for phasing models, performing A/B tests or creating policies for data routed to specific models.\n",
        " - <a href=\"https://docs.bentoml.org/en/latest/\">BentoML</a>\n",
        " - <a href=\"https://www.seldon.io/tech/products/core/\">Seldon</a>\n",
        " - <a href=\"https://oracle.github.io/graphpipe/#/\">GraphPipe</a>\n",
        " - <a href=\"https://stfs.readthedocs.io/en/latest/\">Simple TensorFlow Serving</a>\n",
        " - <a href=\"https://mlflow.org/\">MLflow</a>\n",
        " - <a href=\"https://ray.io/\">Ray Serve</a>\n",
        " - <a href=\"https://github.com/tensorflow/serving/blob/master/tensorflow_serving/config/ssl_config.proto\">SSL config file</a> for configuration of a secure gRPC channel\n",
        " - <a href=\"https://github.com/tensorflow/serving\">TensorFlow Serving Github</a>"
      ]
    }
  ]
}